[["index.html", "Creating a simulated data set Chapter 1 A Monument to my Madness 1.1 What this book is, and what it is not", " Creating a simulated data set Ryan Schneider 2022-06-17 Chapter 1 A Monument to my Madness This book contains all my personal coding notes from the last two years. Why am I doing this? Probably because Im a glutton for punishment, and Id rather procrastinate than write my dissertation proposal. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 1.1 What this book is, and what it is not You know those absolutely amazing, comprehensive guides where you can learn everything you need to know about R? This is is not one of those guides. This book is designed as a quick reference guide for many of the most common things youll need to do in everyday data analysis and research. Think of it like a coding dictionary, as opposed to a manual or comprehensive text. If you want (or need) to learn R in-depth and/or from the ground up (i.e., youre a novice user), then you should go read Hadley Wickhams book and the tidyverse websites. Also, these slides might be a good high-level overview if youve never used the tidyverse before. That said, if youre already familiar with R and the tidyverse and just need a quick reference for what command do I need to accomplish XYZ, youve come to the right place. "],["introduction-r-basics.html", "Chapter 2 Introduction: R Basics 2.1 Importing Data 2.2 Exporting (i.e., saving) Data and Output", " Chapter 2 Introduction: R Basics For the love of God before you do anything, familiarize yourself with R Projects and the here package. These make R so much more user friendly and less of a nightmare. If you need an overview, go here: http://jenrichmond.rbind.io/post/how-to-use-the-here-package/ Now lets get stuck in. library(tidyverse) 2.1 Importing Data 2.1.1 Spreadsheets See https://nacnudus.github.io/spreadsheet-munging-strategies/index.html for more detailed and in-depth tutorials (if you need that kind of thing) 2.2 Exporting (i.e., saving) Data and Output 2.2.1 Exporting to .CSV Generally speaking, unless you have a specific reason to, dont. But if you must: write_csv() 2.2.2 Export to .RData (and load the data again later) save(obj_name, file=here::here(&quot;subfolder&quot;, &quot;save_file_name&quot;), compress = FALSE) load(here::here(&quot;folder&quot;, &quot;save_name.RData&quot;)) 2.2.3 Export to Excel library(openxlsx) #Method 1: If you only want to export 1 thing, and/or only need output document #write as object, with no formatting: write.xlsx(objectname,file = &quot;filenamehere.xlsx&quot;,colnames=TRUE, borders=&quot;columns&quot;) #write as table: write.xlsx(objectname,&quot;filename.xlsx&quot;,asTable = TRUE) #Method 2: If you want to do the above, but add multiple objects or tables to one workbook/file: ## first Create Workbook object wb &lt;- createWorkbook(&quot;AuthorName&quot;) #then add worksheets (as many as desired) addWorksheet(wb, &quot;worksheetnamehere&quot;) #then write the object to the worksheet writeData(wb, &quot;test&quot;, nameofobjectordataframe, startCol = 2, startRow = 3, rowNames = TRUE) #save excel file saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) #Method 3: exact same as method 2, but creating a more fancy tables wb &lt;- createWorkbook(&quot;Ryan&quot;) addWorksheet(wb, &quot;worksheetnamehere&quot;) writeDataTable(wb, sheetName, objectName, startCol = 1, startRow = 1, colNames = TRUE, rowNames = FALSE, tableStyle=&quot;TableStyleLight2&quot;,tableName=NULL, headerStyle = NULL,withFilter=FALSE,keepNA=TRUE,sep=&quot;, &quot;, stack = FALSE, firstColumn = FALSE, lastColumn = FALSE,bandedRows = TRUE,bandedCols = FALSE) saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) 2.2.4 Access/edit specific cell number values rainbow=tibble::tribble(~Color, &quot;red&quot;, &quot;orange&quot;, &quot;black&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) rainbow$Color[3] # access, but can&#39;t overwrite this way ## [1] &quot;black&quot; rainbow[3,&quot;Color&quot;] # access and can overwrite ## # A tibble: 1 x 1 ## Color ## &lt;chr&gt; ## 1 black rainbow[3, &quot;Color&quot;]= &quot;yellow&quot; # save this value to row 3 in column &quot;Color&quot; rainbow ## # A tibble: 6 x 1 ## Color ## &lt;chr&gt; ## 1 red ## 2 orange ## 3 yellow ## 4 green ## 5 blue ## 6 purple "],["wrangle-data.html", "Chapter 3 Wrangle Data 3.1 Joining or Splitting 3.2 Selecting/extracting specific variables with select() 3.3 Advanced Filtering techniques 3.4 If-then and Case-when 3.5 Conditional replacement of values 3.6 Merging variables 3.7 Apply a function to multiple variables at once 3.8 Pivoting (i.e., transposing) data 3.9 Turn row names into a column/variable 3.10 How to edit/change column names 3.11 Re-order columns in a data set 3.12 Date and time variables 3.13 Reverse-code a variable 3.14 Dummy coding (the very fast and easy way) 3.15 Create a relative ranking among several variables 3.16 Manipulating the working environment and many things at once 3.17 Wrangling Lists", " Chapter 3 Wrangle Data This chapter contains useful tips on wrangling (i.e., manipulating) data. If you need to know to do to things like create new variables, split one variable into multiple variables, pivot a data set from wide to long, etc., look no further. If you want a pretty good intro tutorial to the dplyr package, click here 3.1 Joining or Splitting Joining and splitting data is pretty straightforward. 3.1.1 Whole Data Sets The code below is from this excellent tutorial set.seed(2018) df1=data.frame(customer_id=c(1:10), product=sample(c(&#39;toaster&#39;,&#39;TV&#39;,&#39;Dishwasher&#39;),10,replace = TRUE)) df2=data.frame(customer_id=c(sample(df1$customer_id, 5)),state=sample(c(&#39;New York&#39;,&#39;California&#39;),5,replace = TRUE)) df1=tibble::as_tibble(df1) df2=tibble::as_tibble(df2) # df1 =left table # df2= right table Inner join - retains only rows with values that appear in both tables, and matches by keys. If youre joining two Qualtrics surveys together, this is most likely the one you want to use (e.g.Â matching by participant name, and only keeping rows in the joined data set for participants that have responses logged in both survey 1 and survey 2 df1 %&gt;% inner_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York Left join - returns everything in the left, and rows with matching keys in the right df1 %&gt;% left_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Right join - returns everything in the right, and rows with matching keys in the left df1 %&gt;% right_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York # note: example if the customer id column was named something different in the second df #df1 %&gt;% left_join(df2,by=c(&#39;customer_id&#39;=&#39;name2&#39;)) Full join - retain all rows from both tables, and join matching keys in both right and left df1 %&gt;% full_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Anti join - returns all rows in the left that do not have matching keys in the right df1 %&gt;% anti_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 2 ## customer_id product ## &lt;int&gt; &lt;chr&gt; ## 1 2 Dishwasher ## 2 4 toaster ## 3 5 TV ## 4 7 toaster ## 5 10 TV 3.1.2 Individual Columns/Variables Splitting or joining columns is much easier than doing it to whole data sets. You can use dplyr::separate() to accomplish the former, and dplyr::unite() for the latter. print(&quot;hello&quot;) ## [1] &quot;hello&quot; 3.2 Selecting/extracting specific variables with select() Sometimes when working with a data set, you want to work with a few specific variables. For instance, maybe you want to view a graph of only reverse-coded variables (which start with the prefix r); or maybe you want to create a subset of your data that has a few specific variables removed. For this you can use dplyr::select() and its associated helper commands select() can be thought of as extract; it tells R to identify and extract a specific variable (or variables) cars=mtcars # select one column cars %&gt;% select(mpg) # select multiple columns, if they are all next to one another cars %&gt;% select(mpg:hp) # select multiple columns by name (when not next to one another) by defining them in a vector cars %&gt;% select(c(mpg, hp, wt)) # select only variables that start with a certain prefix/character/pattern/etc. cars %&gt;% select(starts_with(&quot;d&quot;)) # ...or columns that end with a certain prefix/etc. cars %&gt;% select(ends_with(&quot;t&quot;)) # ...or contains a certain pattern or string cars %&gt;% select(contains(&quot;se&quot;)) # select ALL OF the variables in a data set that match those of a pre-defined vector # first define the names in a vector vars=c(&quot;hp&quot;, &quot;drat&quot;, &quot;gear&quot;, &quot;carb&quot;) #now use helper cars %&gt;% select(all_of(vars)) # select ANY OF the variables in a pre-defined vector vars_2=c(&quot;hp&quot;, &quot;drat&quot;, &quot;watermelon&quot;, &quot;grilled_cheese&quot;) # only the first two will be in the data cars %&gt;% select(any_of(vars_2)) # only (and all of) the variables actually PRESENT in the data are pulled # select only variables of a certain class or type cars %&gt;% select(where(is.numeric)) cars %&gt;% select(where(is.character)) Other examples can be seen on THIS LINK for a simple but detailed guide. 3.3 Advanced Filtering techniques All info here and the code taken from this link; credit goes to Suzan Baert. 3.3.1 Filter based on partial match Combine filter with str_detect to search for pattern matches in a column 3.3.2 Filtering based on multiple conditions You can filter for more than one value at once, or combine operators to do searches with conditions. Goal Code Return only rows where both conditions are met filter(condition1, condition2) Return all rows where condition 1 is true but condition 2 is not filter(condition1, !condition2) Return rows where condition 1 and/or condition 2 is met filter(condition1 | condition2) Return all rows where only one of the conditions is met, and not when both are met filter(xor(condition1, condition2) The sample code will return all rows with a bodywt above 100 and either have a sleep_total above 15 or are not part of the Carnivora order. msleep %&gt;% select(name, order, sleep_total:bodywt) %&gt;% filter(bodywt &gt; 100, (sleep_total &gt; 15 | order != &quot;Carnivora&quot;)) 3.3.3 Filtering across multiple columns From Susan Baerts blog page: You have three options for how to do this: filter_all(), which filters columns based on your further instructions filter_if(), which requires a boolean to indicate which columns to filter on. If that is true, the filter instructions will be followed. filter_at(), which requires you to specify columns inside a vars argument for which the filtering will be done. Retain all rows with the pattern match Ca inside. Useful for when you want to search for a key pattern (or number) across multiple columns. msleep %&gt;% select(name:order, sleep_total, -vore) %&gt;% filter_all(any_vars(str_detect(., pattern = &quot;Ca&quot;))) msleep %&gt;% select(name, sleep_total:bodywt) %&gt;% filter_all(any_vars(. &lt; 0.1)) # You can also switch any_vars to all_vars to filter across the whole data frame msleep %&gt;% select(name, sleep_total:bodywt, -awake) %&gt;% filter_all(all_vars(. &gt; 1)) What if you want to filter specific column types in your data frame, like just date columns, to find a specific date across multiple date columns? Or filter across multiple number columns to find every instance of a specific number? filter_if is better than filter_all here because the latter would return a filter search across the whole data frame. By using filter_if, we can get more specific about which columns to search through. msleep %&gt;% select(name:order, sleep_total:sleep_rem) %&gt;% filter_if(is.character, any_vars(is.na(.))) The last command, filter_at, does things differently. It doesnt filter all columns at once; and you do not have to specify the column type or which columns. This one allows you to indicate which columns to search through like you would within any select statement. Example: Search through the columns that start with sleep, for values in all of those columns above .5. msleep %&gt;% select(name, sleep_total:sleep_rem, brainwt:bodywt) %&gt;% filter_at(vars(contains(&quot;sleep&quot;)), all_vars(.&gt;5)) 3.4 If-then and Case-when 3.4.1 If-then The premise of an if/then or if/else statement is simple: If condition 1 is satisfied, perform x operation; if not, then do y mtcars %&gt;% mutate(power_level=ifelse(mtcars$hp&lt;350, &quot;Low&quot;, &quot;High&quot;)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb power_level ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Low ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Low ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Low ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Low ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Low ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Low This line of code effectively says: if the length in Sepal.Length is &gt;5, set new variable = to short; else, set it to long 3.4.2 Case-when When you have 3+ conditions, its easier to use case-when. This is a more simple and straightforward approach than nesting multiple if-else commands My_vector= case_when( Condition1 ~ value1, Condition2 ~ value2, Condition3 ~ value3 TRUE ~ valueForEverythingElse #catch all for things that don&#39;t meet the above conditions ) Example: mtcars %&gt;% mutate(size= case_when(cyl==4 ~ &quot;small&quot;, cyl==6 ~ &quot;medium&quot;, cyl==8 ~ &quot;large&quot;)) %&gt;% select(c(cyl,size)) %&gt;% head() ## cyl size ## Mazda RX4 6 medium ## Mazda RX4 Wag 6 medium ## Datsun 710 4 small ## Hornet 4 Drive 6 medium ## Hornet Sportabout 8 large ## Valiant 6 medium 3.5 Conditional replacement of values The following code is useful if you want to replace a value in one column, and the replacement is conditional upon the value in another column. mpg %&gt;% mutate(across(.cols = c(displ, cty, hwy), .fns = ~case_when(cyl == 4L ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 NA 1999 4 auto(l5) f NA NA p compact ## 2 audi a4 NA 1999 4 manual(m5) f NA NA p compact ## 3 audi a4 NA 2008 4 manual(m6) f NA NA p compact ## 4 audi a4 NA 2008 4 auto(av) f NA NA p compact ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact ## 7 audi a4 3.1 2008 6 auto(av) f 18 27 p compact ## 8 audi a4 quattro NA 1999 4 manual(m5) 4 NA NA p compact ## 9 audi a4 quattro NA 1999 4 auto(l5) 4 NA NA p compact ## 10 audi a4 quattro NA 2008 4 manual(m6) 4 NA NA p compact ## # ... with 224 more rows test %&gt;% mutate(across(.cols = c(rank), .fns = ~case_when(is.na(participant_score) ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) 3.6 Merging variables Sometimes youll have multiple variables and you want to collapse them into a single variable. The pmin() command is useful for this. example_data=tribble(~A,~B,~C, 1,NA,NA, 2,NA,NA, 3,NA,NA, NA,4,NA, NA,5,NA, NA,6,NA, NA,NA,7, NA,NA,8, NA,NA,9) example_data %&gt;% mutate(accept_reject = pmin(A,B,C,na.rm = TRUE)) 3.7 Apply a function to multiple variables at once You can either specify each column individually, like above, or tell R to identify columns for you based on their type or their name. This requires adding in one additional verbeither contains() or where() depending on what you want to do. Two simple examples: # turn multiple variables into factors ex_data=dplyr::tribble(~color, ~car, &quot;red&quot;, &quot;corvette&quot;, &quot;blue&quot;, &quot;chevelle&quot;, &quot;green&quot;, &quot;camaro&quot;, &quot;red&quot;, &quot;corvette&quot;, &quot;green&quot;, &quot;chevelle&quot;, &quot;yellow&quot;, &quot;gto&quot;) dplyr::glimpse(ex_data) ## Rows: 6 ## Columns: 2 ## $ color &lt;chr&gt; &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;green&quot;, &quot;yellow&quot; ## $ car &lt;chr&gt; &quot;corvette&quot;, &quot;chevelle&quot;, &quot;camaro&quot;, &quot;corvette&quot;, &quot;chevelle&quot;, &quot;gto&quot; ex_data %&gt;% mutate(across(c(color, car),factor)) ## # A tibble: 6 x 2 ## color car ## &lt;fct&gt; &lt;fct&gt; ## 1 red corvette ## 2 blue chevelle ## 3 green camaro ## 4 red corvette ## 5 green chevelle ## 6 yellow gto # round multiple columns to 1 decimal place mtcars %&gt;% mutate(across(c(disp:qsec),round,1)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.9 2.6 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.9 2.9 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.9 2.3 18.6 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.1 3.2 19.4 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.1 3.4 17.0 0 0 3 2 ## Valiant 18.1 6 225 105 2.8 3.5 20.2 1 0 3 1 3.8 Pivoting (i.e., transposing) data 3.8.1 Condense multiple rows into a single column (pivot wide to long) Rearranging data like this can make it easier to work with and analyze. Example below from my gradebook for stats (exported from Canvas), with fake names. The command structure is as follows: pivot_longer( # Transpose LENGTHWISE by.... cols = everything(), # Taking ALL variable names... names_to=&quot;variable&quot;, # ...and dumping them into this new variable/column values_to=&quot;missing_count&quot;) #...and placing their values in this other new column NOTE!!! Pivoting data from wide to long like this expands the number of rows to make a matrix so that (for example, each student now has as a row for each assignment). Therefore, you can only pivot longways (or wide) ONCE, otherwise you will make duplicates. If you need to pivot multiple columns, just include all of the columns in one single pivot; do not use two separate, back to back pivot commands. Example: gradebook=tibble::tribble( ~Student, ~Homework.1, ~Homework.2, ~Homework.3, ~Homework.4, ~Homework.5, ~Quiz.1, ~Quiz.2, ~Quiz.3, ~Quiz.4, ~Final, &quot;Bob&quot;, 19L, 0L, 13, 16, 0L, 21, 7L, 15, 17.5, 33, &quot;Jane&quot;, 17L, 19L, 16, 16.5, 25L, 21.5, 19L, 14.75, 9.5, 39.5, &quot;John&quot;, 19L, 19L, 14.5, 19.5, 25L, 21, 21L, 18.5, 17, 46.5 ) head(gradebook) ## # A tibble: 3 x 11 ## Student Homework.1 Homework.2 Homework.3 Homework.4 Homework.5 Quiz.1 Quiz.2 Quiz.3 Quiz.4 Final ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bob 19 0 13 16 0 21 7 15 17.5 33 ## 2 Jane 17 19 16 16.5 25 21.5 19 14.8 9.5 39.5 ## 3 John 19 19 14.5 19.5 25 21 21 18.5 17 46.5 gradebook=gradebook %&gt;% pivot_longer( # Transpose lengthwise by: cols = Homework.1:Final, # Taking these variables names_to=&quot;Assignment&quot;, # ...and dumping them into this new variable, storing them lengthwise values_to=&quot;Points&quot;) #...then place their values in this new column gradebook %&gt;% head() ## # A tibble: 6 x 3 ## Student Assignment Points ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bob Homework.1 19 ## 2 Bob Homework.2 0 ## 3 Bob Homework.3 13 ## 4 Bob Homework.4 16 ## 5 Bob Homework.5 0 ## 6 Bob Quiz.1 21 3.9 Turn row names into a column/variable Use the rownames() command to turn row names into a variable cars=rownames_to_column(mtcars, var = &quot;car&quot;) as_tibble(cars) %&gt;% slice(1:6) ## # A tibble: 6 x 12 ## car mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 Hornet 4 Drive 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 3.10 How to edit/change column names TWO WAYS TO DO THIS: Use colnames() (for base R) or rename() (for tidyverse) colnames() pulls up all the column/variable names as a vector. If you want to actually change them, youll need to combine this command with something like the sub() or gsub() commands (for base R). Im going to skip this becauseits base R. To access and change the names faster via tidyverse, run use rename() rm(list=ls()) # clear R&#39;s memory iris %&gt;% rename(&quot;hurr&quot;=&quot;Sepal.Length&quot;, &quot;durr&quot;=&quot;Sepal.Width&quot;, &quot;abcdefgh&quot;=&quot;Species&quot;) %&gt;% head() ## hurr durr Petal.Length Petal.Width abcdefgh ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa If you need to do some really fancy conditional renaming (e.g., changing all variables that start with r to start with rf instead, to make it more clear that the prefix actually stands for risk factor rather than reverse coded), youll need to use rename_with(). This command has two parts to it: the data set, and the function you wish to apply to it (which you put after the ~) rename_with(iris, ~ gsub(pattern = &quot;.&quot;, replacement = &quot;_&quot;, .x, fixed = TRUE)) %&gt;% head() ## Sepal_Length Sepal_Width Petal_Length Petal_Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa The gsub() function from Base R identifies matching patterns in the data and substitutes them with what you want instead. Think of it like Rs version of Find/Replace from Microsoft Word. The above line of code thus does the following: 1. First, it checks the column names of the supplied data set (iris) for a specific pattern (specified in pattern= ) 2. Then it replaces that pattern with your input in replacement= The great thing about rename_with() is that the .fn (or ~ for short) can take ANY function as input. For example, if you want to add an element to the column names rather than replace something, (e.g., a prefix or suffix), you can change the function to: rename_with( iris, ~ paste0(.x, &quot;_text&quot;)) %&gt;% head() ## Sepal.Length_text Sepal.Width_text Petal.Length_text Petal.Width_text Species_text ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa The above line adds a suffix. You can also add a prefix in the exact same way, just by switching the order of the string and the pattern in the paste0 command. Alternative method to the above This is a second way to do the above. It may appear more simple, but its also probably not as theoretically consistent with how the packages were made..it uses the stringr package to rename the column names, and stringr is typically used for editing vectors of strings in a data set. so it works, but its a little unconventional because you call and edit the column names like you would a variable in your data set. colnames(iris)=str_replace(colnames(iris), pattern = &quot;.&quot;, replacement = &quot;_&quot;) In short: rename() and rename_with() are for renaming variables, as their names imply. The str_ verbs from the stringr package are for editing string-based variabels in your data set. Either works though with a little ingenuity. 3.11 Re-order columns in a data set Use relocate() to change column positions. If you need to move multiple columns at once, this command uses the same syntax as select(). mtcars # notice the column order ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 mtcars %&gt;% relocate(hp:wt, .after= am) %&gt;% head() ## mpg cyl disp qsec vs am hp drat wt gear carb ## Mazda RX4 21.0 6 160 16.46 0 1 110 3.90 2.620 4 4 ## Mazda RX4 Wag 21.0 6 160 17.02 0 1 110 3.90 2.875 4 4 ## Datsun 710 22.8 4 108 18.61 1 1 93 3.85 2.320 4 1 ## Hornet 4 Drive 21.4 6 258 19.44 1 0 110 3.08 3.215 3 1 ## Hornet Sportabout 18.7 8 360 17.02 0 0 175 3.15 3.440 3 2 ## Valiant 18.1 6 225 20.22 1 0 105 2.76 3.460 3 1 3.12 Date and time variables Formatting a column of dates can be extremely helpful if you need to work with time data, but also an extreme pain in the ass if its not stored correctly. This tutorial will be divided into two parts to cover both scenarios that you could encounter. It requires things to be done in two stages, and very precisely. 3.12.1 Date-time objects If youre lucky enough to have a vector of date-times, like what Qualtrics gives you, this will be brainless. Just do the following: example_datetime_data=tibble::tribble(~datetime, &quot;2010-08-03 00:50:50&quot;, &quot;2010-08-04 01:40:50&quot;, &quot;2010-08-07 21:50:50&quot;) head(example_datetime_data) # stored as character string ## # A tibble: 3 x 1 ## datetime ## &lt;chr&gt; ## 1 2010-08-03 00:50:50 ## 2 2010-08-04 01:40:50 ## 3 2010-08-07 21:50:50 # Tidyverse lubridate::as_date(example_datetime_data$datetime) ## [1] &quot;2010-08-03&quot; &quot;2010-08-04&quot; &quot;2010-08-07&quot; 3.12.2 Date-only objects If youre unlucky enough to have only dates, and said dates are written in the traditional x/x/xxxx format, this will be an annoyance that has to be done in two stages. First, assuming your data is already imported and is being stored as a vector of character strings, you have to tell R to adjust the formatting of dates. You cannot change it from a character-based object into a Date or DateTime one until it recognizes the correct formatting. example_date_data=tibble::tribble(~X1, ~X2, &quot;8/4/2021&quot;, -49.87, &quot;8/4/2021&quot;, -13.85, &quot;8/3/2021&quot;, -7.45, &quot;8/3/2021&quot;, -172.71) # Correct formatting example_date_data$X1=format(as.POSIXct(example_date_data$X1,format=&#39;%m/%d/%Y&#39;),format=&#39;%Y-%m-%d&#39;) head(as_tibble(example_date_data)) ## # A tibble: 4 x 2 ## X1 X2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 2021-08-04 -49.9 ## 2 2021-08-04 -13.8 ## 3 2021-08-03 -7.45 ## 4 2021-08-03 -173. In the code above, note that there are two format commands: The first one tells R how the date data is currently being stored, while the second at the end tells it how you want it to be stored. In this case, we are changing it from the way we would usually hand write a date (e.g., 10/26/1993) to a format commonly recognized and used in Excel and stats software (1993-10-26). If your column also has times in it, you also need to include that too! Second, you can now correct the objects structure. You can do this with base Rs as.Date() or tidyverses date() verbs. # Tidyverse example_date_data$X1= lubridate::date(example_date_data$X1) # Base R version example_date_data$X1=as.Date(example_date_data$X1) Notice how the object is now stored as the correct type in the table above. NOTE! This entire process has been included in the tidy_date() command in my package, legaldmlab. 3.12.3 Find the difference between two dates/times difftime(part_1$end_date[1], part_2$end_date[1], units=&quot;days&quot;) 3.13 Reverse-code a variable To reverse-score a variable, you should use car::recode() Can be done a few different ways, depending on how many variables youre looking to recode: # Recode just one variable df$column=recode(df$column,&quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;) # Recode a select bunch of variables df=df %&gt;% mutate(across(c(family_close : family_feelings), recode, &quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;)) 3.14 Dummy coding (the very fast and easy way) Use dplyrs pivot_wider in conjunction with mutate to very quickly and automatically dummy code a column with any number of unique values. The middle part of the code below is what you needjust copy and paste it, and tweak the specifics library(tidyverse) mtcars |&gt; mutate(car=rownames(mtcars)) |&gt; dplyr::mutate(n=1) |&gt; tidyr::pivot_wider(names_from = cyl, values_from = n, names_prefix = &quot;number_cyl&quot;, values_fill = list(n=0)) |&gt; select(car, starts_with(&quot;number_&quot;)) |&gt; head() #truncate output for easier reading ## # A tibble: 6 x 4 ## car number_cyl6 number_cyl4 number_cyl8 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 1 0 0 ## 2 Mazda RX4 Wag 1 0 0 ## 3 Datsun 710 0 1 0 ## 4 Hornet 4 Drive 1 0 0 ## 5 Hornet Sportabout 0 0 1 ## 6 Valiant 1 0 0 3.15 Create a relative ranking among several variables If you want to create a variable that is an ordinal ranking of other variables, first you need to make sure your data is long-wise. Then, depending on the type of ranking system you want, youll might need a different ranking command. The min_rank command from dplyr works in a manner similar to base Rs rank command. It ranks things like you see in sporting events. For example, if there is a clear winner in a game but 3 people tie for second place, the ranks would look like this: 1,2,2,2,4,5. Notice that the positions are independent from the counts. Using the same example from above, if you want the ranks to have no gaps (i.e.Â 1,2,2,2,3,4), you need to use dplyrs dense_rank command. In either case, the ranks are generated from lowest to highest, so if you want to flip them around youll need to include desc() in the command. dat=tibble::tribble(~name, ~score, &quot;bob&quot;, 0, &quot;bob&quot;, 5, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, NA, &quot;alice&quot;, 70, &quot;alice&quot;, 80, &quot;alice&quot;, 90, &quot;alice&quot;, 20, &quot;alice&quot;, 20, &quot;alice&quot;, 1) dat %&gt;% mutate(ranked = dense_rank(desc(score))) ## # A tibble: 12 x 3 ## name score ranked ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 bob 0 8 ## 2 bob 5 6 ## 3 bob 50 4 ## 4 bob 50 4 ## 5 bob 50 4 ## 6 bob NA NA ## 7 alice 70 3 ## 8 alice 80 2 ## 9 alice 90 1 ## 10 alice 20 5 ## 11 alice 20 5 ## 12 alice 1 7 3.16 Manipulating the working environment and many things at once 3.16.1 Stuff the WHOLE working environment into a list files=mget(ls()) 3.16.2 Extract everything from a list into the environment list2env(cog_data, globalenv()) 3.16.3 Delete everything in the entire environment, except for one item rm(list=setdiff(ls(), &quot;cog_data&quot;)) # delete everything in the local environment except the final data set 3.17 Wrangling Lists 3.17.1 Nesting Imagine the concept of Russian Dolls, applied to data sets. You can manage data sets more effectively my collapsing them into a single tiny, mini data frame, and stuffing that inside of another one. This is done via nesting Effectively, you smush/collapse everything down so it fits inside one column. You can unnest to expand this data back out later when you need it, and keep it collapsed when you dont. This works because a vector/column in a data frame is a list of a defined length; and a data frame is thus simply a collection of lists that are all the same length. You can store anything in a data frame. You can keep the df connected to the model, which makes it very easy to manage a whole slew of related models You can use functional programming (i.e., iterative functions) to map functions or combinations of functions in new ways. Moreover and more importantly, when you use purrr to map functions onto multiple models or objects simultaneously, youre doing it to all of them at once with a single command, and the objects are kept together while you do it. This limits the mistakes you can make (e.g., copying and pasting code and forgetting to tweak something important; applying a function to the wrong object or set of objects by accident), and also reduces unnecessary code in your script. Converting data into tidy data sets gives you a whole new way (and easier way) to manage lots of information head(mtcars |&gt; nest(crap=vs:carb)) ## # A tibble: 6 x 8 ## mpg cyl disp hp drat wt qsec crap ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 21 6 160 110 3.9 2.62 16.5 &lt;tibble [1 x 4]&gt; ## 2 21 6 160 110 3.9 2.88 17.0 &lt;tibble [1 x 4]&gt; ## 3 22.8 4 108 93 3.85 2.32 18.6 &lt;tibble [1 x 4]&gt; ## 4 21.4 6 258 110 3.08 3.22 19.4 &lt;tibble [1 x 4]&gt; ## 5 18.7 8 360 175 3.15 3.44 17.0 &lt;tibble [1 x 4]&gt; ## 6 18.1 6 225 105 2.76 3.46 20.2 &lt;tibble [1 x 4]&gt; 3.17.2 Combining/collapsing list levels (Reducing) demo_vars=files |&gt; map(import_spss) |&gt; reduce(left_join, by=&quot;catieid&quot;) "],["clean-data.html", "Chapter 4 Clean Data 4.1 Replace a value with NA 4.2 Replace NAs with a value 4.3 Identify columns or rows with Missing values 4.4 Find the percentage of a variable that is missing 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set", " Chapter 4 Clean Data 4.1 Replace a value with NA Use dplyr::na_if() if you have a value coded in your data (e.g., 999) that you want to convert to NA example_data=dplyr::tribble(~name, ~bday_month, &quot;Ryan&quot;, 10, &quot;Z&quot;, 3, &quot;Jen&quot;, 999, &quot;Tristin&quot;, 999, &quot;Cassidy&quot;, 6) example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen 999 ## 4 Tristin 999 ## 5 Cassidy 6 example_data$bday_month=na_if(example_data$bday_month, 999) #example doing one column at a time example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 example_data %&gt;% # can also pass the data to mutate and do it the tidyverse way mutate(bday_month=na_if(bday_month, 999)) ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 4.2 Replace NAs with a value tidyr::replace_na() is very useful if you have some NAs in your data and you want to fill them in with some value. example_data=tibble::tribble(~name, ~fav_color, ~fav_food, &quot;Ryan&quot;, &quot;green&quot;, &quot;Mexican&quot;, &quot;Cassidy&quot;, &quot;blue&quot;, NA, &quot;Z&quot;, NA, NA, &quot;Tristin&quot;, &quot;purple&quot;, NA, &quot;Tarika&quot;, NA, NA, &quot;Jen&quot;, NA, &quot;Italian&quot;) example_data ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue &lt;NA&gt; ## 3 Z &lt;NA&gt; &lt;NA&gt; ## 4 Tristin purple &lt;NA&gt; ## 5 Tarika &lt;NA&gt; &lt;NA&gt; ## 6 Jen &lt;NA&gt; Italian # replace NA&#39;s in one col tidyr::replace_na(example_data$fav_food, &quot;MISSING&quot;) ## [1] &quot;Mexican&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;Italian&quot; # replace in multiple columns example_data %&gt;% mutate(across(c(fav_color, fav_food), replace_na, &quot;MISSING&quot;)) ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue MISSING ## 3 Z MISSING MISSING ## 4 Tristin purple MISSING ## 5 Tarika MISSING MISSING ## 6 Jen MISSING Italian 4.3 Identify columns or rows with Missing values is.na() is the base R way to identify, in a TRUE/FALSE manner, whether or not there are missing values in a vector y &lt;- c(1,2,3,NA) is.na(y) # returns a vector (F F F T) ## [1] FALSE FALSE FALSE TRUE 4.4 Find the percentage of a variable that is missing Sometimes necessary to check before conducting an analysis. This requires my package, legaldmlab ?legaldmlab::count_missing mtcars %&gt;% select(hp:drat) %&gt;% legaldmlab::count_missing() ## # A tibble: 2 x 3 ## variable missing_count percent_missing ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 hp 0 0.0% ## 2 drat 0 0.0% 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set Use tidyr::drop_na() to remove rows with missing values. example_data=dplyr::tribble(~name, ~bday_month, ~car, &quot;Ryan&quot;, 10, &quot;kia&quot;, &quot;Z&quot;, NA, &quot;toyota&quot;, &quot;Jen&quot;, NA, NA, &quot;Tristin&quot;, 999, NA, &quot;Cassidy&quot;, 6, &quot;honda&quot;) knitr::kable(example_data) name bday_month car Ryan 10 kia Z NA toyota Jen NA NA Tristin 999 NA Cassidy 6 honda example_data %&gt;% drop_na() # with nothing specified, it drops ALL variables that have &gt;=1 missing value ## # A tibble: 2 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Cassidy 6 honda example_data %&gt;% drop_na(car) # drops only rows with values missing in the specified column ## # A tibble: 3 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Z NA toyota ## 3 Cassidy 6 honda "],["working-with-factors.html", "Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels 5.2 Collapse factor levels 5.3 Add levels to a factor 5.4 Drop unused levels 5.5 Change the order of a factors levels", " Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels Use forcats::fct_recode() diamonds=diamonds %&gt;% as_tibble() diamonds$cut=fct_recode(diamonds$cut, &quot;meh&quot;=&quot;Fair&quot;, &quot;Wow&quot;=&quot;Premium&quot;) summary(diamonds$cut) ## meh Good Very Good Wow Ideal ## 1610 4906 12082 13791 21551 5.2 Collapse factor levels Extremely useful command for when you have infrequent cases in one factor and need to combine it with another. Works by specifying a series of new level names, each of which contains the information from the old variables. Format is as follows: fct_collapse(dataset$variable, NewLevelA=c(&quot;OldLevel1&quot;,&quot;Oldlevel2&quot;), # NewLevelA is the new variable that contains both variables 1 and 2 NewLevelB=c(&quot;OldLevel3&quot;)) 5.3 Add levels to a factor use fct_expand() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.4 Drop unused levels Use fct_drop() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.5 Change the order of a factors levels example_data=tribble(~person, ~condition, &quot;bob&quot;, &quot;25 years&quot;, &quot;jane&quot;, &quot;5 years&quot;, &quot;jim&quot;, &quot;5 years&quot;, &quot;john&quot;, &quot;25 years&quot;) example_data$condition=factor(example_data$condition) str(example_data$condition) ## Factor w/ 2 levels &quot;25 years&quot;,&quot;5 years&quot;: 1 2 2 1 Notice that R thinks these are nominal factors, and that 25 comes before 5. To fix this and correct the level order example_data$condition =fct_relevel(example_data$condition, c(&quot;5 years&quot;, &quot;25 years&quot;)) # specify level order str(example_data$condition) ## Factor w/ 2 levels &quot;5 years&quot;,&quot;25 years&quot;: 2 1 1 2 "],["working-with-strings.html", "Chapter 6 Working with Strings 6.1 Remove a pattern from a string 6.2 Replace one pattern in a string with another 6.3 Find (i.e., filter for) all instances of a string 6.4 Drop all rows from a data set that contain a certain string 6.5 Force all letters to lower case", " Chapter 6 Working with Strings 6.1 Remove a pattern from a string price_table=tribble(~car, ~price, &quot;Corvette&quot;, &quot;$65,000&quot;, &quot;Mustang GT&quot;, &quot;$40,000&quot;) # BASE R METHOD (sub by replacing something with nothing) gsub(&quot;\\\\$&quot;, &quot;&quot;,price_table$price) # (pattern, replace with, object$column) ## [1] &quot;65,000&quot; &quot;40,000&quot; # TIDYVERSE METHOD str_remove(price_table$price, pattern = &quot;\\\\$&quot;) ## [1] &quot;65,000&quot; &quot;40,000&quot; You can remove numbers by typing \"[:digit:]\" panss_sem_data$cgi_sev=str_remove(panss_sem_data$cgi_sev, pattern = &quot;[:digit:]&quot;) 6.2 Replace one pattern in a string with another Tidyverse command: str_replace() or str_replace_all() Base R command: gsub() # base R gsub(mtcars, replacement = ) #tidyverse str_replace_all(iris$Species, pattern=c(&quot;e&quot;, &quot;a&quot;), replacement=&quot;ZZZZ&quot;) |&gt; head() str_replace(iris$Species, pattern=c(&quot;e&quot;, &quot;a&quot;), replacement=&quot;ZZZZ&quot;) |&gt; head() 6.3 Find (i.e., filter for) all instances of a string Useful for finding very specific things inside a column (e.g., one particular persons name in a roster of names; everyone with a particular last name) Tidyverse command: str_detect() Base R command: grepl() Note both must be nested inside of filter() cars_df=rownames_to_column(mtcars, var = &quot;car&quot;) # base R cars_df |&gt; filter(grepl(&quot;Firebird&quot;, car)) # tidyverse cars_df %&gt;% filter(str_detect(car,&quot;Firebird&quot;)) You can also search for multiple strings simultaneously by including the or logical operator inside the quotes. cars_df |&gt; filter(str_detect(car, &quot;Firebird|Fiat&quot;)) You can also include the negation logical operator to filter for all instances except those with the specified string. # base R cars_df |&gt; filter(!(grepl(&quot;Pontiac&quot;, car))) # tidyverse cars_df |&gt; filter(!(str_detect(car, &quot;Pontiac&quot;))) 6.4 Drop all rows from a data set that contain a certain string # Tidyverse method cars_df |&gt; filter(str_detect(car, &quot;Merc&quot;, negate = TRUE)) #including negate=TRUE will negate all rows with the matched string # base R cars_df[!grepl(&quot;Merc&quot;, cars_df$car),] 6.5 Force all letters to lower case Use stringr::str_to_lower() blah=tribble(~A, ~B, &quot;A&quot;,&quot;X&quot;, &quot;A&quot;,&quot;X&quot;) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 A X ## 2 A X blah$A=str_to_lower(blah$A) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 a X ## 2 a X "],["figures-and-graphs-with-the-ggplot-and-see-packages.html", "Chapter 7 Figures and Graphs with the ggplot and see packages 7.1 Commands for ggplot graph types 7.2 Specific Commands for Specific Types of Analysis 7.3 Highlight specific points 7.4 Add labels to data points 7.5 Plotting multiple graphs at once 7.6 Change the colors (bars; columns; dots; etc.) 7.7 Other aesthetic mappings 7.8 Adding and Customizing Text 7.9 Remove gridlines 7.10 Faceting 7.11 Log transformations 7.12 Changing the scale of the axis 7.13 Add a regression line 7.14 Save a graph or figure", " Chapter 7 Figures and Graphs with the ggplot and see packages There are three parts to a ggplot2 call: 1. data 2. aesthetic mapping 3. Layer There is no piping involved in ggplot. You simply invoke ggplot, and tell it what they dataset is. Then you specify the aesthetics, and then the mapping. Lastly, include other optional stuff (e.g.Â expanded y-axis scale; titles and legends; etc.) Every single plot has the exact same layout that ONLY USES the above three points: ggplot(dataframe, aes(graph dimensions and variables used)) + geom_GraphType(specific graph controls) ## OR ## ggplot(dataframe) + geom_GraphType(aes(graph dimensions and variables used), specific graph controls) # mapping= aes() can go in either spot Then if you have other stuff you want to add on top of this, like axis labels, annotations, highlights, etc., you keep adding those in separate lines 7.1 Commands for ggplot graph types Graph Type Geom command Scatter geom_point() Line geom_line() Box geom_boxplot() Bar geom_bar() Column geom_col() Histogram geom_histogram() Density curve geom_density() Note that bar and column graphs look identical at first glance, but they serve two different purposes. Bar graphs are for frequency counts, and thus only take an X-axis variable; Column graphs are for showing the relationship between two variables X and Y, and display the values in the data # BAR GRAPH # height of bars is a frequency count of each level of the X variable cut bar_plot=ggplot(diamonds, aes(x=cut)) + geom_bar()+ theme_classic() # COLUMN GRAPH # height of bars represents relationship between price and cut col_plot=ggplot(diamonds, aes(x=cut, y=price)) + geom_col()+ theme_classic() see::plots(bar_plot, col_plot, n_columns = 2, tags = c(&quot;Bar&quot;, &quot;Column&quot;)) 7.2 Specific Commands for Specific Types of Analysis 7.2.1 lavaan stuff 7.2.1.1 Plotting an SEM or CFA model First lets set up a model to use. library(lavaan) HS.model &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9&#39; fit1 &lt;- cfa(HS.model, data=HolzingerSwineford1939) Two options for graphing it. Option 1 is graph_sem() from the tidySEM package. tidySEM::graph_sem(fit1) Option 2 is from the easystats suite plot(parameters::parameters(fit1)) ## Using `sugiyama` as default layout 7.2.2 Bayes stuff Quick highlights here of my favorite functions from this package. See (ha) the full package overview at this link You can adjust the colors of the figures by setting them yourself (with scale_fill_manual), or by using the appropriate scale_fill command 7.2.2.1 Probability of Direction (Pd) figure Use plot(pd()) to visualize the Probability of Direction index. plot(bayestestR::pd(fit1))+ scale_fill_manual(values=c(&quot;#FFC107&quot;, &quot;#E91E63&quot;))+ theme_classic()+ theme(plot.title = element_text(hjust = 0.5, size = 14, face = &quot;italic&quot;)) 7.2.2.2 ROPE figure plot(fit1, rope_color = &quot;grey70&quot;)+ gameofthrones::scale_fill_got_d(option = &quot;white_walkers&quot;) # scale_fill_manual(values = c(&quot;gray75&quot;,&quot;red&quot;) ROPE tests are plots of distributions, and therefore use scale_fill_xyz_d commands. (the d stands for discrete). You can use any scale theme color set from any package, as long as it ends in _d values=c(#FFC107, #E91E63) is the default bayestestR theme colors from their website 7.2.2.3 Bayes factor models comparison figure plot(bayesfactor_models(Thesis_Model,discount_model))+ scale_fill_flat(palette = &quot;complement&quot; , reverse = TRUE)+ # scale color adjustment 7.2.3 Histograms and density curves Since I use these so often I figure they deserve their own special section. Basic histograms can be built with the following code: ggplot(data = mtcars, aes(x=cyl)) + geom_histogram(binwidth = .5, colour=&quot;Black&quot;, fill=&quot;green&quot;) + # histogram theme_classic() and your basic density curve with the following: ggplot(diamonds, aes(x=price)) + geom_density(alpha=.3)+ # density plot. Alpha sets the transparency level of the fill. theme_classic() You can also use the following code from bayestestR to build a really quick and nice density curve plot(bayestestR::point_estimate(diamonds, centrality=c(&quot;median&quot;,&quot;mean&quot;)))+ labs(title=&quot;Mean and Median&quot;) 7.3 Highlight specific points The gghighlight package is great for this # example 1 ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp))+ # add data labels (optional) gghighlight::gghighlight(hp &gt; 200) # add highlights, according to some criteria # example 2 diamonds_abr=diamonds %&gt;% slice(1:100) ggplot(diamonds_abr, aes(x= cut, y= price, colour=price))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = diamonds_abr, aes(label = price))+ # this line labels gghighlight::gghighlight(cut %in% c(&quot;Very Good&quot;, &quot;Ideal&quot;)) #this line highlights 7.4 Add labels to data points ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp)) ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point() + geom_text(aes(label=hp, hjust=2.5, vjust=2.5)) #geom_label(aes(label = scales::comma(n)), size = 2.5, nudge_y = 6) 7.5 Plotting multiple graphs at once see::plots() is good for this. print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.6 Change the colors (bars; columns; dots; etc.) This can be done in at least two different ways, depending on your goal. To change the fill color by factor or group, add fill = ___ within the aes() command. If you want to add color and/or fill to a continuous variable, do that within the geom_density() command. If you want to add color and make all of the (bars; dots; lines; etc.) the same color, than that is a graph-wide control and needs to be put in geom_point(). This manually sets the color for the whole graph. # add a color scale to the dots ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(color=&quot;blue&quot;) If you want to add color that changes according to a variable (e.g., by factor level), then the color needs to be specified as a variable name, in the aes mapping with the other variables. ggplot(mtcars, aes(x= mpg, y=hp, color=cyl))+ geom_point() 7.6.1 Fine-tuning colors You can change the spectrum of colors to specific colors if you want. Useful for example, when making graphs for APLS presentations; you can change the colors to be Montclair State University themed. When changing the color scale of graphs, note that scale_fill commands are used for representing nominal data, while scale_color commands are for representing continuous data. As such, you use scale_fill to fill in area on a graph that shows a whole category or distinct things; and scale_color to use gradients of color to show changes in continuous data. For figures that have solid area (e.g., density; box; bar; violin plots; etc.), use scale_fill For figures that have continuous changes (e.g., line and scatter plots), use scale_color # Set colors manually ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_manual(values=c(&quot;green&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;blue&quot;)) ggplot(mtcars, aes(x = wt, y = mpg, color=as.factor(cyl)))+ geom_point() + scale_color_manual(values=c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;)) # Use color scales from a package library(gameofthrones) # NOTICE THAT scale_fill AND scale_color STILL APPLY TO THEIR RESPECTIVE GRAPH TYPES # bar graphs ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Tully&quot;) ggplot(mtcars, aes(factor(cyl), fill=factor(vs)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Daenerys&quot;) # scatter plot ggplot(mtcars, aes(x = mpg, y = disp, colour = hp))+ geom_point(size = 2) + scale_colour_got(option = &quot;Lannister&quot;) Fill graphs also come with an extra option: Setting the outline color. You can change the outline of the bar/column/etc. by specifying the color inside geom_x() # change only the fill of the bars ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() # Change the outline of the bars by adding color inside the geom_bar() command ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar(color=&quot;black&quot;) 7.6.2 More options with the see package See this link for setting color gradients for continuous variables, or using other custom color palattes like the gameofthrones package. Check out the see package for some good color scales; the commands for which are here. Incidentally, see is great not only for regular ggplot graphs, but also Bayesian stats graphs link; effect size graphs link; correlation graphs link; and more. 7.7 Other aesthetic mappings shape() controls the shapes on the graph alpha() controls transparency size() controls size Note again that if you want it to change by variable, it goes INSIDE aes(); but if you want to set it manually for the whole graph, it goes in geom_x() # shape ggplot(mtcars, aes(x= mpg, y=hp, shape=as.factor(cyl)))+ geom_point() ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(shape=23) # transparency ggplot(mtcars, aes(x= mpg, y=hp, alpha=hp))+ geom_point() # size ggplot(mtcars, aes(x= mpg, y=hp, size=cyl))+ geom_point() 7.8 Adding and Customizing Text 7.8.1 Add a title, axis labels, and captions All three can be added with labs(). ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;) 7.8.2 Center graph title Add the line theme(plot.title = element_text(hjust = 0.5)) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 7.8.3 Use different fonts See tutorial on this web page Or, use the extrafont package, and set everything using the theme() command. # Visualize new groups library(extrafont) loadfonts(device=&quot;win&quot;) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;)) 7.9 Remove gridlines Add theme(panel.grid = element_blank()) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;))+ theme(panel.grid = element_blank()) 7.10 Faceting This is dividing one plot into subplots, in order to communicate relationships better. Again, this is just a single extra command, this time at the end of the code: facet_wrap(~columnhead) The tilde sign in R means by, as in divide (something) by this print(&quot;temp&quot;) This line produces a graph of population and life expectency, breaking it down to make a separate graph per each continent 7.11 Log transformations Sometimes when your data is really squished together on a graph it is hard to read. In this case, log transformations are really helpful, to change the scale of the data. For example, by multiplying all your points by 10x To create a log transformation of the same scatter plot above, add one extra bit: scale_x_log10() print(&quot;temp&quot;) You can also make both axis be logged by adding +scale again for y 7.12 Changing the scale of the axis Add coord_cartesian(xlim = c(lower,upper)) print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.13 Add a regression line Add the line geom_smooth(method = \"lm\", formula = y ~ x) ggplot(mtcars, aes(x= mpg, y=hp, color=mpg))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, formula = y ~ x) 7.14 Save a graph or figure Use the ggsave command ggsave( &quot;panss_total_scores.png&quot;, plot = scatter_plot, device = &quot;png&quot;, path = here::here(&quot;Figures and Tables&quot;), scale = 1, width = NA, height = NA, units = c(&quot;in&quot;, &quot;cm&quot;, &quot;mm&quot;, &quot;px&quot;), dpi = 300, limitsize = TRUE, bg = NULL, ) "],["making-tables-with-flextable.html", "Chapter 8 Making Tables with flextable 8.1 APA Table Components 8.2 Indent values 8.3 Add a Horizontal border (AKA horizontal spanner) 8.4 Change font and font size 8.5 Grouped table 8.6 Adjust line spacing 8.7 Set global options for all flextables 8.8 Complete Example 8.9 Wraper-function to create APA-style tables", " Chapter 8 Making Tables with flextable NOTES: - j refers to the column - i refers to the row number 8.1 APA Table Components 8.2 Indent values https://davidgohel.github.io/flextable/reference/padding.html https://stackoverflow.com/questions/64134725/indentation-in-the-first-column-of-a-flextable-object Use the padding function: ft &lt;- padding(ft, i=2, j=1, padding.left=20) 8.3 Add a Horizontal border (AKA horizontal spanner) hline(., i=4, j=1:2, part = &quot;body&quot;) 8.4 Change font and font size glm_table&lt;-flextable::font(glm_table, part = &quot;all&quot;, fontname = &quot;Times&quot;) %&gt;% # Font fontsize(., size = 11, part = &quot;all&quot;) # Font size 8.5 Grouped table cars=rownames_to_column(mtcars, var = &quot;Model&quot;) test=flextable::as_grouped_data(x=cars, groups = c(&quot;cyl&quot;)) 8.6 Adjust line spacing flextable::line_spacing(space = &quot;0.5&quot;) 8.7 Set global options for all flextables # set global options for all flextables flextable::set_flextable_defaults(font.family = &quot;Times New Roman&quot;, font.size = 11) # create an fp_border object to set the borders for the flextables border.test=officer::fp_border(color = &quot;black&quot;, style = &quot;solid&quot;, width = 1) 8.8 Complete Example library(flextable) ## ## Attaching package: &#39;flextable&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## compose # set global options for all flextables flextable::set_flextable_defaults(font.family = &quot;Times New Roman&quot;, font.size = 11) # create an fp_border object to set the borders for the flextables border.test=officer::fp_border(color = &quot;black&quot;, style = &quot;solid&quot;, width = 1) mtcars |&gt; slice(1:5) |&gt; flextable() |&gt; # FIX BORDERS IN THE HEADER hline_bottom(border = border.test, part = &quot;header&quot;) |&gt; hline_top(border = border.test, part = &quot;header&quot;) |&gt; # CREATE A TITLE HEADER; APPLY FORMATTING add_header_lines(values = &quot;Car Stuff&quot;) |&gt; hline_top(border = fp_border_default(width = 0), part = &quot;header&quot;) |&gt; flextable::align(align = &quot;left&quot;, part = &quot;header&quot;, i=1) |&gt; italic(part = &quot;header&quot;, i=1) |&gt; # FIX BORDER IN TABLE BODY hline_bottom(border = border.test, part = &quot;body&quot;) |&gt; # Line spacing flextable::line_spacing(space = &quot;0.5&quot;) |&gt; # Add a footer flextable::add_footer_lines(values = &quot;A test footer&quot;) |&gt; # SET COLUMN WIDTH/DIMENSIONS autofit(part = &quot;all&quot;) |&gt; width(j=1, width = 0.4, unit = &quot;in&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-f3751f0c{}.cl-f36d2f68{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f36d2f69{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-f36d42f0{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-f36d42f1{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-f36d42f2{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 0.5;background-color:transparent;}.cl-f36d9110{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d9111{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d9112{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d9113{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d9114{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d9115{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d9116{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d9117{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d9118{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d9119{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36d911a{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da498{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da499{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da49a{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da49b{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da49c{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da49d{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da49e{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da49f{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da4a0{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da4a1{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36da4a2{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db820{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db821{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db822{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db823{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db824{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db825{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db826{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db827{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db828{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db829{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36db82a{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36dcbb2{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36dcbb3{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36dcbb4{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36dcbb5{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36dcbb6{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36dcbb7{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-f36dcbb8{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Car Stuffmpgcyldisphpdratwtqsecvsamgearcarb21.061601103.902.62016.46014421.061601103.902.87517.02014422.84108933.852.32018.61114121.462581103.083.21519.44103118.783601753.153.44017.020032A test footer 8.9 Wraper-function to create APA-style tables This took way too many hours. Youre welcome. APA_table=function(flextable_object, table_title, include_note){ flextable_object=flextable_object |&gt; hline_bottom(border = border.test, part = &quot;header&quot;) |&gt; hline_top(border = border.test, part = &quot;header&quot;) |&gt; # CREATE A TITLE HEADER; APPLY FORMATTING add_header_lines(values = table_title) |&gt; hline_top(border = fp_border_default(width = 0), part = &quot;header&quot;) |&gt; flextable::align(align = &quot;left&quot;, part = &quot;header&quot;, i=1) |&gt; italic(part = &quot;header&quot;, i=1) |&gt; # FIX BORDER IN TABLE BODY hline_bottom(border = border.test, part = &quot;body&quot;) |&gt; # SET FONT flextable::font(part = &quot;all&quot;, fontname = &quot;Times New Roman&quot;) |&gt; flextable::fontsize(part = &quot;all&quot;, size = 11) |&gt; # SET COLUMN WIDTH/DIMENSIONS autofit(part = &quot;all&quot;) if(include_note==FALSE) (return(flextable_object)) if(include_note!=FALSE) (flextable_object=add_footer_lines(flextable_object, values = paste0(&quot;Note.&quot;,&quot; &quot;, include_note)) |&gt; fontsize(part = &quot;footer&quot;, size = 11) |&gt; font(part = &quot;footer&quot;, fontname = &quot;Times&quot;)) return(flextable_object) } "],["misc.-stuff.html", "Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables 9.2 Read SPSS files into R 9.3 Turn numbers into percentages 9.4 Find all possible combindations of items in a vector 9.5 Download files from the internet 9.6 Print multiple things in one statement", " Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables Note. See Chapter 10s example purrr walk through for a guide on how to scrape multiple web tables simultaneously Simple example. library(rvest) ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding library(tidyverse) html=read_html(&#39;https://shop.tcgplayer.com/price-guide/pokemon/base-set&#39;) %&gt;% html_table(fill = TRUE) html ## [[1]] ## # A tibble: 101 x 6 ## PRODUCT Rarity Number `Market Price` `Listed Median` `` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abra Common 43 $0.40 $0.50 View ## 2 Alakazam Holo Rare 1 $32.68  View ## 3 Arcanine Uncommon 23 $1.87 $2.00 View ## 4 Beedrill Rare 17 $3.54 $3.55 View ## 5 Bill Common 91 $0.36 $0.38 View ## 6 Blastoise Holo Rare 2 $119.35  View ## 7 Bulbasaur Common 44 $1.87 $2.51 View ## 8 Caterpie Common 45 $0.47 $0.70 View ## 9 Chansey Holo Rare 3 $18.96  View ## 10 Charizard Holo Rare 4 $355.71  View ## # ... with 91 more rows # Saved as a list by default. Now extract your table from said list html=as_tibble(html[[1]] %&gt;% # find out which number it is in the list select(&#39;PRODUCT&#39;,&#39;Rarity&#39;,&#39;Number&#39;,&#39;Market Price&#39;)) # if needed, specify which columns you want too html ## # A tibble: 101 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Abra Common 43 $0.40 ## 2 Alakazam Holo Rare 1 $32.68 ## 3 Arcanine Uncommon 23 $1.87 ## 4 Beedrill Rare 17 $3.54 ## 5 Bill Common 91 $0.36 ## 6 Blastoise Holo Rare 2 $119.35 ## 7 Bulbasaur Common 44 $1.87 ## 8 Caterpie Common 45 $0.47 ## 9 Chansey Holo Rare 3 $18.96 ## 10 Charizard Holo Rare 4 $355.71 ## # ... with 91 more rows # remove $ symbol in Price column to make it easier to work with html$`Market Price`=str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) html=html %&gt;% mutate(`Market Price`=as.numeric(`Market Price`)) # convert from string to numeric # view finished table head(html) ## # A tibble: 6 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Abra Common 43 0.4 ## 2 Alakazam Holo Rare 1 32.7 ## 3 Arcanine Uncommon 23 1.87 ## 4 Beedrill Rare 17 3.54 ## 5 Bill Common 91 0.36 ## 6 Blastoise Holo Rare 2 119. Slightly more complicated example Reading a table into R takes a few steps. Step 1 is to copy and paste the URL into the read_html() verb like below: pacman::p_load(rvest, tidyverse) exonerations_table=read_html(&quot;https://www.law.umich.edu/special/exoneration/Pages/detaillist.aspx&quot;) %&gt;% html_nodes(&quot;table.ms-listviewtable&quot;) %&gt;% html_table(fill=TRUE, header = TRUE) Sometimes if the web page is extremely basic and pretty much the only thing on it is a table, you can stop there. Most of the time though, there will be tons of other stuff on the website and you need to get more specific so R can find the table. This is the html_nodes() part of the above command; in there you specify the exact part of the web page where the table is located/what object file it is. To find this you will need to use the Developer mode in your browser. See this screenshot for an example knitr::include_graphics(here::here(&quot;pics&quot;, &quot;scrape.png&quot;)) In Firefox you open this by going to Settings &gt; More Tools &gt; Web Developer Tools (or CNTRL + Shift + I). Begin by looking through the console in the center bottom for names that look like they would be related to your table. A good place to start might be  , which contains the main body of the web page. Click on a name to expand it and see all the elements on the page contained there. Ultimately what youre looking for is what you see above: an element that, when selected, highlights ONLY the area of the web page youre looking for. To get at this you will need to keep expanding, highlighting, and clicking repeatedly.it can take some digging. Keep drilling down through page elements until you find the one that highlights the table and just the table. When you find this, look for the .ms file in that name; you should also see this in the smaller console box on the right. That is the file youll need. Write that name in the html_node command and read it into R. Thats stage 1. From here you now need to clean up the table. exonerations_table=as.data.frame(exonerations_table) # convert into a df Your table might be different, but this ones names were messed up when read in, so lets fix those first and then fix the rows and columns. # save the names to a vector table_names=exonerations_table$Last.Name[1:20] # Trim out the garbage rows and columns exonerations_table=exonerations_table %&gt;% select(Last.Name:Tags.1) %&gt;% slice(22:n()) # over-write incorrect col names with the vector of correct ones we saved above colnames(exonerations_table)=table_names # clean up names exonerations_table=exonerations_table %&gt;% janitor::clean_names() # verify structure of columns is correct # glimpse(exonerations_table) Yikes, a lot of stuff is stored incorrectly, and as a result theres some missing values that need to be addressed and other data that needs to be corrected. exonerations_table=as_tibble(exonerations_table) %&gt;% # convert to tibble mutate(across(c(dna,mwid:ild), na_if,&quot;&quot;)) %&gt;% # turn missing values into NA&#39;s mutate(across(c(dna,mwid:ild), replace_na, &quot;derp&quot;)) %&gt;% # replace NA&#39;s with a string (required for the next lines to work) mutate(dna=ifelse(dna==&quot;DNA&quot;,1,0), # change these variables from text to numeric to better facilitate analysis mwid=ifelse(mwid==&quot;MWID&quot;,1,0), fc=ifelse(fc==&quot;FC&quot;,1,0), p_fa=ifelse(p_fa==&quot;P/FA&quot;,1,0), f_mfe=ifelse(f_mfe==&quot;F/MFE&quot;,1,0)) %&gt;% mutate(across(c(st, crime, dna:f_mfe),factor)) # correct form by converting to factors And thats it! Check out final result! head(exonerations_table) ## # A tibble: 6 x 20 ## last_name first_name age race st county_of_crime tags om_tags crime sentence convicted exonerated dna x mwid fc p_fa f_mfe om ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; ## 1 Abbitt Joseph 31 Black NC Forsyth CV, I~ &quot;&quot; Chil~ Life 1995 2009 1 &quot;&quot; 1 0 0 0 derp ## 2 Abbott Cinque 19 Black IL Cook CIU, ~ &quot;OF, W~ Drug~ Probati~ 2008 2022 0 &quot;&quot; 0 0 1 0 OM ## 3 Abdal Warith Habib 43 Black NY Erie IO, SA &quot;OF, W~ Sexu~ 20 to L~ 1983 1999 1 &quot;&quot; 1 0 0 1 OM ## 4 Abernathy Christopher 17 White IL Cook CIU, ~ &quot;OF, W~ Murd~ Life wi~ 1987 2015 1 &quot;&quot; 0 1 1 0 OM ## 5 Abney Quentin 32 Black NY New York CV &quot;&quot; Robb~ 20 to L~ 2006 2012 0 &quot;&quot; 1 0 0 0 derp ## 6 Acero Longino 35 Hispa~ CA Santa Clara NC, P &quot;&quot; Sex ~ 2 years~ 1994 2006 0 &quot;&quot; 0 0 0 0 derp ## # ... with 1 more variable: ild &lt;chr&gt; Check out this page for a quick overview. 9.2 Read SPSS files into R Use foreign::read.spss spss_version=foreign::read.spss(here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;, &quot;JLWOP_RYAN.sav&quot;), to.data.frame = TRUE) Might also want to add as_tibble() on the end. 9.3 Turn numbers into percentages Use scales::percent(), which converts normal numbers into percentages and includes the percent sign (%) afterwards simple_table=tribble(~n_people, ~votes_in_favor, 25, 14) simple_table=simple_table %&gt;% mutate(percent_voted_for=scales::percent(votes_in_favor/n_people, accuracy = 0.1, scale = 100)) simple_table ## # A tibble: 1 x 3 ## n_people votes_in_favor percent_voted_for ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 25 14 56.0% Scale is what to multiple the original number by (e.g., convert 0.05 to 5% by x100) Accuracy controls how many places out the decimal goes 9.4 Find all possible combindations of items in a vector y &lt;- c(2,4,6,8) combn(c(2,4,6,8),2) # find all possible combinations of these numbers, drawn two at a time ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 2 2 2 4 4 6 ## [2,] 4 6 8 6 8 8 9.5 Download files from the internet 9.6 Print multiple things in one statement Use cat() from base R cat(&quot;The p-value dropped below 0.05 for the first time as sample size&quot;, 100) ## The p-value dropped below 0.05 for the first time as sample size 100 "],["intermediate-r-functions-loops-and-iterative-programming.html", "Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions 10.2 For-loops 10.3 purrr and Iterative Functions 10.4 Other purrr commands 10.5 Using purrr to manage many models", " Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions A function is a command that performs a specified operation and returns an output in accordance with that operation. You can literally make a function to do anything you want. General structure of a basic function: # example structure Function_name=function(argument){ Expressions return(output) } Argument is your input. It is the thing you want to perform the operation on. Expressions is the actual operation (or operations) you want to perform on the supplied argument return tells R to return the result of the Expression to you when done. This example function takes an input of numbers in the form of a vector and subtracts two from each. numbers=c(2,10,12,80) sub_2=function(x){ result= x - 2 return(result) } sub_2(numbers) ## [1] 0 8 10 78 We can also supply the function with a single number and it still works sub_2(100) ## [1] 98 Well this looks useful. So whats the bigger picture? One of the primary advantages of functions are that they can reduce a long and complex process, or a process that involves many steps, into a single line of code; thus, creating your own functions is a fast way to make your life easier down the line either at some point in the far future or even in just a few minutes, if you know you will be writing the code for some process two or more times. Take this script for instance. You can see from the circled parts that I needed to transform three different data sets in a similar way: knitr::include_graphics(here::here(&quot;pics&quot;, &quot;repeat_process.jpg&quot;)) Yes, I could have just done a copy-paste of the original code and tweak it slightly each time. But that is time consuming, produces a sloppier and longer script, and introduces a lot more room for error because of the repeated code and extra steps. Better to write a single function that could be applied to all three. In short, use functions to reduce a multi-step process or a process that youre implementing &gt;=2 times in a single script into one command. This saves you space and makes the script shorter; it saves you the trouble and effort of re-writing or adapting code from earlier sections; and importantly, reduces the chances of you making a coding error by proxy of the former two. As a quick example, I was able to replace each of the circled paragraphs of code above with a custom function that ran everything in one simple line. Now instead of 3 whole (and redundant) paragraphs, I now have 3 short lines, like so. na_zero_helpreint=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;reintegrate_&quot;) na_blank=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) na_zero=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) Limitations to your average, everyday functions. While reducing a whole process or sequence of commands is extremely useful, it still leaves a limitation. For instance, while we avoided copying and pasting whole paragraphs or processes, I still had to copy-paste the same function three times. This still leaves chances for error on the table, and it still leaves us with wasted lines that make the script longer. In general, when you want to perform some function or process multiple times on multiple items (as above where the same command is used three times on three different data frames), you need to use a for-loop or iterating function. These can reduce further unwanted redundancies by applying the function or process iteratively. Read on for more info. 10.2 For-loops A for loop is essentially a function that applies a function or given set of operations to multiple things at once, and returns an output of many items. For example, this code finds the means of every vector/column in a dataset by repeatedly applying the same code over and over to element i in the given list: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) output &lt;- vector(&quot;double&quot;, ncol(df)) # 1.Output. Create the object you want the results of the loop stored in. for (i in seq_along(df)) { # 2.Sequence of operations. &quot;For each item &#39;i&#39; along data frame&quot; output[[i]] &lt;- median(df[[i]]) # 3.Body:&quot;every individual item in &#39;output&#39; = the median of each col in df } output ## [1] 0.3771802 -0.5176346 0.4171879 0.5704655 Check out this book chapter for a great and detailed explanation of for-loops and functional coding. Although for loops are nice, they are unwieldy. R programmers typically use iterating functions instead. Examples of iterating functions are the lapply, vapply, sapply, etc. family of base R commands. But these can also be confusing and the commands are not great. The purrr package offers a better way to do iterating functions over base R; its the tidyverse way to make efficient and understandable for loops! If you have a need for a for-loop for something, see the next section instead on how to use purrr to make an iterative function. Important to understand conceptually what a for-loop is, but using them is impractical when you have purrr 10.3 purrr and Iterative Functions All notes here come from Charlotte Wickhams lecture tutorial below Part 1: https://www.youtube.com/watch?v=7UlWJWfZO9M Part 2: https://www.youtube.com/watch?v=b0ozKTUho0A&amp;t=1210s purrrs map() series of functions offer a way to apply any existing function (even functions youve made) to multiple things at once, be it lists, data frame columns, individual items in vector, etc. In short, they are for doing the same type of task repeatedly in a very quick and efficient manner. They work in much the same way as for-loops, but are far simpler to write, and can be applied in the same way to solve the same problems. How to use purrr The structure of map() commands is the same as the others in the tidyverse: #option 1 map(data, function) # option 2 data %&gt;% map(function) As a quick example and to highlight why purrr is so much more efficient and easier to use than for-loops, look at the same example from before, now using map() instead of a for: df |&gt; map_dbl(median) ## a b c d ## 0.3771802 -0.5176346 0.4171879 0.5704655 A single line is all it took to get the same results! And, it follows tidyverse grammar structure. Now lets get into how it works. map() commands work like this: For each element of x, do f. So if you pass it object x and object x is. - A vector, it will perform function f on every item in the vector - A data frame, it will perform function f on every column in the data frame - A list, it will perform function f on every level in the list Etc., etc.; the point is it applies a function repeatedly to every element in the object you supply it with. So lets walk through a case example. 10.3.1 Reproducible example: Scraping web data This is an example walk through showing how we can use purrr to speed things up dramatically and/or reduce the use of unwanted, extra code in our scripts. In this guide Ill be building a table of LPGA Tour statistics from multiple webpages. The workflow for purrr goes like this: First, you want to figure out how to do each step of your process line-by-line, for a single item. The idea is to try and walk through each step of the process and see exactly what will need to be done each each step and what the code will like, before trying to code it all at once at a higher level. Once you have each step for the first item figured out, then you make functions for each step that condense that code down to one command. Lastly, apply each function from your individual steps to all items in your list by using purr::map(). Do for One library(rvest) # STEP 1 # Figure out a line-by-line process for one item/one single web page html1=read_html(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; html_table(fill = TRUE, header=TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() head(html1) ## rank name distance ## 1 1 Emily Pedersen 281.768 ## 2 2 Nanna Koerstz Madsen 276.758 ## 3 3 Maude-Aimee Leblanc 275.393 ## 4 4 Yuka Saso 274.671 ## 5 5 Bianca Pagdanganan 274.640 ## 6 6 Madelene Sagstrom 274.488 # STEP 2 # create a custom function of the above to shorten and generalize the process quick_read_html=function(url){ web_page=read_html(url) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; # fortunately this node works for all four pages so it can be baked into the function html_table(fill = TRUE, header = TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() return(web_page) } # test to verify it works test=quick_read_html(url= &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;) head(test) # nice ## rank name putt_average ## 1 1 Jeong Eun Lee 1.706 ## 2 2 Lydia Ko 1.719 ## 3 3 Danielle Kang 1.720 ## 4 4 Minjee Lee 1.726 ## 5 5 Patty Tavatanakit 1.727 ## 6 6 Xiyu Lin 1.738 DO FOR ALL. Now create the object that contains all the elements you want to iterate over, and then pass it to your generalized function with map. # Step 3a # create an object that contains ALL elements of interest URLs=c(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=06&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=12&quot;) # Step 4 # use the power of map and be amazed lpga_data= URLs |&gt; map(quick_read_html) head(lpga_data) ## [[1]] ## rank name distance ## 1 1 Emily Pedersen 281.768 ## 2 2 Nanna Koerstz Madsen 276.758 ## 3 3 Maude-Aimee Leblanc 275.393 ## 4 4 Yuka Saso 274.671 ## 5 5 Bianca Pagdanganan 274.640 ## 6 6 Madelene Sagstrom 274.488 ## 7 7 A Lim Kim 274.075 ## 8 8 Lexi Thompson 273.463 ## 9 9 Pauline Roussin-Bouchard 273.333 ## 10 10 Maria Fassi 273.295 ## 11 11 Patty Tavatanakit 272.773 ## 12 12 Angel Yin 272.745 ## 13 13 Carlota Ciganda 272.394 ## 14 14 Brooke Matthews 272.208 ## 15 15 Yu Liu 271.848 ## 16 16 Atthaya Thitikul 271.756 ## 17 17 Charley Hull 271.519 ## 18 18 Brooke Henderson 271.174 ## 19 19 Minjee Lee 270.938 ## 20 20 Alana Uriell 270.783 ## 21 21 Janie Jackson 270.760 ## 22 22 Jessica Korda 270.675 ## 23 23 Rachel Rohanna 270.488 ## 24 24 Hannah Green 270.406 ## 25 25 Amanda Doherty 269.827 ## 26 26 Perrine Delacour 269.375 ## 27 27 Nelly Korda 269.200 ## 28 28 Jennifer Kupcho 269.063 ## 29 29 Yealimi Noh 267.986 ## 30 30 Stephanie Meadow 267.883 ## 31 31 Weiwei Zhang 267.850 ## 32 32 Gaby Lopez 266.984 ## 33 33 Hyejin Choi 266.592 ## 34 34 Sei Young Kim 266.591 ## 35 35 Albane Valenzuela 266.561 ## 36 36 Frida Kinhult 266.190 ## 37 37 Alison Lee 265.782 ## 38 38 Gerina Mendoza 265.521 ## 39 39 Ally Ewing 265.517 ## 40 40 Ryann O&#39;Toole 265.218 ## 41 41 Xiyu Lin 264.962 ## 42 42 Georgia Hall 264.855 ## 43 43 Jeong Eun Lee 264.732 ## 44 44 Pajaree Anannarukarn 264.451 ## 45 45 Fatima Fernandez Cano 264.324 ## 46 46 Sanna Nuutinen 264.281 ## 47 47 Nasa Hataoka 264.193 ## 48 48 Lauren Coughlin 264.167 ## 49 49 Cydney Clanton 263.977 ## 50 50 Amy Yang 263.953 ## 51 51 Lilia Vu 263.806 ## 52 52 Giulia Molinaro 263.266 ## 53 53 Sarah Schmelzel 263.122 ## 54 54 Sophia Schubert 263.045 ## 55 55 Hyo Joo Kim 262.750 ## 56 56 Ruixin Liu 262.375 ## 57 57 Sung Hyun Park 261.280 ## 58 58 Lauren Stephenson 261.117 ## 59 59 Haylee Harford 261.071 ## 60 60 Brittany Lincicome 260.542 ## 61 61 Dewi Weber 260.240 ## 62 62 Annie Park 260.177 ## 63 63 Ariya Jutanugarn 260.108 ## 64 64 Savannah Vilaubi 260.000 ## 65 65 Agathe Laisne 259.875 ## 66 66 Katherine Perry-Hamski 259.860 ## 67 67 Eun-Hee Ji 259.808 ## 68 68 Hinako Shibuno 259.710 ## 69 69 Min Lee 259.707 ## 70 70 Mel Reid 259.703 ## 71 71 Stephanie Kyriacou 259.529 ## 72 72 Jodi Ewart Shadoff 259.524 ## 73 73 Katherine Kirk 259.286 ## 74 74 Jaye Marie Green 259.183 ## 75 75 Peiyun Chien 259.100 ## 76 76 Isi Gabsa 259.000 ## 77 78 Sophia Popov 258.250 ## 78 79 Lauren Hartlage 258.219 ## 79 80 Na Rin An 258.164 ## 80 81 Gina Kim 257.654 ## 81 82 Austin Ernst 257.472 ## 82 83 Matilda Castren 257.319 ## 83 84 Allisen Corpuz 257.239 ## 84 85 Ruoning Yin 257.188 ## 85 86 Esther Henseleit 257.115 ## 86 87 Kaitlyn Papp 256.952 ## 87 88 Wei-Ling Hsu 256.846 ## 88 89 Lydia Ko 256.564 ## 89 90 Amy Olson 256.509 ## 90 91 Gemma Dryburgh 256.480 ## 91 92 Casey Danielson 256.222 ## 92 93 Sarah Kemp 256.179 ## 93 94 Pernilla Lindberg 255.969 ## 94 95 Brittany Lang 255.912 ## 95 96 Linnea Johansson 255.900 ## 96 97 Jennifer Song 255.879 ## 97 98 Ashleigh Buhai 255.617 ## 98 99 Yu-Sang Hou 255.286 ## 99 100 Celine Boutier 254.978 ## 100 101 Elizabeth Nagel 254.844 ## 101 102 Wichanee Meechai 254.736 ## 102 103 Megan Khang 254.459 ## 103 104 Angela Stanford 254.444 ## 104 105 Ana Belac 254.292 ## 105 106 Paula Reto 254.221 ## 106 107 Jenny Shin 254.032 ## 107 108 Jin Young Ko 253.855 ## 108 109 Kelly Tan 253.609 ## 109 110 Jenny Coleman 253.545 ## 110 111 Morgane Metraux 253.476 ## 111 112 In Gee Chun 253.413 ## 112 113 Leona Maguire 253.064 ## 113 114 Su-Hyun Oh 252.946 ## 114 115 Sarah Jane Smith 252.682 ## 115 116 Cristie Kerr 252.433 ## 116 117 Hee Young Park 252.423 ## 117 118 Jennifer Chang 252.111 ## 118 119 Bronte Law 252.103 ## 119 120 Pornanong Phatlum 251.893 ## 120 121 Danielle Kang 251.471 ## 121 122 Emma Talley 251.314 ## 122 123 Jasmine Suwannapura 251.306 ## 123 124 So Yeon Ryu 251.229 ## 124 125 Brittany Altomare 251.081 ## 125 126 Anna Nordqvist 250.721 ## 126 127 Marina Alex 250.663 ## 127 128 Lauren Kim 250.500 ## 128 129 Cheyenne Knight 250.409 ## 129 130 Chella Choi 250.286 ## 130 131 Muni He 250.262 ## 131 132 Ayaka Furue 249.863 ## 132 133 Robynn Ree 249.577 ## 133 134 Youngin Chun 249.000 ## 134 135 Haeji Kang 248.955 ## 135 136 Andrea Lee 248.500 ## 136 137 Moriya Jutanugarn 248.027 ## 137 138t Mariajo Uribe 247.438 ## 138 138t Lindsey Weaver 247.438 ## 139 140 Mirim Lee 247.111 ## 140 141 Allison Emrey 246.479 ## 141 142 Christina Kim 246.350 ## 142 143 Maddie Szeryk 246.292 ## 143 144 Mina Harigae 246.000 ## 144 145 Caroline Masson 245.484 ## 145 146 Jiwon Jeon 245.188 ## 146 147 Dottie Ardina 245.077 ## 147 148 In-Kyung Kim 244.556 ## 148 149 Lizette Salas 244.291 ## 149 150 Caroline Inglis 243.000 ## 150 151 Inbee Park 242.552 ## 151 152 Na Yeon Choi 242.304 ## 152 153 Yae Eun Hong 242.022 ## 153 154 Marissa Steen 241.452 ## 154 155 Mariah Stackhouse 241.083 ## 155 156 Stacy Lewis 240.316 ## 156 157 Olivia Cowan 239.375 ## 157 158 Charlotte Thomas 239.208 ## 158 159 Vivian Hou 238.125 ## 159 160 Aditi Ashok 235.375 ## 160 161 Dana Finkelstein 233.938 ## ## [[2]] ## rank name putt_average ## 1 1 Jeong Eun Lee 1.706 ## 2 2 Lydia Ko 1.719 ## 3 3 Danielle Kang 1.720 ## 4 4 Minjee Lee 1.726 ## 5 5 Patty Tavatanakit 1.727 ## 6 6 Xiyu Lin 1.738 ## 7 7 Celine Boutier 1.739 ## 8 8 Yuka Saso 1.741 ## 9 9 Su-Hyun Oh 1.742 ## 10 10 Nasa Hataoka 1.744 ## 11 11 Amy Yang 1.755 ## 12 12 Madelene Sagstrom 1.755 ## 13 13 Jessica Korda 1.757 ## 14 14 Nanna Koerstz Madsen 1.759 ## 15 15 Leona Maguire 1.760 ## 16 16 Carlota Ciganda 1.762 ## 17 17 Amanda Doherty 1.764 ## 18 18 Hyo Joo Kim 1.765 ## 19 19 Gemma Dryburgh 1.765 ## 20 20 Atthaya Thitikul 1.766 ## 21 21 Jin Young Ko 1.767 ## 22 22 Inbee Park 1.769 ## 23 23 Cristie Kerr 1.772 ## 24 24 Yae Eun Hong 1.773 ## 25 25 Brooke Henderson 1.774 ## 26 26 Chella Choi 1.774 ## 27 27 Georgia Hall 1.775 ## 28 28 Sung Hyun Park 1.780 ## 29 29 Angel Yin 1.781 ## 30 30 Haeji Kang 1.782 ## 31 31 Lexi Thompson 1.784 ## 32 32 Nelly Korda 1.785 ## 33 33 Stephanie Kyriacou 1.786 ## 34 34 Perrine Delacour 1.787 ## 35 35 Charley Hull 1.789 ## 36 36 Hannah Green 1.789 ## 37 37 Alison Lee 1.790 ## 38 38 Ryann O&#39;Toole 1.791 ## 39 39 Ruoning Yin 1.791 ## 40 40 Brittany Altomare 1.791 ## 41 41 Megan Khang 1.791 ## 42 42 Hyejin Choi 1.791 ## 43 43 Lauren Stephenson 1.791 ## 44 44 Jenny Shin 1.791 ## 45 45 Isi Gabsa 1.792 ## 46 46 Aditi Ashok 1.793 ## 47 47 Allisen Corpuz 1.793 ## 48 48 Hinako Shibuno 1.797 ## 49 49 Sarah Schmelzel 1.797 ## 50 50 Ayaka Furue 1.798 ## 51 51 Pajaree Anannarukarn 1.798 ## 52 52 Jasmine Suwannapura 1.799 ## 53 53 Pauline Roussin-Bouchard 1.799 ## 54 54 Lindsey Weaver 1.799 ## 55 55 Annie Park 1.799 ## 56 56 Gaby Lopez 1.800 ## 57 57 Na Rin An 1.803 ## 58 58 Lizette Salas 1.804 ## 59 59 Mina Harigae 1.804 ## 60 60 Janie Jackson 1.805 ## 61 61t Stacy Lewis 1.805 ## 62 61t Sarah Jane Smith 1.805 ## 63 63 Emma Talley 1.807 ## 64 64 Lilia Vu 1.807 ## 65 65 In Gee Chun 1.808 ## 66 66 Paula Reto 1.808 ## 67 67t Frida Kinhult 1.809 ## 68 67t Sophia Popov 1.809 ## 69 69 A Lim Kim 1.810 ## 70 70 Sei Young Kim 1.810 ## 71 71 Brittany Lincicome 1.811 ## 72 72 Marina Alex 1.811 ## 73 73 Eun-Hee Ji 1.812 ## 74 74 Katherine Kirk 1.812 ## 75 75 Kaitlyn Papp 1.814 ## 76 76 Min Lee 1.815 ## 77 77 Yu Liu 1.815 ## 78 78 Kelly Tan 1.816 ## 79 79t Morgane Metraux 1.818 ## 80 79t Giulia Molinaro 1.818 ## 81 81 Moriya Jutanugarn 1.818 ## 82 82 Jennifer Chang 1.819 ## 83 83 Jennifer Kupcho 1.819 ## 84 84 Cheyenne Knight 1.821 ## 85 85 Alana Uriell 1.821 ## 86 86 So Yeon Ryu 1.822 ## 87 87 Ariya Jutanugarn 1.822 ## 88 88 Linnea Johansson 1.824 ## 89 89 In-Kyung Kim 1.824 ## 90 90 Stephanie Meadow 1.825 ## 91 91 Maude-Aimee Leblanc 1.826 ## 92 92 Ashleigh Buhai 1.828 ## 93 93 Matilda Castren 1.828 ## 94 94 Angela Stanford 1.830 ## 95 95 Pernilla Lindberg 1.830 ## 96 96 Esther Henseleit 1.830 ## 97 97 Sanna Nuutinen 1.830 ## 98 98 Jodi Ewart Shadoff 1.832 ## 99 99 Ally Ewing 1.832 ## 100 101 Andrea Lee 1.833 ## 101 102 Bronte Law 1.834 ## 102 103 Albane Valenzuela 1.835 ## 103 104 Cydney Clanton 1.836 ## 104 105 Amy Olson 1.838 ## 105 106 Wei-Ling Hsu 1.840 ## 106 107 Jiwon Jeon 1.841 ## 107 108 Dottie Ardina 1.841 ## 108 109 Muni He 1.841 ## 109 110 Ana Belac 1.847 ## 110 111 Hee Young Park 1.847 ## 111 112 Charlotte Thomas 1.848 ## 112 113 Maria Fassi 1.848 ## 113 114 Yealimi Noh 1.849 ## 114 115 Brittany Lang 1.851 ## 115 116 Anna Nordqvist 1.851 ## 116 117 Bianca Pagdanganan 1.851 ## 117 118 Jennifer Song 1.851 ## 118 119 Sophia Schubert 1.851 ## 119 120 Dewi Weber 1.851 ## 120 121 Agathe Laisne 1.852 ## 121 122 Caroline Masson 1.853 ## 122 123 Katherine Perry-Hamski 1.853 ## 123 124 Yu-Sang Hou 1.857 ## 124 125 Allison Emrey 1.858 ## 125 126 Mariah Stackhouse 1.859 ## 126 127 Youngin Chun 1.860 ## 127 128 Dana Finkelstein 1.862 ## 128 129 Wichanee Meechai 1.865 ## 129 130 Jaye Marie Green 1.865 ## 130 131 Jenny Coleman 1.865 ## 131 132 Austin Ernst 1.865 ## 132 133 Fatima Fernandez Cano 1.866 ## 133 134 Sarah Kemp 1.868 ## 134 135 Weiwei Zhang 1.868 ## 135 136 Emily Pedersen 1.869 ## 136 137 Lauren Kim 1.870 ## 137 138 Ruixin Liu 1.870 ## 138 139 Gerina Mendoza 1.871 ## 139 140 Rachel Rohanna 1.872 ## 140 141 Elizabeth Nagel 1.874 ## 141 142 Peiyun Chien 1.874 ## 142 143 Na Yeon Choi 1.875 ## 143 144 Lauren Coughlin 1.876 ## 144 145 Christina Kim 1.878 ## 145 146 Gina Kim 1.879 ## 146 147 Mirim Lee 1.880 ## 147 148 Pornanong Phatlum 1.886 ## 148 149 Mel Reid 1.887 ## 149 150 Haylee Harford 1.888 ## 150 151 Lauren Hartlage 1.888 ## 151 152 Olivia Cowan 1.895 ## 152 153 Maddie Szeryk 1.905 ## 153 154 Mariajo Uribe 1.909 ## 154 155 Savannah Vilaubi 1.912 ## 155 156 Marissa Steen 1.914 ## 156 157 Robynn Ree 1.918 ## 157 158 Caroline Inglis 1.928 ## 158 159 Brooke Matthews 1.929 ## 159 160 Casey Danielson 1.939 ## 160 161 Vivian Hou 1.962 ## ## [[3]] ## rank name greens_hit ## 1 1 Lexi Thompson 76.1 ## 2 2 Sanna Nuutinen 75.7 ## 3 3 Xiyu Lin 75.6 ## 4 4 Hyejin Choi 75.6 ## 5 5 Jodi Ewart Shadoff 75.3 ## 6 6 Jennifer Kupcho 75.3 ## 7 7 Brooke Henderson 75.2 ## 8 8 Minjee Lee 74.8 ## 9 9 Megan Khang 74.0 ## 10 10 Hannah Green 74.0 ## 11 11 Jin Young Ko 73.8 ## 12 12 Celine Boutier 73.3 ## 13 13 Jeong Eun Lee 73.2 ## 14 14 Nasa Hataoka 73.1 ## 15 15 A Lim Kim 73.1 ## 16 16 Hyo Joo Kim 73.0 ## 17 17 Ally Ewing 73.0 ## 18 18 Matilda Castren 72.8 ## 19 19 Emily Pedersen 72.8 ## 20 20 Marina Alex 72.8 ## 21 21 Lauren Coughlin 72.8 ## 22 22 Nanna Koerstz Madsen 72.7 ## 23 23 Sarah Schmelzel 72.5 ## 24 24 Yealimi Noh 72.4 ## 25 25 Hinako Shibuno 72.0 ## 26 26 Pajaree Anannarukarn 72.0 ## 27 27 Perrine Delacour 71.9 ## 28 28 Charley Hull 71.8 ## 29 29 Chella Choi 71.7 ## 30 30t In Gee Chun 71.5 ## 31 30t Amy Yang 71.5 ## 32 32 Atthaya Thitikul 71.2 ## 33 33 Casey Danielson 71.0 ## 34 34 Caroline Inglis 70.9 ## 35 35 Lauren Stephenson 70.9 ## 36 36 Jessica Korda 70.8 ## 37 37 Ryann O&#39;Toole 70.8 ## 38 38 Nelly Korda 70.7 ## 39 39 Ariya Jutanugarn 70.7 ## 40 40 Brittany Altomare 70.4 ## 41 41 Patty Tavatanakit 70.4 ## 42 42t Danielle Kang 70.1 ## 43 42t Anna Nordqvist 70.1 ## 44 44 Ayaka Furue 70.0 ## 45 45 Sophia Schubert 69.7 ## 46 46 Madelene Sagstrom 69.6 ## 47 47 Maude-Aimee Leblanc 69.6 ## 48 48 Gaby Lopez 69.6 ## 49 49 Min Lee 69.5 ## 50 50t Ruixin Liu 69.4 ## 51 50t Inbee Park 69.4 ## 52 52 Sei Young Kim 69.2 ## 53 53 Jenny Shin 69.1 ## 54 54 Stacy Lewis 69.0 ## 55 55 Austin Ernst 68.8 ## 56 56 Lilia Vu 68.8 ## 57 57t Andrea Lee 68.8 ## 58 57t So Yeon Ryu 68.8 ## 59 57t Kelly Tan 68.8 ## 60 60 Jaye Marie Green 68.7 ## 61 61 Carlota Ciganda 68.7 ## 62 62 Moriya Jutanugarn 68.6 ## 63 63 Peiyun Chien 68.6 ## 64 64 Caroline Masson 68.6 ## 65 65t Lydia Ko 68.4 ## 66 65t Leona Maguire 68.4 ## 67 67 Albane Valenzuela 68.4 ## 68 68 Kaitlyn Papp 68.3 ## 69 69 Janie Jackson 68.2 ## 70 70 Ruoning Yin 68.1 ## 71 71 Bronte Law 68.0 ## 72 72t Wei-Ling Hsu 67.9 ## 73 72t Robynn Ree 67.9 ## 74 74 Cydney Clanton 67.9 ## 75 75 Isi Gabsa 67.9 ## 76 76 Yuka Saso 67.7 ## 77 77 Allisen Corpuz 67.6 ## 78 78 Jasmine Suwannapura 67.6 ## 79 79t Jenny Coleman 67.5 ## 80 79t Yu Liu 67.5 ## 81 81t Sarah Kemp 67.5 ## 82 81t Emma Talley 67.5 ## 83 84 Brittany Lincicome 67.4 ## 84 85 Dewi Weber 67.3 ## 85 86 Paula Reto 67.2 ## 86 87 Gina Kim 67.1 ## 87 88 Haylee Harford 67.1 ## 88 89 Jennifer Song 67.1 ## 89 90 Annie Park 67.0 ## 90 91 Charlotte Thomas 66.9 ## 91 92 Eun-Hee Ji 66.9 ## 92 93 Giulia Molinaro 66.8 ## 93 94t Stephanie Meadow 66.7 ## 94 94t Katherine Perry-Hamski 66.7 ## 95 96 Alison Lee 66.5 ## 96 97 Na Rin An 66.5 ## 97 98 Wichanee Meechai 66.2 ## 98 99 Alana Uriell 66.2 ## 99 100 Mina Harigae 66.0 ## 100 101t Olivia Cowan 66.0 ## 101 101t Elizabeth Nagel 66.0 ## 102 103 Lizette Salas 65.9 ## 103 104 Bianca Pagdanganan 65.8 ## 104 105 Cheyenne Knight 65.7 ## 105 106t Ashleigh Buhai 65.6 ## 106 106t Sung Hyun Park 65.6 ## 107 108 Esther Henseleit 65.4 ## 108 109 Frida Kinhult 65.3 ## 109 110t Pernilla Lindberg 65.3 ## 110 110t Su-Hyun Oh 65.3 ## 111 112 Georgia Hall 65.2 ## 112 113 Lauren Hartlage 64.9 ## 113 114 Maria Fassi 64.9 ## 114 115 Gerina Mendoza 64.6 ## 115 116 Dottie Ardina 64.5 ## 116 117t Pornanong Phatlum 64.3 ## 117 117t Rachel Rohanna 64.3 ## 118 117t Marissa Steen 64.3 ## 119 120 Lindsey Weaver 64.1 ## 120 121 Stephanie Kyriacou 64.1 ## 121 122 Morgane Metraux 64.0 ## 122 123 Christina Kim 63.9 ## 123 124 Na Yeon Choi 63.8 ## 124 125 Dana Finkelstein 63.7 ## 125 126 Haeji Kang 63.6 ## 126 127t Amy Olson 63.6 ## 127 127t Pauline Roussin-Bouchard 63.6 ## 128 129 Agathe Laisne 63.5 ## 129 130 Sophia Popov 63.5 ## 130 131 Ana Belac 63.4 ## 131 132t Fatima Fernandez Cano 63.4 ## 132 132t Brittany Lang 63.4 ## 133 134t Gemma Dryburgh 63.3 ## 134 134t Weiwei Zhang 63.3 ## 135 136 Angel Yin 63.3 ## 136 137 In-Kyung Kim 63.3 ## 137 138 Amanda Doherty 63.2 ## 138 139 Mel Reid 63.2 ## 139 140 Linnea Johansson 63.1 ## 140 141 Hee Young Park 63.0 ## 141 142 Jennifer Chang 63.0 ## 142 143 Muni He 61.6 ## 143 144t Jiwon Jeon 61.1 ## 144 144t Mariajo Uribe 61.1 ## 145 146 Katherine Kirk 60.8 ## 146 147 Yae Eun Hong 60.6 ## 147 148 Aditi Ashok 60.2 ## 148 149 Vivian Hou 60.2 ## 149 150 Cristie Kerr 60.0 ## 150 151 Sarah Jane Smith 59.6 ## 151 152 Mariah Stackhouse 59.3 ## 152 153 Mirim Lee 59.0 ## 153 154 Savannah Vilaubi 58.9 ## 154 155 Brooke Matthews 58.8 ## 155 156 Lauren Kim 58.7 ## 156 157 Yu-Sang Hou 58.3 ## 157 158 Youngin Chun 57.4 ## 158 159 Allison Emrey 55.3 ## 159 160 Angela Stanford 54.3 ## 160 161 Maddie Szeryk 53.7 ## ## [[4]] ## rank name rounds score_average_actual ## 1 1 Minjee Lee 32 68.750 ## 2 2 Jin Young Ko 31 69.548 ## 3 3 Lexi Thompson 27 69.630 ## 4 4 Hyo Joo Kim 22 69.682 ## 5 5 Brooke Henderson 35 69.686 ## 6 6 Xiyu Lin 39 69.744 ## 7 7 Hyejin Choi 38 69.816 ## 8 8 Nanna Koerstz Madsen 33 69.848 ## 9 9 Celine Boutier 46 69.870 ## 10 10 Atthaya Thitikul 43 69.884 ## 11 11 Lydia Ko 39 69.897 ## 12 12 Nasa Hataoka 44 70.114 ## 13 13 Hannah Green 32 70.125 ## 14 14 Nelly Korda 15 70.133 ## 15 15 Danielle Kang 34 70.206 ## 16 16 Amy Yang 32 70.250 ## 17 17 Patty Tavatanakit 33 70.303 ## 18 18 Madelene Sagstrom 41 70.317 ## 19 19 Inbee Park 34 70.441 ## 20 20 Chella Choi 35 70.514 ## 21 21 Jeong Eun Lee 28 70.536 ## 22 22 Megan Khang 37 70.541 ## 23 23 In Gee Chun 40 70.575 ## 24 24 Marina Alex 40 70.625 ## 25 25 Yuka Saso 38 70.684 ## 26 26 Sarah Schmelzel 37 70.730 ## 27 27 Pajaree Anannarukarn 41 70.756 ## 28 28t Leona Maguire 39 70.795 ## 29 28t Ryann O&#39;Toole 39 70.795 ## 30 30 Jennifer Kupcho 40 70.825 ## 31 31 Charley Hull 26 70.923 ## 32 32 Sanna Nuutinen 16 70.938 ## 33 33 Hinako Shibuno 31 70.968 ## 34 34 A Lim Kim 40 71.050 ## 35 35 Brittany Altomare 37 71.054 ## 36 36 Andrea Lee 16 71.063 ## 37 37 Alison Lee 39 71.077 ## 38 38 Jodi Ewart Shadoff 32 71.156 ## 39 39 Jasmine Suwannapura 36 71.167 ## 40 40 Stacy Lewis 38 71.184 ## 41 41t Perrine Delacour 32 71.188 ## 42 41t Jenny Shin 32 71.188 ## 43 43 Jessica Korda 20 71.200 ## 44 44 Ayaka Furue 40 71.225 ## 45 45 Gemma Dryburgh 25 71.240 ## 46 46 Allisen Corpuz 23 71.261 ## 47 47 Albane Valenzuela 33 71.273 ## 48 48 Georgia Hall 31 71.290 ## 49 49 Kelly Tan 32 71.313 ## 50 50 Su-Hyun Oh 28 71.321 ## 51 51 Cheyenne Knight 33 71.333 ## 52 52 Gaby Lopez 32 71.344 ## 53 53 Mina Harigae 34 71.353 ## 54 54 Sei Young Kim 22 71.364 ## 55 55 Carlota Ciganda 33 71.394 ## 56 56 Emma Talley 35 71.400 ## 57 57 So Yeon Ryu 24 71.458 ## 58 58 Lizette Salas 28 71.464 ## 59 59 Lauren Stephenson 30 71.467 ## 60 60 Paula Reto 34 71.471 ## 61 61 Min Lee 29 71.483 ## 62 62 Lilia Vu 31 71.484 ## 63 63 Eun-Hee Ji 26 71.577 ## 64 65 Lindsey Weaver 32 71.625 ## 65 66 Anna Nordqvist 34 71.647 ## 66 67 Moriya Jutanugarn 37 71.649 ## 67 68 Bronte Law 29 71.655 ## 68 69 Kaitlyn Papp 21 71.667 ## 69 70 Aditi Ashok 32 71.688 ## 70 71 Frida Kinhult 29 71.724 ## 71 72 Ariya Jutanugarn 37 71.730 ## 72 73 Wei-Ling Hsu 26 71.731 ## 73 74 Alana Uriell 23 71.739 ## 74 75 Ruixin Liu 24 71.750 ## 75 76 Stephanie Meadow 30 71.767 ## 76 77 Annie Park 31 71.774 ## 77 78 Ally Ewing 30 71.800 ## 78 79 Isi Gabsa 28 71.821 ## 79 80t Jennifer Chang 18 71.833 ## 80 80t Brittany Lincicome 24 71.833 ## 81 82 Yu Liu 33 71.848 ## 82 83 Na Rin An 31 71.871 ## 83 84 Matilda Castren 36 71.889 ## 84 85 Yealimi Noh 37 71.892 ## 85 86 Charlotte Thomas 24 71.958 ## 86 87 Ruoning Yin 16 72.000 ## 87 88 Emily Pedersen 28 72.036 ## 88 89 Caroline Masson 32 72.063 ## 89 90 Robynn Ree 13 72.077 ## 90 91 Ashleigh Buhai 30 72.100 ## 91 92t Maude-Aimee Leblanc 28 72.107 ## 92 92t Angel Yin 28 72.107 ## 93 94 Yae Eun Hong 23 72.130 ## 94 95 Lauren Coughlin 21 72.143 ## 95 96 Dottie Ardina 13 72.154 ## 96 97 Giulia Molinaro 32 72.156 ## 97 98 Pauline Roussin-Bouchard 27 72.222 ## 98 99 Sophia Schubert 22 72.227 ## 99 100 Jennifer Song 29 72.345 ## 100 101 Amanda Doherty 26 72.385 ## 101 102 Wichanee Meechai 36 72.389 ## 102 103 Sarah Kemp 28 72.393 ## 103 104 Pernilla Lindberg 32 72.406 ## 104 105 Ana Belac 24 72.417 ## 105 106 Dewi Weber 25 72.440 ## 106 107t Austin Ernst 18 72.444 ## 107 107t In-Kyung Kim 18 72.444 ## 108 109 Dana Finkelstein 24 72.458 ## 109 110 Stephanie Kyriacou 17 72.471 ## 110 111t Caroline Inglis 13 72.538 ## 111 111t Gina Kim 13 72.538 ## 112 113 Sung Hyun Park 25 72.560 ## 113 114 Brittany Lang 17 72.588 ## 114 115t Cydney Clanton 22 72.591 ## 115 115t Maria Fassi 22 72.591 ## 116 115t Haeji Kang 22 72.591 ## 117 118 Jaye Marie Green 30 72.600 ## 118 119 Peiyun Chien 20 72.650 ## 119 120t Janie Jackson 25 72.680 ## 120 120t Katherine Perry-Hamski 25 72.680 ## 121 122 Esther Henseleit 26 72.692 ## 122 123 Sophia Popov 28 72.714 ## 123 124 Sarah Jane Smith 11 72.727 ## 124 125 Amy Olson 27 72.778 ## 125 126 Pornanong Phatlum 28 72.786 ## 126 127 Muni He 21 72.810 ## 127 128 Katherine Kirk 18 72.833 ## 128 129 Bianca Pagdanganan 25 72.840 ## 129 130 Jenny Coleman 33 72.848 ## 130 131 Cristie Kerr 15 72.933 ## 131 132 Rachel Rohanna 21 73.048 ## 132 133 Agathe Laisne 16 73.063 ## 133 134 Gerina Mendoza 24 73.125 ## 134 135 Morgane Metraux 21 73.143 ## 135 136 Christina Kim 20 73.150 ## 136 137 Mel Reid 32 73.188 ## 137 138 Linnea Johansson 25 73.200 ## 138 139 Casey Danielson 18 73.278 ## 139 140 Mariah Stackhouse 6 73.333 ## 140 141 Hee Young Park 26 73.346 ## 141 142 Haylee Harford 14 73.357 ## 142 143 Marissa Steen 21 73.429 ## 143 144 Jiwon Jeon 16 73.563 ## 144 145 Brooke Matthews 12 73.667 ## 145 146 Angela Stanford 18 73.833 ## 146 147 Elizabeth Nagel 16 73.938 ## 147 148 Fatima Fernandez Cano 17 73.941 ## 148 149 Olivia Cowan 8 74.125 ## 149 150 Yu-Sang Hou 14 74.214 ## 150 151t Lauren Hartlage 16 74.250 ## 151 151t Lauren Kim 16 74.250 ## 152 151t Mariajo Uribe 8 74.250 ## 153 154 Na Yeon Choi 23 74.261 ## 154 155 Savannah Vilaubi 15 74.400 ## 155 156 Weiwei Zhang 10 74.500 ## 156 157 Youngin Chun 9 74.556 ## 157 158 Allison Emrey 24 74.583 ## 158 159 Maddie Szeryk 12 74.917 ## 159 160 Mirim Lee 18 75.278 ## 160 161 Vivian Hou 12 76.000 All done!! And just like that, weve downloaded four different web pages, extracted the tabled info, and formatted them without copying and pasting any code. The same process for all four was only used one time to write the initial function. Just apply some final formatting to clean it up a bit and combine the separate data frames into a single, unified one. lpga_data= lpga_data %&gt;% reduce(left_join, by=&quot;name&quot;) %&gt;% # Combine all list levels into a single tibble, matching by the &quot;Name&quot; column select(-contains(&quot;rank.&quot;)) |&gt; rename(&quot;score_average&quot;=&quot;score_average_actual&quot;) # VOILA! head(lpga_data) ## name distance putt_average greens_hit rounds score_average ## 1 Emily Pedersen 281.768 1.869 72.8 28 72.036 ## 2 Nanna Koerstz Madsen 276.758 1.759 72.7 33 69.848 ## 3 Maude-Aimee Leblanc 275.393 1.826 69.6 28 72.107 ## 4 Yuka Saso 274.671 1.741 67.7 38 70.684 ## 5 Bianca Pagdanganan 274.640 1.851 65.8 25 72.840 ## 6 Madelene Sagstrom 274.488 1.755 69.6 41 70.317 10.3.2 Non-reproducible example (Juvenile Life Without Parole study) In the Juvenile Lifers study, there were a series of questions that participants rated on a scale of 0-100 in terms of difficulty. Part of our analysis involved taking the ratings on those variables and giving them relative rankings, so that each of the 6 variables in the series was rated from the least to most difficult, by participant. Now if we only needed to compute these rankings once this wouldnt have been any big deal; however, we needed to do it three times. Much of the same code and the same process would need to be copied and pasted, resulting in a very long, messy, harder to read script. With purrr however, we can reduce the redundancies to a minimum, saving time and reducing the chances of mistakes. Step 1. Just like before, the first step is to find a line-by-line solution for a single item, and then to generalize this into a shortcut function that can be applied to the any item i in a series of items. For the sake of brevity, Im going to skip most of that and just include the functions below. load(&quot;C:/Github Repos/Studies/JLWOP/Data and Models/jlwop_reentry_survey.RData&quot;) #### CREATE THE DATA SETS WE NEED#### na_blank=jlwop_reentry_survey # analysis 1 keeps the data as-is na_zero=jlwop_reentry_survey %&gt;% # supplementary analysis replaces the NA&#39;s with 0 mutate(across(c(barrier_housing:barrier_identification), replace_na,0)) rm(jlwop_reentry_survey) # remove old data set to avoid confusion #### Functions #### # transformation function to wrangle the data into proper formatting rotate_data=function(data, variable_prefix){ data=data %&gt;% pivot_longer( cols= starts_with(variable_prefix), # collect all the desired variables (i.e., columns).... names_to = &quot;variable&quot;, #...and put them into a new categorical variable called &quot;variable&quot; values_to = &quot;participant_score&quot;) %&gt;% # ...and store their values in a new variable called &quot;participant_score&quot; arrange(unique,participant_score) %&gt;% select(c(unique, participant_score, variable)) %&gt;% # keep only these 3 variables relocate(variable, .before = participant_score) # put the newly created variable up front return(data) } # creating the rankings for each variable; then transform data back to original structure rank_and_unpivot=function(data){ data=data %&gt;% group_by(unique) %&gt;% # group the scores so they can be ranked by participant mutate(rank1=dense_rank(participant_score), # create ranking variable rank=max(rank1,na.rm = TRUE) + 1 - rank1) %&gt;% # fix ranks by flipping to ascending order mutate(rank=factor(rank)) %&gt;% # convert rank to factor structure select(-rank1) # Pivot back to wide data=data %&gt;% pivot_wider(names_from = variable, values_from = rank:participant_score) %&gt;% ungroup() # un-group the data and delete the generated names return(data) } Step 2. Again, like before, we want to combine all elements of interest into some object. Once we have that, we then pass said object to map() and supply the map call with our custom function. dfs=list(na_blank=na_blank, na_zero=na_zero) %&gt;% # create lists map(.f=rotate_data, variable_prefix = &quot;barrier&quot;) %&gt;% # apply custom function along whole list map(rank_and_unpivot) # again!! DO IT AGAIN! With another function this time. # extract list elements to make them data frames again list2env(dfs, globalenv()) rm(dfs) #discard list. It has fulfilled its purpose. And just like that, were done! 10.3.3 Example 3: Read/Import several files at once with map() Multiple ways you can do this. ################### Option 1: read all into the global environment, keeping them as SEPERATE df&#39;s ############### legaldmlab::read_all(path=&quot;Data Repository/Stats Data Repository/JASP files&quot;, extension = &quot;.csv&quot;) # Option 1.A: Squish ALL OBJECTS in the working environment into a list # Again, note the &quot;ALL OBJECTS&quot; part; make sure there are no functions or other things in the environment when you run this. files=mget(ls()) ############# Option 2: Read in all files as a LIST of df&#39;s, then stitch in the names ##################################### files=paste0(here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;, &quot;/&quot;), list.files(path=here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;), pattern = &quot;.csv&quot;)) files_list=files |&gt; map(readr::read_csv) names(files_list)=file.path(here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;)) |&gt; #specify file path as a string list.files(pattern = &quot;.csv&quot;) |&gt; # pass the path string to list files; search in this location for files with this extension gsub(pattern=&quot;.csv&quot;, replacement = &quot;&quot;) # remove this pattern to save only the name # Option 2.A: Extract each data frame and put everything into the global environment list2env(cog_data, globalenv()) 10.4 Other purrr commands Note that map() always returns a list, and depending on the output that you want, you may need to use a variation of map(). These variations are as follows: Command Return map_lgl() logical vector map_int() integer vector map_dbl() double vector map_chr() character vector walk() only returns the side effects of a function 10.4.1 walk and walk2 Walk() is useful for when you just want to plot something or write a save file to your disk, etc. It does not give you any return to store something in the environment. You use it to write/read files, open graphics windows, and so on. Example: Writing multiple files at once Utilize purrr::walk2() to apply a function iteratively on TWO objects simultaneously. To save multiple .csv files with walk2, we need two distinct lists: 1. A list of data frames that we wish to export, 2. and the file paths, complete with the file names and extensions, for each file to be written. First create and define both list items. Then apply walk2() to pluck an element from list 1 and its corresponding element from list 2, and apply the write_csv function in for-loop fashion. # DEMO 1: Writing multiple plots at once # Create list one, the list of objects figs = list(scatter_plot=scatter_plot, multi_plot=multi_plot) # create list 2, the list of file names fig_names = figs |&gt; names() |&gt; map(paste0, &quot;.png&quot;) # pass both to purrr to use ggsave iteratively over both lists once and save all graphs with one command walk2(figs, fig_names, ~ggsave(plot = figs, filename = fig_names path=here::here(&quot;Figures and Tables&quot;), device = &quot;png&quot;, dpi = 300)) A second demo, this time using write.csv to save/export multiple CSV files at once # DEMO 2: Wrinting multiple .csv files at once ### Custom function #### # Create needed function that grabs file names and stitches them together with the correct path and extension # Included in legaldmlab package bundle_paths=function(df_list, output_location, file_type){ names=names(df_list) paths=rep(here::here(output_location), length(names)) extension=rep(c(file_type), length(names)) fixed_names=paste0(&quot;/&quot;,names) path_bundle=list(paths,fixed_names, extension) %&gt;% pmap(., paste0) return(path_bundle) } #### Exporting the .csv files for SPSS/JASP/etc. #### # Define list 1 dfs=list(na_blank=na_blank, na_zero=na_zero, na_zero_helpreint=na_zero_helpreint) # list 2 paths_csv=bundle_paths(df_list = dfs, folder_location = &quot;JLWOP/Data and Models&quot;, file_type = &quot;.sav&quot;) # Iterate over all elements in list 1 and corresponding element in list 2; # and apply the the write_csv function to each walk2(.x=dfs, .y= paths, .f=haven::write_sav) #### .RData file for R users #### # Combine multiple data frames into a single .RData file and export save(list = c(&quot;na_blank&quot;, &quot;na_zero&quot;, &quot;na_zero_helpreint&quot;), file = here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;,&quot;ranking_data.RData&quot;)) 10.4.2 map2 knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_a.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_b.png&quot;)) 10.4.3 pmap for when you have a bunch of shit This function is for iterating over three or more elements. As soon as you have &gt;2 items you have to iterate over, you need pmap(), which acts on a list object called .i instead of a list object. The list .i is a list of all the objects you want to iterate over. If you give it a list of 18 items, it iterates over all 18. If the list only has two things, it only acts on those two. She says its easiest to imagine the list as a data frame, and the columns of the data frame like the elements of that list. knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap_2.png&quot;)) 10.5 Using purrr to manage many models Below is the full script I copied from Hadley Wickhams lecture, which you can watch here pacman::p_load(dplyr,purrr,tidyverse,gapminder) #### Workflow for managing many models in R #### # 1. Nest data with {tidyr} # 2. Use {purrr} to map a modeling function # 3. Use {broom} to inspect your tidy data gapminder=gapminder %&gt;% mutate(year1950= year-1950) #the number of years it&#39;s been since 1950 #-------------------------------------------------------------------------------------------- #### Step 1. Nest the data. #### # A nested data frame has one column per country. You&#39;re essentially # creating a Russian doll; a data frame inside of a larger data frame. by_country=gapminder %&gt;% group_by(continent,country) %&gt;% # variables to keep at the top level nest() # smush everything else into a df, and store this mini-df in its own column # with this, you can have an entire table per row; a whole data frame for each country # Essentially condensing a list into a table by_country$data[[1]] #-------------------------------------------------------------------------------------------- #### Step 2. Use purrr to map stuff. #### # 12:50 country_model=function(df){ lm(lifeExp ~ year1950, data = df) } models= by_country %&gt;% mutate( mod=map(data,country_model) ) gapminder %&gt;% group_by(continent,country) %&gt;% nest() %&gt;% mutate( mod= data %&gt;% map(country_model) ) # 27:11 #-------------------------------------------------------------------------------------------- ##### Step 3. #### # This creates another nested df inside of your main data frame that has the summary stats of each model models=models %&gt;% mutate( tidy=map(mod, broom::tidy), # tidy() gives model estimates glance=map(mod,broom::glance), # glance() gives model summaries augment=map(mod,broom::augment) # model coefficients ) # What can you do with this nest of data frames? # The reverse of step 1; un-nest it to unpack everything! # 34:40 # Keeps a massive list of related information neatly organized! unnest(models,data) # back to where we started unnest(models,glance, .drop = TRUE) unnest(models,tidy) and here is a version I made of the above to manage many Latent Growth Curve models. # CONDENSED MASTER TABLE VERSION ----------------------------------------------------------------------------- # Models table that has all models condensed models_noCovs=tibble( #### Define model names #### model_name=c(&quot;Linear&quot;, &quot;Quadratic&quot;, &quot;Latent_Basis&quot;), ##### List model specifications for lavaan #### model_spec=list( linear_model= &#39; # intercept and slope with fixed coefficients i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 + 1*panss_total_3400 + 1*panss_total_5200 s =~ 0*panss_total_400 + 3*panss_total_1000 + 6*panss_total_1600 + 9*panss_total_2200 + 12*panss_total_2800 + 15*panss_total_3400 + 24*panss_total_5200 &#39;, quadratic_model= &#39; # intercept and slope with fixed coefficients i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 + 1*panss_total_3400 s =~ 0*panss_total_400 + 3*panss_total_1000 + 6*panss_total_1600 + 9*panss_total_2200 + 12*panss_total_2800 + 15*panss_total_3400 + 24*panss_total_5200 qs =~ 0*panss_total_400 + 9*panss_total_1000 + 36*panss_total_1600 + 81*panss_total_2200 + 144*panss_total_2800 + 225*panss_total_3400 + 576*panss_total_5200 &#39;, latentBasis_model= &#39; # intercept and slope with fixed coefficients i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 + 1*panss_total_3400 + 1*panss_total_5200 s =~ 0*panss_total_400 + NA*panss_total_1000 + NA*panss_total_1600 + NA*panss_total_2200 + NA*panss_total_2800 + NA*panss_total_3400 + 1*panss_total_5200 &#39; ), #### Fit all models at once with purrr #### fitted_model=model_spec |&gt; map(lavaan::growth, data=panss_sem_data, missing=&quot;FIML&quot;), ) #### Add parameter estimates and fit stats #### models_noCovs=models_noCovs |&gt; mutate(tidy_parameters=map(fitted_model, tidy, conf.int=.95), #parameter estimates global_fit=map(fitted_model, performance::model_performance)) #global fit of models #### Clean up stuff #### models_noCovs$tidy_parameters=models_noCovs$tidy_parameters |&gt; map(select,-c(std.lv:std.nox, op)) |&gt; # remove extra columns map(mutate, estimate=round(estimate, digits = 2)) |&gt; # round numbers map(mutate, (across(c(std.error:p.value, conf.low, conf.high), round, 3))) models_noCovs$global_fit=models_noCovs$global_fit |&gt; map(select, c(Chi2:p_Chi2, RMSEA:SRMR, CFI, AIC)) Note how the functions inside map take on a slightly different form, but work the same. Using this framework, you can easily drill down into any column and be sure that youre accessing the right thing. Everything is always kept together, and always acted upon in the same way. This minimizes mistakes. models_noCovs$tidy_parameters$latent_Basis_model "],["intro-to-r-markdown.html", "Chapter 11 Intro to R Markdown 11.1 Important code chunk options 11.2 Writing math equations and symbols 11.3 Including graphics/inserting pictures 11.4 Footnotes 11.5 Change the color of your text 11.6 Re-using code chunk options 11.7 Making better tables 11.8 Running in-line code", " Chapter 11 Intro to R Markdown R Markdown is a better and more organized way to write scripts. Seriously, once you learn it, theres no going back. New and dont know where to start? Read The R Markdown Cookbook. Amazing overview with tons of neat tricks and how-tos. This other source may also be of some help. Below are some quick tips for common tasks; but be sure to read the Cookbook above. 11.1 Important code chunk options cache: TRUE or FALSE. Do you want to save the output of the chunk so it doesnt have to run next time? Creates a cached folder in the directory. eval: Do you want to evaluate (i.e., run) the code in the chunk? echo: Do you want to print the code after its run? include: Do you want to include code output in the final output document? Setting to FALSE means the code does not appear in the output document, but it is still run. 11.2 Writing math equations and symbols 11.2.1 Greek symbols A few notes first: Math notation is done with dollar signs and forward slashes For Greek letters, just type the name of the letter: $\\mu$ for \\(\\mu\\) $\\sigma$ for \\(\\sigma\\) $\\alpha$ for \\(\\alpha\\) $\\pi$ for \\(\\pi\\) $\\rho$ for \\(\\rho\\) 11.2.2 Math notation $\\pm$ for Â± $\\ge$ for  $\\le$ for  $\\neq$ for  11.2.3 Statistics notation 11.2.4 Writing in-line code Use the funny looking symbol on the tilde key that looks like this: ` To write in line, code, put one of those symbols on either side of the code, like you would with quotation marks. Helps you write lines like: I love dplyr 11.3 Including graphics/inserting pictures The default method doesnt work for me for some reason, but you can still insert images using a combination of the here package and knitr. Use the include_graphics() command and specify both the file location and its name: knitr::include_graphics(here::here(&quot;pics&quot;,&quot;snapchat.png&quot;)) NOTE. Use 300-600 DPI to get good looking pictures. The bookdown book notes that: The syntax for controlling the image attributes is the same as when images are generated from R code. Chunk options fig.cap, out.width, and fig.show still have the same meanings. and: You can easily scale these images proportionally using the same ratio. This can be done via the dpi argument (dots per inch), which takes the value from the chunk option dpi by default If it is a numeric value and the chunk option out.width is not set, the output width of an image will be its actual width (in pixels) divided by dpi , and the unit will be inches. For example, for an image with the size 672 x 480, its output width will be 7 inches ( 7in ) when dpi=96. This feature requires the package png and/or jpeg to be installed. You can always override the automatic calculation of width in inches by providing a non-NULL value to the chunk option out.width , or use include_graphics(dpi = NA) 11.4 Footnotes To add a footnote, use the ^ symbol and put the note in brackets: You can also write footnotes1 like this. 11.5 Change the color of your text YOUR TEXT HERE 11.6 Re-using code chunk options https://yihui.org/en/2021/05/knitr-reuse/ 11.7 Making better tables https://rfortherestofus.com/2019/11/how-to-make-beautiful-tables-in-r/ 11.8 Running in-line code To run code in the middle of a sentence, you create a mini code chunk inside the sentence. For example: &gt; There are 2x2 apples in the basket Could be typed as There are 4 apples in the basket "],["statistics-and-psych-specific-stuff.html", "Chapter 12 Statistics and Psych-specific Stuff 12.1 Create or sample from a distribution 12.2 Calculating Interrater reliability 12.3 Statistical tests and modeling with easystats", " Chapter 12 Statistics and Psych-specific Stuff 12.1 Create or sample from a distribution Creating a binomial distribution When you do this, you are setting the true population parameter; you are in control of the Data Generating Process and the true distribution In a binomial distribution, the parameter is normally distributed, and can take any value from 0.0 to 1.0 But the data that this process generates is not normal rbinom(n= 1000, size= 1, prob = 0.5) ## [1] 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 ## [74] 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 ## [147] 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 ## [220] 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 ## [293] 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 ## [366] 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 ## [439] 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 ## [512] 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 ## [585] 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 ## [658] 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 ## [731] 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 ## [804] 1 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 ## [877] 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 ## [950] 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 rnorm(n=2500,mean=500, sd=100) ## [1] 690.7642 503.0016 446.5626 484.7566 514.2144 558.1073 372.0421 486.2422 445.8801 507.1753 582.8456 556.2646 505.3560 457.7520 552.2014 443.2911 ## [17] 516.2216 507.5774 496.2267 700.0870 486.4038 544.2314 610.0555 490.9881 564.8317 588.5202 438.1985 365.3048 467.5639 521.1525 617.3123 554.5895 ## [33] 489.1160 512.8746 711.0196 692.8460 519.8707 324.4048 441.7998 534.3156 490.2324 585.4221 430.4960 460.9629 703.4337 510.0744 519.9681 468.5821 ## [49] 527.3318 413.0651 460.6879 664.7801 354.3382 370.0553 489.8274 326.0638 343.5181 669.8929 410.4743 259.5956 581.9515 479.6523 434.0253 461.1315 ## [65] 584.3208 423.5085 556.7683 466.5056 433.8107 588.9122 358.9104 296.3319 490.2643 374.4722 508.8257 550.2508 413.3520 491.6750 450.5777 637.9321 ## [81] 418.8566 530.1711 558.6032 531.9450 422.4871 509.5001 436.4969 609.5397 390.5833 573.7023 332.9137 355.7153 459.6057 416.6499 435.9233 393.1971 ## [97] 450.7034 370.8988 325.2500 560.9520 538.7495 596.3238 456.8505 378.8018 389.1940 546.0309 464.9669 437.3562 595.2106 358.9524 488.1387 577.6982 ## [113] 451.5795 555.9834 455.0614 424.9773 512.1967 333.4369 349.7942 497.6879 461.4079 579.9255 570.4377 496.8774 468.6098 553.1056 693.8511 668.4453 ## [129] 418.9909 571.5872 626.5654 682.8749 381.1959 380.5799 529.1706 495.0200 602.4397 488.4812 481.2003 480.3037 517.4619 430.3098 410.7283 383.9239 ## [145] 507.2224 677.0574 551.9872 743.8753 447.8830 553.9661 660.0133 426.8833 378.0482 586.2658 608.6438 414.7819 467.3578 443.9643 505.8476 582.4859 ## [161] 515.8577 579.6892 452.8202 320.2918 429.6083 496.7879 444.7553 546.7475 461.6923 495.1302 305.6665 481.9163 570.0053 411.7289 659.6989 319.8401 ## [177] 577.8611 325.9026 545.4157 460.2025 534.0228 406.5574 532.8106 611.0717 470.5804 470.7250 399.0586 376.7103 432.2565 510.2988 501.0330 311.5054 ## [193] 613.5306 491.6324 592.4899 497.2093 253.9098 540.8763 524.1655 570.1845 568.8148 649.2078 357.9979 505.1117 560.0656 488.2058 512.4326 429.8364 ## [209] 369.7317 562.7482 380.4301 691.9237 552.0891 642.8095 381.2714 610.1536 393.5623 353.8832 557.3873 478.6679 677.2078 493.7744 381.9982 534.3512 ## [225] 519.5267 354.7870 427.7809 492.3991 527.7539 236.7285 634.5613 611.5118 573.7486 494.7876 447.2701 450.1239 447.2279 528.8756 495.6735 432.1601 ## [241] 480.7788 407.7178 519.6089 533.9601 478.7551 535.1155 502.5280 342.3943 446.9967 523.1670 562.8475 462.3585 525.2291 523.7450 532.5428 435.9035 ## [257] 458.1338 661.8657 463.5148 520.1895 497.6584 427.7127 590.0779 563.6812 527.4667 580.2521 450.2358 609.7054 687.3053 315.5045 381.4677 354.9862 ## [273] 621.2616 445.6282 493.3062 557.0388 535.6489 391.4441 517.1641 643.9891 462.9767 456.7823 594.6212 679.8447 469.5304 585.8058 495.7101 515.8654 ## [289] 585.2954 475.4714 245.2662 573.7207 571.2908 510.1705 489.8945 427.6127 381.9606 549.0644 731.1598 624.7910 351.4736 516.0527 502.4989 575.3473 ## [305] 508.8300 598.5068 626.5723 394.5084 551.1109 481.5339 602.6052 490.7534 524.8933 594.8019 511.1983 451.8964 493.0614 628.4617 477.1235 576.7203 ## [321] 469.4596 528.6711 536.4981 466.4876 529.7832 514.9179 594.0735 475.0174 379.3909 551.3659 416.0800 653.6031 349.7248 498.9691 603.6682 541.4988 ## [337] 497.6942 419.0618 461.9743 433.0824 291.6662 454.9965 429.0149 266.4133 580.8652 417.8723 625.2045 507.4710 391.1100 654.7852 627.8170 398.9321 ## [353] 384.7853 501.1054 675.6450 476.3180 440.5276 509.3056 599.2357 592.1269 651.7668 524.3816 421.1368 452.8244 529.8621 644.1803 478.4175 508.7750 ## [369] 595.8007 558.5413 571.2349 555.1023 559.2593 489.7140 493.0326 669.2377 469.0042 347.3663 475.9720 421.2802 645.9101 453.0382 541.1887 411.5055 ## [385] 666.8346 494.7872 397.1767 497.7115 486.7867 534.5902 439.6220 480.8484 554.9179 368.8868 460.8274 489.0922 711.1718 501.9165 418.5724 718.6710 ## [401] 423.1709 435.8882 532.8551 649.3885 451.9812 587.7714 516.2318 511.8405 380.5151 573.8564 462.3429 510.8129 275.7200 535.5461 623.0225 456.8365 ## [417] 490.8698 594.5922 653.8893 516.6514 332.8932 612.6280 524.1005 538.2043 487.1793 524.3440 497.0323 502.0975 587.9191 448.9578 476.7128 602.9146 ## [433] 469.7850 376.6824 499.8641 650.7851 615.8094 493.3007 570.9333 608.2727 559.6828 587.6076 477.4335 535.3579 459.4781 448.0090 531.2777 522.1706 ## [449] 543.7536 606.8745 463.3416 491.5422 684.6416 437.6934 476.8128 499.2137 527.9584 599.6644 396.7951 477.0358 532.5132 478.9701 610.1375 570.7250 ## [465] 458.6793 490.3939 492.9727 548.0784 491.5192 534.6258 592.4548 429.9015 622.2405 514.6087 482.5179 631.1590 548.9731 476.7655 550.6699 458.7896 ## [481] 500.6046 489.9678 521.7070 376.1594 455.4748 478.8103 500.6990 450.0641 705.1408 493.3444 340.2326 450.3086 565.5474 510.5845 607.4647 525.2464 ## [497] 601.9312 393.1189 581.0030 537.7657 453.3838 318.0283 508.4975 531.2463 373.1126 548.7601 623.4519 440.0869 652.5000 437.5633 506.5816 415.4071 ## [513] 516.0706 544.6454 487.4728 544.8934 566.9092 617.6771 548.2616 631.0511 473.4739 571.3268 368.3856 501.6164 522.8763 629.5952 457.4299 527.2467 ## [529] 326.8235 513.6585 420.4447 447.2285 441.3478 517.2391 292.0001 494.4801 527.8838 542.8061 566.1641 578.9908 357.2008 473.6306 677.2587 535.5803 ## [545] 301.3727 403.7398 414.6190 625.7101 433.4926 513.9790 290.1241 249.8849 590.1190 487.6803 571.0442 601.2068 469.5465 475.7392 432.6426 496.2470 ## [561] 418.4045 303.4358 726.0839 609.2849 500.5518 578.1233 408.7615 380.4873 594.1007 499.7439 531.1113 368.6508 455.1496 609.9466 458.9388 604.4078 ## [577] 402.9088 415.8239 448.0483 462.1295 522.6474 487.6938 456.0863 577.8719 340.5999 633.7612 443.4922 454.3063 610.4940 435.0683 599.6636 397.2973 ## [593] 374.6455 325.8588 321.3024 482.7620 374.4408 425.1393 510.4970 531.1596 645.9354 448.5209 531.6261 348.1992 476.0654 562.8976 379.9974 532.0484 ## [609] 445.4427 634.4011 368.7167 672.7004 565.7535 491.0597 731.4451 604.8716 421.3364 300.1127 440.5535 527.3207 420.5020 543.1092 499.6028 422.6449 ## [625] 742.7753 436.8232 530.8857 546.1302 647.6666 448.8163 366.9691 396.1402 562.1381 741.5654 426.0017 551.1103 478.2167 600.2266 532.1149 436.3339 ## [641] 660.6611 354.6999 488.2334 632.6080 390.4946 389.2055 641.5392 502.8989 586.2955 447.9865 349.9972 500.9418 283.4726 583.2403 523.0787 475.2691 ## [657] 539.1305 525.1959 503.8490 297.1299 485.4389 480.3741 525.1787 659.0466 519.6565 513.0671 360.0704 637.1515 586.5342 580.8653 435.4805 624.2765 ## [673] 464.9148 409.2928 565.2380 556.4711 392.7148 588.4800 616.1251 523.3135 495.8529 359.6408 597.8614 557.3185 547.4084 554.3561 469.9123 471.6370 ## [689] 552.7903 457.7323 509.1782 484.3411 310.7837 349.0889 469.9034 736.6610 482.3783 423.4637 607.1870 525.9129 456.5308 358.0275 605.7850 646.5580 ## [705] 477.9809 377.2324 403.8891 601.9674 613.8450 408.5394 510.2801 372.3809 693.2728 609.3073 704.5412 441.4024 623.5082 511.3888 539.7074 465.6184 ## [721] 663.6630 699.4078 444.3459 485.8474 463.9139 436.9269 583.1504 435.3909 584.0109 687.2442 626.6060 504.6432 497.2372 306.4394 430.5330 566.2029 ## [737] 441.3459 484.8510 524.0242 440.1531 554.9754 572.8674 558.5630 621.3363 635.4167 549.3009 548.0824 469.9470 728.0815 691.3732 650.7351 311.2510 ## [753] 528.0012 513.0207 443.2585 495.4862 494.3729 577.6868 511.6519 387.7507 500.9570 528.0799 392.6372 575.0573 649.4153 424.9899 445.2550 589.0665 ## [769] 456.6779 417.0215 494.1086 466.9487 507.0484 617.9516 408.3503 486.6273 519.3648 367.0132 603.6742 357.5127 459.2164 414.9186 477.9580 375.7157 ## [785] 227.5575 447.0915 552.5945 541.2205 477.3557 511.3888 366.3676 541.8846 641.1269 493.1544 444.6010 571.4874 507.8334 376.4408 729.4874 607.8687 ## [801] 453.6202 377.1565 284.5644 514.1725 582.8606 423.0604 485.5145 430.1117 616.0109 367.8656 393.5513 450.6351 475.9432 513.4465 620.5404 560.5519 ## [817] 450.3671 520.9705 493.8261 507.4381 665.0495 325.3849 508.1991 343.4316 380.3415 424.6257 606.1888 579.1084 462.3792 389.8099 490.8874 591.3056 ## [833] 465.1908 624.4178 423.0522 253.6473 462.7211 452.0061 365.4780 334.9249 430.0378 310.6141 587.8560 435.5059 352.7550 366.2667 496.9301 464.1797 ## [849] 426.3634 524.6137 575.3314 462.3060 354.1794 577.5041 288.6520 467.3059 478.7292 635.0938 467.7270 610.1464 442.0070 583.9132 363.9433 530.5671 ## [865] 549.5344 468.3479 466.9145 655.7451 512.8609 504.2036 538.2761 475.7039 458.7146 418.8669 606.1192 448.4299 387.2663 474.5578 576.6278 502.9695 ## [881] 509.4259 505.9550 557.8631 606.7899 546.4254 517.9671 456.8737 534.7139 532.2812 506.9922 437.5018 354.5535 547.5040 546.1719 496.7769 594.6566 ## [897] 566.8912 651.1099 569.2072 399.9619 499.5997 453.0542 503.6737 479.9079 603.6589 588.4268 480.5298 719.0702 476.4992 463.0849 515.4054 557.1812 ## [913] 480.2085 517.2196 429.5344 664.7642 565.2157 489.8956 502.2124 514.0178 534.4487 468.6748 477.4474 428.4724 342.0031 591.6231 361.7524 579.8610 ## [929] 503.2338 531.1024 378.8424 436.3192 708.5300 524.2979 451.3790 368.0187 343.0128 649.4367 593.2486 716.6372 566.0013 563.3512 323.0505 712.7242 ## [945] 283.4797 593.6304 368.4733 586.7725 402.3331 557.0522 511.9797 398.3025 623.5914 529.3349 515.1055 495.3027 594.1564 578.8444 386.4326 402.7863 ## [961] 526.0448 507.9440 334.2873 398.9222 523.8656 365.6481 488.9171 459.0526 286.3321 337.5755 613.3931 478.8902 514.2966 745.8776 475.3705 517.7410 ## [977] 544.9654 630.6918 565.4086 335.1642 531.3661 436.6194 427.9133 425.6029 557.2813 501.2163 547.3963 370.5879 490.5978 447.3090 512.0111 506.0593 ## [993] 444.2074 446.8414 401.0092 463.4028 441.7573 460.8381 494.1673 437.6966 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1500 entries ] 12.2 Calculating Interrater reliability Cohens Kappa is useful for IRR agreement on categorical variablesUse the psych package for this see here; and read this web page for an overview of what Cohens Kappa is if you need a recap/intro. For 3+ raters on a continuous variable, use Intraclass Correlation. See this page. 12.3 Statistical tests and modeling with easystats https://easystats.github.io/easystats/ 12.3.1 Getting parameter estimates from model objects Scenario: Youve run some statistical test (like the below regression), and want a summary of the model estimates. rm(iris) model &lt;- lm(Sepal.Length ~ Species, data = iris) You have a few options when it comes to getting a summary of a model and getting the coefficient estimates: - summary() - broom::tidy() - paramters::model_paramters(), or just paramters::paramters() for short Theres no reason to use summary, generally speaking, because it sucks. It doesnt give you tidy output thats easy to manipulate or extract, its hard to read, and it cant be turned into a useful table. Skip it unless you need something specific from its output (i.e., youre using lavaan) Options two and three are pretty similar and both give you most of the same information, though parameters() prints neater to the console window. Generally I find parameters preferable. Note though that neither command will round the numbers if you store it as a table in the environment. So. If you want to manipulate ANY info in the table and/or extract info, use tidy or parameters. Both make tidy tibbles. If youre using the command to export said info in a neat, presentable MS Word table or HTML table, and you do not care about extracting/modifying/manipulating anything in it, then use parameters and pipe it to format_table() Using format_table() rounds all columns to 2 decimal places, reformats p-values to APA format, and collapses CIs into a single column. Do note though that it makes every column into a Character column! So this is for exporting-use only. Heres a comparison of brooms output (first) vs.Â parameters (second) when you save each in the environment. As you can see, both produce tidy tibbles And heres what parameters(model) |&gt; format_table() does to the a parameters table: Much cleaner for making a table to export to Word. 12.3.2 Getting model information and performance metrics Again, two options here. You can use either glance from the broom package, or performance from the package of the same name. These each produce slightly different output, though unlike above, I dont think one is necessarily better than the other. Use whichever one you prefer. broom::glance(model) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.619 0.614 0.515 119. 1.67e-31 2 -112. 231. 243. 39.0 147 150 performance::performance(model) ## # Indices of model performance ## ## AIC | BIC | R2 | R2 (adj.) | RMSE | Sigma ## ----------------------------------------------------- ## 231.452 | 243.494 | 0.619 | 0.614 | 0.510 | 0.515 12.3.3 Effect size info with effectsize logreg_model=glm(smoke ~ age + sex, data= legaldmlab::survey, family = &quot;binomial&quot;) logreg_model_coeff=parameters::parameters(logreg_model) logreg_model_coeff=logreg_model_coeff |&gt; dplyr::mutate(odds_ratio=exp(Coefficient)) effectsize::interpret_oddsratio(logreg_model_coeff$odds_ratio, rules = &quot;chen2010&quot;) ## [1] &quot;small&quot; &quot;very small&quot; &quot;very small&quot; ## (Rules: chen2010) 12.3.4 Quick, detailed, and automated reporting with report Check out https://easystats.github.io/report/ 12.3.5 Running correlations with correlation https://easystats.github.io/correlation/ "],["advanced-coding-tips.html", "Chapter 13 Advanced Coding Tips 13.1 Grammar and Syntax 13.2 Creating a package 13.3 Creating a bookdown", " Chapter 13 Advanced Coding Tips 13.1 Grammar and Syntax 13.1.1 Regex expressions and symbols str_remove(html$`Market Price`, pattern = &quot;$&quot;) # doesn&#39;t remove the $ sign str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) # works 13.1.2 The colon-equals (:=) operator Sometimes when making a function you need to use the colon-equals operator, rather than just the normal &lt;- or = assignment operators Specifically, when you have multiple named arguments in your functionRead my question and someones answer on this blogpost: https://community.rstudio.com/t/help-creating-simple-function/109011/2 function(df, col, newCol_name){ # drop all NA&#39;s so the function can work properly df_NAdropped=df |&gt; drop_na({{col}}) # apply the function, saving it in a small tibble with only the outlier column and a key to join by outliers_key=df_NAdropped |&gt; dplyr::mutate(outliers=dplyr::if_else(round(abs({{col}}-median({{col}}))/(1.483*mad({{col}}, constant = 1)),2)&gt;2.24,1,0)) |&gt; select(subject_id, outliers) # join back together df=df |&gt; left_join(outliers_key, by=c(&quot;subject_id&quot;)) num.outliers=df |&gt; dplyr::filter(outliers==1) |&gt; dplyr::count() df$outliers=replace_na(df$outliers, 999) df=df |&gt; rename({{newCol_name}}:=&quot;outliers&quot;) message(paste0(num.outliers, &quot; outlier(s) detected&quot;)) return(df) } 13.1.3 User-supplied expressions or named columns in functions This is when you have to put double braces around something 13.1.4 When a command requires a named column or data set, but youve already supplied it and its required a second time If youre writing a function with a pipe but the command youre using needs the data set defined in it, you specify it as .x , or simply just .; Here is an example: 13.2 Creating a package https://rstudio4edu.github.io/rstudio4edu-book/data-pkg.html 13.2.1 Documenting package meta-data https://r-pkgs.org/description.html 13.2.2 Connecting to other packages https://kbroman.org/pkg_primer/pages/depends.html 13.2.3 Linking Git and Github view this detailed guide by Jenny Bryan, and this YouTube video if you want the full guide; or just follow the TL;DR below. Quick summary of steps in YouTube video: Open project folder in Windows Explorer and click in the URL bar, then type cmd to open command prompt If there are any pre-existing git files or repository info there, remove it with the following: rd .git /S/Q Tell git to create a new repo by typing: git init Then tell it to include all files in the current place by typing: git add . Commit these files with: git commit -m \"Initial commit\" At this point youve created a git and GitHub repo each; now link them with: git remote add origin [https URL of GitHub repo] Push all these changes live with: git push -u origin master 13.3 Creating a bookdown https://www.youtube.com/watch?app=desktop&amp;v=m5D-yoH416Y&amp;feature=youtu.be 13.3.1 Rendering the book once its done Render locally with bookdown::render_book(index.Rmd) Use browseURL(\"docs/index.html\") to view your book locally (or just open index.html in a browser). If it looks good, commit and push all changed files to GitHub. "],["creating-a-simulated-data-set.html", "Chapter 14 Creating a simulated data set 14.1 Part 1: Independent samples from a normal distribution 14.2 Part 2: Creating data sets with quantitative and categorical variables 14.3 Part 3: Repeatedly simulate samples with replicate() 14.4 Part 4: repeatedly making whole data sets 14.5 Part 5: Using purrr", " Chapter 14 Creating a simulated data set From the tutorial on this page 14.1 Part 1: Independent samples from a normal distribution Consider the following first before you start doing stuff: - How many subjects are in each condition? - What are the means and standard deviations of each group? Set that shit below. # number of subjects per group A_sub_n &lt;- 50 B_sub_n &lt;- 50 # distribution parameters A_mean &lt;- 10 A_sd &lt;- 2.5 B_mean &lt;- 11 B_sd &lt;- 2.5 Now generate scores for each group A_scores &lt;- rnorm(A_sub_n, A_mean, A_sd) B_scores &lt;- rnorm(B_sub_n, B_mean, B_sd) Technically you could stop here and just analyze the data in this fashionbut its better to organize it into a table. One that looks like something you would import after real data collection. So do that next; make it look nice. dat &lt;- tibble( sub_condition = rep( c(&quot;A&quot;, &quot;B&quot;), c(A_sub_n, B_sub_n) ), score = c(A_scores, B_scores) ) head(dat) ## # A tibble: 6 x 2 ## sub_condition score ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 11.4 ## 2 A 12.0 ## 3 A 12.5 ## 4 A 8.84 ## 5 A 6.84 ## 6 A 11.0 Always perform a quality and consistency check on your data to verify that shits ok. dat %&gt;% group_by(sub_condition) %&gt;% summarise(n = n() , mean = mean(score), sd = sd(score)) ## # A tibble: 2 x 4 ## sub_condition n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 50 9.98 2.78 ## 2 B 50 10.9 3.02 14.2 Part 2: Creating data sets with quantitative and categorical variables From the web page at this link 14.2.1 2.a. DATA WITH NO DIFFERENCE AMONG GROUPS Critically important notes to know: When you use the rep() function, there are several different arguments you can specify inside it that control how stuff is repeated: using rep(x, each= ) repeats things element-wise; each element gets replicated n times, in order rep(c(&quot;A&quot;,&quot;B&quot;), each=3) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; using rep(x, times= ) repeats the sequence; the vector as a whole, as it appears, will be repeated with one sequence following the next rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), times=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; using rep(x, length.out) repeats only the number of elements you specify, in their original order rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), length.out=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; In this particular data, we want every combination of group and letter to be present ONCE. letters=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;) tibble(group = rep(letters[1:2], each = 3), factor = rep(LETTERS[3:5], times = 2), response = rnorm(n = 6, mean = 0, sd = 1) ) ## # A tibble: 6 x 3 ## group factor response ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A C -0.559 ## 2 A D 0.0378 ## 3 A E -1.14 ## 4 B C -1.10 ## 5 B D 0.509 ## 6 B E -0.304 14.2.2 2.b. Data WITH A DIFFERENCE among groups What if we want data where the means are different between groups? Lets make two groups of three observations where the mean of one group is 5 and the other is 10. The two groups have a shared variance (and so standard deviation) of 1. 14.2.2.1 Some notes first Creating a difference between the two groups average score means we have to tell R to sample itteratively from distributions with different means. We do this by specifying a vector of means within rnorm, like so: response = rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 4.066141 9.831952 4.307055 11.059641 4.808349 10.009015 You can see that: 1. draw 1 is from the distribution \\((\\mu=5,\\sigma=1)\\) 2. draw 2 is from the distribution \\((\\mu=5,\\sigma=1)\\) And this process repeats a total of six times. And if you happen to also specify a vector of standard deviations (purely to demonstrate what is happening, we wont actually do this), the first mean is paired with the first SD; the second mean is paired with the second SD; and so on. rnorm(n = 6, mean = c(5, 10), sd = c(2,0.1)) ## [1] 5.744356 10.043787 7.378609 9.952296 8.064520 10.119440 14.2.2.2 Ok, back to creating the data If you want there to be differences between the groups, we need to change the way the vector of factors is replicated, in addition to specifying the vector of means. We want to ensure that the sequence of A, B in the group column matches the sequence repeated in the response column. Here we are going to use length.out so that the whole sequence of A,B is repeated exactly in line with the alternating drawing from \\(\\mu=5\\), \\(\\mu=10\\). Its often best to do this by building each thing separately, and then combining it into a tibble when you have it figured out. group=rep(letters[1:2], length.out = 6) group ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; response=rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 4.948867 10.118024 3.109300 8.589811 4.047384 10.121841 tibble(group, response) ## # A tibble: 6 x 2 ## group response ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 4.95 ## 2 B 10.1 ## 3 A 3.11 ## 4 B 8.59 ## 5 A 4.05 ## 6 B 10.1 14.2.3 2.c. Data with MULTIPLE QUANTITATIVE VARIABLES with groups 14.3 Part 3: Repeatedly simulate samples with replicate() Instead of drawing values one at a time from a distribution, we want to do it many times. This is a job for replicate(). What replicate() does is run a function repeatedly. The replicate() function will perform a given operation as many times as you tell it to. Here we tell it to generate numbers from the distribution \\(N~(\\mu=0, \\sigma=1)\\), three times (as specified in the n=3 argument in line one) replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = FALSE ) ## [[1]] ## [1] 1.3203910 0.2040899 -0.7970636 -1.2431823 -0.4844866 ## ## [[2]] ## [1] -1.2322047 0.1063858 0.7922842 0.4413982 -0.8931153 ## ## [[3]] ## [1] 1.7023983 0.1654943 0.7787761 -0.1186586 -0.2111680 The argument simplify=FALSE tells it to return the output as a list. If you set this to TRUE it returns a matrix instead replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) ## [,1] [,2] [,3] ## [1,] 0.72115635 0.7402300 0.1134487 ## [2,] 0.92810115 -1.1035296 -1.0247406 ## [3,] 0.53107386 0.0286275 -0.3009290 ## [4,] -0.01083094 0.5085364 1.7663764 ## [5,] -0.51711197 0.3824463 -1.0787283 Specifying as.data.frame() with the matrix output can turn it into a data frame. replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) %&gt;% as.data.frame() %&gt;% rename(sample_a=V1, sample_b=V2, sample_c=V3) ## sample_a sample_b sample_c ## 1 0.56415036 -1.7060196 1.1146509 ## 2 -0.24011787 1.8521680 1.4152690 ## 3 -0.09948105 1.9130434 0.1568660 ## 4 -1.76657069 -0.2574811 -1.0662388 ## 5 0.67864934 0.5891597 -0.1166993 14.4 Part 4: repeatedly making whole data sets This is combining parts 2 and 3 to repeatedly create and sample data sets, resulting in a list of many data sets. simlist = replicate(n = 3, expr = data.frame(group = rep(letters[1:2], each = 3), response = rnorm(n = 6, mean = 0, sd = 1) ), simplify = FALSE) simlist ## [[1]] ## group response ## 1 A -1.1482417 ## 2 A -0.5806775 ## 3 A 0.3688638 ## 4 B 0.3304444 ## 5 B 0.3483946 ## 6 B -1.1257075 ## ## [[2]] ## group response ## 1 A 0.8829318 ## 2 A -1.3660498 ## 3 A -0.7142761 ## 4 B -0.2700098 ## 5 B -0.2419871 ## 6 B -0.4623374 ## ## [[3]] ## group response ## 1 A -0.6018124 ## 2 A 1.0289143 ## 3 A 0.1779939 ## 4 B 1.7056511 ## 5 B -0.3255277 ## 6 B -0.7897194 14.5 Part 5: Using purrr See this blog post Kruschke, J. (2015). Goals, power, and sample size. In J. K. Kruschke (Ed.), Doing bayesian data analysis: A tutorial with r, jags, and stan (2nd ed., pp.Â 359-398). Academic Press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
