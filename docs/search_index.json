[["index.html", "Creating a simulated data set Chapter 1 A Monument to my Madness 1.1 What this book is, and what it is not", " Creating a simulated data set Ryan Schneider 2022-11-28 Chapter 1 A Monument to my Madness This book contains all my personal coding notes from the last two years. Why am I doing this? Probably because Im a glutton for punishment, and Id rather procrastinate than write my dissertation proposal. 1.1 What this book is, and what it is not You know those absolutely amazing, comprehensive guides where you can learn everything you need to know about R? This is is not one of those guides. This book is designed as a quick reference guide for many of the most common things youll need to do in everyday data analysis and research. Think of it like a coding dictionary, as opposed to a manual or comprehensive text. If you want (or need) to learn R in-depth and/or from the ground up (i.e., youre a novice user), then you should go read Hadley Wickhams book and the tidyverse websites. Also, these slides might be a good high-level overview if youve never used the tidyverse before. That said, if youre already familiar with R and the tidyverse and just need a quick reference for what command do I need to accomplish XYZ, youve come to the right place. "],["introduction-r-basics.html", "Chapter 2 Introduction: R Basics 2.1 Importing Data 2.2 Exporting (i.e., saving) Data and Output", " Chapter 2 Introduction: R Basics For the love of God before you do anything, familiarize yourself with R Projects and the here package. These make R so much more user friendly and less of a nightmare. If you need an overview, go here: http://jenrichmond.rbind.io/post/how-to-use-the-here-package/ Now lets get stuck in. library(tidyverse) 2.1 Importing Data 2.1.1 Spreadsheets See https://nacnudus.github.io/spreadsheet-munging-strategies/index.html for more detailed and in-depth tutorials (if you need that kind of thing) 2.2 Exporting (i.e., saving) Data and Output 2.2.1 Exporting to .CSV Generally speaking, unless you have a specific reason to, dont. But if you must: write_csv() 2.2.2 Export to .RData (and load the data again later) save(obj_name, file=here::here(&quot;subfolder&quot;, &quot;save_file_name&quot;), compress = FALSE) load(here::here(&quot;folder&quot;, &quot;save_name.RData&quot;)) 2.2.3 Export to Excel library(openxlsx) #Method 1: If you only want to export 1 thing, and/or only need output document #write as object, with no formatting: write.xlsx(objectname,file = &quot;filenamehere.xlsx&quot;,colnames=TRUE, borders=&quot;columns&quot;) #write as table: write.xlsx(objectname,&quot;filename.xlsx&quot;,asTable = TRUE) #Method 2: If you want to do the above, but add multiple objects or tables to one workbook/file: ## first Create Workbook object wb &lt;- createWorkbook(&quot;AuthorName&quot;) #then add worksheets (as many as desired) addWorksheet(wb, &quot;worksheetnamehere&quot;) #then write the object to the worksheet writeData(wb, &quot;test&quot;, nameofobjectordataframe, startCol = 2, startRow = 3, rowNames = TRUE) #save excel file saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) #Method 3: exact same as method 2, but creating a more fancy tables wb &lt;- createWorkbook(&quot;Ryan&quot;) addWorksheet(wb, &quot;worksheetnamehere&quot;) writeDataTable(wb, sheetName, objectName, startCol = 1, startRow = 1, colNames = TRUE, rowNames = FALSE, tableStyle=&quot;TableStyleLight2&quot;,tableName=NULL, headerStyle = NULL,withFilter=FALSE,keepNA=TRUE,sep=&quot;, &quot;, stack = FALSE, firstColumn = FALSE, lastColumn = FALSE,bandedRows = TRUE,bandedCols = FALSE) saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) 2.2.4 Access/edit specific cell number values rainbow=tibble::tribble(~Color, &quot;red&quot;, &quot;orange&quot;, &quot;black&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) rainbow$Color[3] # access, but can&#39;t overwrite this way ## [1] &quot;black&quot; rainbow[3,&quot;Color&quot;] # access and can overwrite ## # A tibble: 1 x 1 ## Color ## &lt;chr&gt; ## 1 black rainbow[3, &quot;Color&quot;]= &quot;yellow&quot; # save this value to row 3 in column &quot;Color&quot; rainbow ## # A tibble: 6 x 1 ## Color ## &lt;chr&gt; ## 1 red ## 2 orange ## 3 yellow ## 4 green ## 5 blue ## 6 purple "],["wrangle-data.html", "Chapter 3 Wrangle Data 3.1 Joining or Splitting 3.2 Selecting/extracting specific variables with select() 3.3 Advanced Filtering techniques 3.4 If-then and Case-when 3.5 Conditional replacement of values 3.6 Merging variables 3.7 Apply a function to multiple variables at once 3.8 Pivoting (i.e., transposing) data 3.9 Turn row names into a column/variable 3.10 How to edit/change column names 3.11 Re-order columns in a data set 3.12 Date and time variables 3.13 Reverse-code a variable 3.14 Dummy coding (the very fast and easy way) 3.15 Create a relative ranking among several variables 3.16 Manipulating the working environment and many things at once 3.17 Wrangling Lists", " Chapter 3 Wrangle Data This chapter contains useful tips on wrangling (i.e., manipulating) data. If you need to know to do to things like create new variables, split one variable into multiple variables, pivot a data set from wide to long, etc., look no further. If you want a pretty good intro tutorial to the dplyr package, click here 3.1 Joining or Splitting Joining and splitting data is pretty straightforward. 3.1.1 Whole Data Sets The code below is from this excellent tutorial set.seed(2018) df1=data.frame(customer_id=c(1:10), product=sample(c(&#39;toaster&#39;,&#39;TV&#39;,&#39;Dishwasher&#39;),10,replace = TRUE)) df2=data.frame(customer_id=c(sample(df1$customer_id, 5)),state=sample(c(&#39;New York&#39;,&#39;California&#39;),5,replace = TRUE)) df1=tibble::as_tibble(df1) df2=tibble::as_tibble(df2) # df1 =left table # df2= right table Inner join - retains only rows with values that appear in both tables, and matches by keys. If youre joining two Qualtrics surveys together, this is most likely the one you want to use (e.g. matching by participant name, and only keeping rows in the joined data set for participants that have responses logged in both survey 1 and survey 2 df1 %&gt;% inner_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York Left join - returns everything in the left, and rows with matching keys in the right df1 %&gt;% left_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Right join - returns everything in the right, and rows with matching keys in the left df1 %&gt;% right_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York # note: example if the customer id column was named something different in the second df #df1 %&gt;% left_join(df2,by=c(&#39;customer_id&#39;=&#39;name2&#39;)) Full join - retain all rows from both tables, and join matching keys in both right and left df1 %&gt;% full_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Anti join - returns all rows in the left that do not have matching keys in the right df1 %&gt;% anti_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 2 ## customer_id product ## &lt;int&gt; &lt;chr&gt; ## 1 2 Dishwasher ## 2 4 toaster ## 3 5 TV ## 4 7 toaster ## 5 10 TV 3.1.2 Individual Columns/Variables Splitting or joining columns is much easier than doing it to whole data sets. You can use dplyr::separate() to accomplish the former, and dplyr::unite() for the latter. print(&quot;hello&quot;) ## [1] &quot;hello&quot; 3.2 Selecting/extracting specific variables with select() Sometimes when working with a data set, you want to work with a few specific variables. For instance, maybe you want to view a graph of only reverse-coded variables (which start with the prefix r); or maybe you want to create a subset of your data that has a few specific variables removed. For this you can use dplyr::select() and its associated helper commands select() can be thought of as extract; it tells R to identify and extract a specific variable (or variables) cars=mtcars # select one column cars %&gt;% select(mpg) # select multiple columns, if they are all next to one another cars %&gt;% select(mpg:hp) # select multiple columns by name (when not next to one another) by defining them in a vector cars %&gt;% select(c(mpg, hp, wt)) # select only variables that start with a certain prefix/character/pattern/etc. cars %&gt;% select(starts_with(&quot;d&quot;)) # ...or columns that end with a certain prefix/etc. cars %&gt;% select(ends_with(&quot;t&quot;)) # ...or contains a certain pattern or string cars %&gt;% select(contains(&quot;se&quot;)) # select ALL OF the variables in a data set that match those of a pre-defined vector # first define the names in a vector vars=c(&quot;hp&quot;, &quot;drat&quot;, &quot;gear&quot;, &quot;carb&quot;) #now use helper cars %&gt;% select(all_of(vars)) # select ANY OF the variables in a pre-defined vector vars_2=c(&quot;hp&quot;, &quot;drat&quot;, &quot;watermelon&quot;, &quot;grilled_cheese&quot;) # only the first two will be in the data cars %&gt;% select(any_of(vars_2)) # only (and all of) the variables actually PRESENT in the data are pulled # select only variables of a certain class or type cars %&gt;% select(where(is.numeric)) cars %&gt;% select(where(is.character)) Other examples can be seen on THIS LINK for a simple but detailed guide. 3.3 Advanced Filtering techniques All info here and the code taken from this link; credit goes to Suzan Baert. 3.3.1 Filter based on partial match Combine filter with str_detect to search for pattern matches in a column 3.3.2 Filtering based on multiple conditions You can filter for more than one value at once, or combine operators to do searches with conditions. Goal Code Return only rows where both conditions are met filter(condition1, condition2) Return all rows where condition 1 is true but condition 2 is not filter(condition1, !condition2) Return rows where condition 1 and/or condition 2 is met filter(condition1 | condition2) Return all rows where only one of the conditions is met, and not when both are met filter(xor(condition1, condition2) The sample code will return all rows with a bodywt above 100 and either have a sleep_total above 15 or are not part of the Carnivora order. msleep %&gt;% select(name, order, sleep_total:bodywt) %&gt;% filter(bodywt &gt; 100, (sleep_total &gt; 15 | order != &quot;Carnivora&quot;)) 3.3.3 Filtering across multiple columns From Susan Baerts blog page: You have three options for how to do this: filter_all(), which filters columns based on your further instructions filter_if(), which requires a boolean to indicate which columns to filter on. If that is true, the filter instructions will be followed. filter_at(), which requires you to specify columns inside a vars argument for which the filtering will be done. Retain all rows with the pattern match Ca inside. Useful for when you want to search for a key pattern (or number) across multiple columns. msleep %&gt;% select(name:order, sleep_total, -vore) %&gt;% filter_all(any_vars(str_detect(., pattern = &quot;Ca&quot;))) msleep %&gt;% select(name, sleep_total:bodywt) %&gt;% filter_all(any_vars(. &lt; 0.1)) # You can also switch any_vars to all_vars to filter across the whole data frame msleep %&gt;% select(name, sleep_total:bodywt, -awake) %&gt;% filter_all(all_vars(. &gt; 1)) What if you want to filter specific column types in your data frame, like just date columns, to find a specific date across multiple date columns? Or filter across multiple number columns to find every instance of a specific number? filter_if is better than filter_all here because the latter would return a filter search across the whole data frame. By using filter_if, we can get more specific about which columns to search through. msleep %&gt;% select(name:order, sleep_total:sleep_rem) %&gt;% filter_if(is.character, any_vars(is.na(.))) The last command, filter_at, does things differently. It doesnt filter all columns at once; and you do not have to specify the column type or which columns. This one allows you to indicate which columns to search through like you would within any select statement. Example: Search through the columns that start with sleep, for values in all of those columns above .5. msleep %&gt;% select(name, sleep_total:sleep_rem, brainwt:bodywt) %&gt;% filter_at(vars(contains(&quot;sleep&quot;)), all_vars(.&gt;5)) 3.4 If-then and Case-when 3.4.1 If-then The premise of an if/then or if/else statement is simple: If condition 1 is satisfied, perform x operation; if not, then do y mtcars %&gt;% mutate(power_level=ifelse(mtcars$hp&lt;350, &quot;Low&quot;, &quot;High&quot;)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb power_level ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Low ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Low ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Low ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Low ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Low ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Low This line of code effectively says: if the length in Sepal.Length is &gt;5, set new variable = to short; else, set it to long 3.4.2 Case-when When you have 3+ conditions, its easier to use case-when. This is a more simple and straightforward approach than nesting multiple if-else commands My_vector= case_when( Condition1 ~ value1, Condition2 ~ value2, Condition3 ~ value3 TRUE ~ valueForEverythingElse #catch all for things that don&#39;t meet the above conditions ) Example: mtcars %&gt;% mutate(size= case_when(cyl==4 ~ &quot;small&quot;, cyl==6 ~ &quot;medium&quot;, cyl==8 ~ &quot;large&quot;)) %&gt;% select(c(cyl,size)) %&gt;% head() ## cyl size ## Mazda RX4 6 medium ## Mazda RX4 Wag 6 medium ## Datsun 710 4 small ## Hornet 4 Drive 6 medium ## Hornet Sportabout 8 large ## Valiant 6 medium 3.5 Conditional replacement of values The following code is useful if you want to replace a value in one column, and the replacement is conditional upon the value in another column. mpg %&gt;% mutate(across(.cols = c(displ, cty, hwy), .fns = ~case_when(cyl == 4L ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 NA 1999 4 auto(l5) f NA NA p comp~ ## 2 audi a4 NA 1999 4 manual(~ f NA NA p comp~ ## 3 audi a4 NA 2008 4 manual(~ f NA NA p comp~ ## 4 audi a4 NA 2008 4 auto(av) f NA NA p comp~ ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p comp~ ## 6 audi a4 2.8 1999 6 manual(~ f 18 26 p comp~ ## 7 audi a4 3.1 2008 6 auto(av) f 18 27 p comp~ ## 8 audi a4 quattro NA 1999 4 manual(~ 4 NA NA p comp~ ## 9 audi a4 quattro NA 1999 4 auto(l5) 4 NA NA p comp~ ## 10 audi a4 quattro NA 2008 4 manual(~ 4 NA NA p comp~ ## # ... with 224 more rows test %&gt;% mutate(across(.cols = c(rank), .fns = ~case_when(is.na(participant_score) ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) 3.6 Merging variables Sometimes youll have multiple variables and you want to collapse them into a single variable. The pmin() command is useful for this. example_data=tribble(~A,~B,~C, 1,NA,NA, 2,NA,NA, 3,NA,NA, NA,4,NA, NA,5,NA, NA,6,NA, NA,NA,7, NA,NA,8, NA,NA,9) example_data %&gt;% mutate(accept_reject = pmin(A,B,C,na.rm = TRUE)) 3.7 Apply a function to multiple variables at once You can either specify each column individually, like above, or tell R to identify columns for you based on their type or their name. This requires adding in one additional verbeither contains() or where() depending on what you want to do. Two simple examples: # turn multiple variables into factors ex_data=dplyr::tribble(~color, ~car, &quot;red&quot;, &quot;corvette&quot;, &quot;blue&quot;, &quot;chevelle&quot;, &quot;green&quot;, &quot;camaro&quot;, &quot;red&quot;, &quot;corvette&quot;, &quot;green&quot;, &quot;chevelle&quot;, &quot;yellow&quot;, &quot;gto&quot;) dplyr::glimpse(ex_data) ## Rows: 6 ## Columns: 2 ## $ color &lt;chr&gt; &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;green&quot;, &quot;yellow&quot; ## $ car &lt;chr&gt; &quot;corvette&quot;, &quot;chevelle&quot;, &quot;camaro&quot;, &quot;corvette&quot;, &quot;chevelle&quot;, &quot;gto&quot; ex_data %&gt;% mutate(across(c(color, car),factor)) ## # A tibble: 6 x 2 ## color car ## &lt;fct&gt; &lt;fct&gt; ## 1 red corvette ## 2 blue chevelle ## 3 green camaro ## 4 red corvette ## 5 green chevelle ## 6 yellow gto # round multiple columns to 1 decimal place mtcars %&gt;% mutate(across(c(disp:qsec),round,1)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.9 2.6 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.9 2.9 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.9 2.3 18.6 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.1 3.2 19.4 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.1 3.4 17.0 0 0 3 2 ## Valiant 18.1 6 225 105 2.8 3.5 20.2 1 0 3 1 3.8 Pivoting (i.e., transposing) data 3.8.1 Condense multiple rows into a single column (pivot wide to long) Rearranging data like this can make it easier to work with and analyze. Example below from my gradebook for stats (exported from Canvas), with fake names. The command structure is as follows: pivot_longer( # Transpose LENGTHWISE by.... cols = everything(), # Taking ALL variable names... names_to=&quot;variable&quot;, # ...and dumping them into this new variable/column values_to=&quot;missing_count&quot;) #...and placing their values in this other new column NOTE!!! Pivoting data from wide to long like this expands the number of rows to make a matrix so that (for example, each student now has as a row for each assignment). Therefore, you can only pivot longways (or wide) ONCE, otherwise you will make duplicates. If you need to pivot multiple columns, just include all of the columns in one single pivot; do not use two separate, back to back pivot commands. Example: gradebook=tibble::tribble( ~Student, ~Homework.1, ~Homework.2, ~Homework.3, ~Homework.4, ~Homework.5, ~Quiz.1, ~Quiz.2, ~Quiz.3, ~Quiz.4, ~Final, &quot;Bob&quot;, 19L, 0L, 13, 16, 0L, 21, 7L, 15, 17.5, 33, &quot;Jane&quot;, 17L, 19L, 16, 16.5, 25L, 21.5, 19L, 14.75, 9.5, 39.5, &quot;John&quot;, 19L, 19L, 14.5, 19.5, 25L, 21, 21L, 18.5, 17, 46.5 ) head(gradebook) ## # A tibble: 3 x 11 ## Student Homework.1 Homework.2 Homework.3 Homework.4 Homework.5 Quiz.1 Quiz.2 ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Bob 19 0 13 16 0 21 7 ## 2 Jane 17 19 16 16.5 25 21.5 19 ## 3 John 19 19 14.5 19.5 25 21 21 ## # ... with 3 more variables: Quiz.3 &lt;dbl&gt;, Quiz.4 &lt;dbl&gt;, Final &lt;dbl&gt; gradebook=gradebook %&gt;% pivot_longer( # Transpose lengthwise by: cols = Homework.1:Final, # Taking these variables names_to=&quot;Assignment&quot;, # ...and dumping them into this new variable, storing them lengthwise values_to=&quot;Points&quot;) #...then place their values in this new column gradebook %&gt;% head() ## # A tibble: 6 x 3 ## Student Assignment Points ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bob Homework.1 19 ## 2 Bob Homework.2 0 ## 3 Bob Homework.3 13 ## 4 Bob Homework.4 16 ## 5 Bob Homework.5 0 ## 6 Bob Quiz.1 21 3.9 Turn row names into a column/variable Use the rownames() command to turn row names into a variable cars=rownames_to_column(mtcars, var = &quot;car&quot;) as_tibble(cars) %&gt;% slice(1:6) ## # A tibble: 6 x 12 ## car mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 Hornet 4 Drive 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 Hornet Sportab~ 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 3.10 How to edit/change column names TWO WAYS TO DO THIS: Use colnames() (for base R) or rename() (for tidyverse) colnames() pulls up all the column/variable names as a vector. If you want to actually change them, youll need to combine this command with something like the sub() or gsub() commands (for base R). Im going to skip this becauseits base R. To access and change the names faster via tidyverse, run use rename() rm(list=ls()) # clear R&#39;s memory iris %&gt;% rename(&quot;hurr&quot;=&quot;Sepal.Length&quot;, &quot;durr&quot;=&quot;Sepal.Width&quot;, &quot;abcdefgh&quot;=&quot;Species&quot;) %&gt;% head() ## hurr durr Petal.Length Petal.Width abcdefgh ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa If you need to do some really fancy conditional renaming (e.g., changing all variables that start with r to start with rf instead, to make it more clear that the prefix actually stands for risk factor rather than reverse coded), youll need to use rename_with(). This command has two parts to it: the data set, and the function you wish to apply to it (which you put after the ~) rename_with(iris, ~ gsub(pattern = &quot;.&quot;, replacement = &quot;_&quot;, .x, fixed = TRUE)) %&gt;% head() ## Sepal_Length Sepal_Width Petal_Length Petal_Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa The gsub() function from Base R identifies matching patterns in the data and substitutes them with what you want instead. Think of it like Rs version of Find/Replace from Microsoft Word. The above line of code thus does the following: 1. First, it checks the column names of the supplied data set (iris) for a specific pattern (specified in pattern= ) 2. Then it replaces that pattern with your input in replacement= The great thing about rename_with() is that the .fn (or ~ for short) can take ANY function as input. For example, if you want to add an element to the column names rather than replace something, (e.g., a prefix or suffix), you can change the function to: rename_with( iris, ~ paste0(.x, &quot;_text&quot;)) %&gt;% head() ## Sepal.Length_text Sepal.Width_text Petal.Length_text Petal.Width_text ## 1 5.1 3.5 1.4 0.2 ## 2 4.9 3.0 1.4 0.2 ## 3 4.7 3.2 1.3 0.2 ## 4 4.6 3.1 1.5 0.2 ## 5 5.0 3.6 1.4 0.2 ## 6 5.4 3.9 1.7 0.4 ## Species_text ## 1 setosa ## 2 setosa ## 3 setosa ## 4 setosa ## 5 setosa ## 6 setosa The above line adds a suffix. You can also add a prefix in the exact same way, just by switching the order of the string and the pattern in the paste0 command. Alternative method to the above This is a second way to do the above. It may appear more simple, but its also probably not as theoretically consistent with how the packages were made..it uses the stringr package to rename the column names, and stringr is typically used for editing vectors of strings in a data set. so it works, but its a little unconventional because you call and edit the column names like you would a variable in your data set. colnames(iris)=str_replace(colnames(iris), pattern = &quot;.&quot;, replacement = &quot;_&quot;) In short: rename() and rename_with() are for renaming variables, as their names imply. The str_ verbs from the stringr package are for editing string-based variabels in your data set. Either works though with a little ingenuity. 3.11 Re-order columns in a data set Use relocate() to change column positions. If you need to move multiple columns at once, this command uses the same syntax as select(). mtcars # notice the column order ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 mtcars %&gt;% relocate(hp:wt, .after= am) %&gt;% head() ## mpg cyl disp qsec vs am hp drat wt gear carb ## Mazda RX4 21.0 6 160 16.46 0 1 110 3.90 2.620 4 4 ## Mazda RX4 Wag 21.0 6 160 17.02 0 1 110 3.90 2.875 4 4 ## Datsun 710 22.8 4 108 18.61 1 1 93 3.85 2.320 4 1 ## Hornet 4 Drive 21.4 6 258 19.44 1 0 110 3.08 3.215 3 1 ## Hornet Sportabout 18.7 8 360 17.02 0 0 175 3.15 3.440 3 2 ## Valiant 18.1 6 225 20.22 1 0 105 2.76 3.460 3 1 3.12 Date and time variables Formatting a column of dates can be extremely helpful if you need to work with time data, but also an extreme pain in the ass if its not stored correctly. This tutorial will be divided into two parts to cover both scenarios that you could encounter. It requires things to be done in two stages, and very precisely. 3.12.1 Date-time objects If youre lucky enough to have a vector of date-times, like what Qualtrics gives you, this will be brainless. Just do the following: example_datetime_data=tibble::tribble(~datetime, &quot;2010-08-03 00:50:50&quot;, &quot;2010-08-04 01:40:50&quot;, &quot;2010-08-07 21:50:50&quot;) head(example_datetime_data) # stored as character string ## # A tibble: 3 x 1 ## datetime ## &lt;chr&gt; ## 1 2010-08-03 00:50:50 ## 2 2010-08-04 01:40:50 ## 3 2010-08-07 21:50:50 # Tidyverse lubridate::as_date(example_datetime_data$datetime) ## [1] &quot;2010-08-03&quot; &quot;2010-08-04&quot; &quot;2010-08-07&quot; 3.12.2 Date-only objects If youre unlucky enough to have only dates, and said dates are written in the traditional x/x/xxxx format, this will be an annoyance that has to be done in two stages. First, assuming your data is already imported and is being stored as a vector of character strings, you have to tell R to adjust the formatting of dates. You cannot change it from a character-based object into a Date or DateTime one until it recognizes the correct formatting. example_date_data=tibble::tribble(~X1, ~X2, &quot;8/4/2021&quot;, -49.87, &quot;8/4/2021&quot;, -13.85, &quot;8/3/2021&quot;, -7.45, &quot;8/3/2021&quot;, -172.71) # Correct formatting example_date_data$X1=format(as.POSIXct(example_date_data$X1,format=&#39;%m/%d/%Y&#39;),format=&#39;%Y-%m-%d&#39;) head(as_tibble(example_date_data)) ## # A tibble: 4 x 2 ## X1 X2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 2021-08-04 -49.9 ## 2 2021-08-04 -13.8 ## 3 2021-08-03 -7.45 ## 4 2021-08-03 -173. In the code above, note that there are two format commands: The first one tells R how the date data is currently being stored, while the second at the end tells it how you want it to be stored. In this case, we are changing it from the way we would usually hand write a date (e.g., 10/26/1993) to a format commonly recognized and used in Excel and stats software (1993-10-26). If your column also has times in it, you also need to include that too! Second, you can now correct the objects structure. You can do this with base Rs as.Date() or tidyverses date() verbs. # Tidyverse example_date_data$X1= lubridate::date(example_date_data$X1) # Base R version example_date_data$X1=as.Date(example_date_data$X1) Notice how the object is now stored as the correct type in the table above. NOTE! This entire process has been included in the tidy_date() command in my package, legaldmlab. 3.12.3 Find the difference between two dates/times difftime(part_1$end_date[1], part_2$end_date[1], units=&quot;days&quot;) 3.13 Reverse-code a variable To reverse-score a variable, you should use car::recode() Can be done a few different ways, depending on how many variables youre looking to recode: # Recode just one variable df$column=recode(df$column,&quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;) # Recode a select bunch of variables df=df %&gt;% mutate(across(c(family_close : family_feelings), recode, &quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;)) 3.14 Dummy coding (the very fast and easy way) Use dplyrs pivot_wider in conjunction with mutate to very quickly and automatically dummy code a column with any number of unique values. The middle part of the code below is what you needjust copy and paste it, and tweak the specifics library(tidyverse) mtcars |&gt; mutate(car=rownames(mtcars)) |&gt; dplyr::mutate(n=1) |&gt; tidyr::pivot_wider(names_from = cyl, values_from = n, names_prefix = &quot;number_cyl&quot;, values_fill = list(n=0)) |&gt; select(car, starts_with(&quot;number_&quot;)) |&gt; head() #truncate output for easier reading ## # A tibble: 6 x 4 ## car number_cyl6 number_cyl4 number_cyl8 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 1 0 0 ## 2 Mazda RX4 Wag 1 0 0 ## 3 Datsun 710 0 1 0 ## 4 Hornet 4 Drive 1 0 0 ## 5 Hornet Sportabout 0 0 1 ## 6 Valiant 1 0 0 3.15 Create a relative ranking among several variables If you want to create a variable that is an ordinal ranking of other variables, first you need to make sure your data is long-wise. Then, depending on the type of ranking system you want, youll might need a different ranking command. The min_rank command from dplyr works in a manner similar to base Rs rank command. It ranks things like you see in sporting events. For example, if there is a clear winner in a game but 3 people tie for second place, the ranks would look like this: 1,2,2,2,4,5. Notice that the positions are independent from the counts. Using the same example from above, if you want the ranks to have no gaps (i.e. 1,2,2,2,3,4), you need to use dplyrs dense_rank command. In either case, the ranks are generated from lowest to highest, so if you want to flip them around youll need to include desc() in the command. dat=tibble::tribble(~name, ~score, &quot;bob&quot;, 0, &quot;bob&quot;, 5, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, NA, &quot;alice&quot;, 70, &quot;alice&quot;, 80, &quot;alice&quot;, 90, &quot;alice&quot;, 20, &quot;alice&quot;, 20, &quot;alice&quot;, 1) dat %&gt;% mutate(ranked = dense_rank(desc(score))) ## # A tibble: 12 x 3 ## name score ranked ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 bob 0 8 ## 2 bob 5 6 ## 3 bob 50 4 ## 4 bob 50 4 ## 5 bob 50 4 ## 6 bob NA NA ## 7 alice 70 3 ## 8 alice 80 2 ## 9 alice 90 1 ## 10 alice 20 5 ## 11 alice 20 5 ## 12 alice 1 7 3.16 Manipulating the working environment and many things at once 3.16.1 Stuff the WHOLE working environment into a list files=mget(ls()) 3.16.2 Extract everything from a list into the environment list2env(cog_data, globalenv()) 3.16.3 Delete everything in the entire environment, except for one item rm(list=setdiff(ls(), &quot;cog_data&quot;)) # delete everything in the local environment except the final data set 3.17 Wrangling Lists 3.17.1 Nesting Imagine the concept of Russian Dolls, applied to data sets. You can manage data sets more effectively my collapsing them into a single tiny, mini data frame, and stuffing that inside of another one. This is done via nesting Effectively, you smush/collapse everything down so it fits inside one column. You can unnest to expand this data back out later when you need it, and keep it collapsed when you dont. This works because a vector/column in a data frame is a list of a defined length; and a data frame is thus simply a collection of lists that are all the same length. You can store anything in a data frame. You can keep the df connected to the model, which makes it very easy to manage a whole slew of related models You can use functional programming (i.e., iterative functions) to map functions or combinations of functions in new ways. Moreover and more importantly, when you use purrr to map functions onto multiple models or objects simultaneously, youre doing it to all of them at once with a single command, and the objects are kept together while you do it. This limits the mistakes you can make (e.g., copying and pasting code and forgetting to tweak something important; applying a function to the wrong object or set of objects by accident), and also reduces unnecessary code in your script. Converting data into tidy data sets gives you a whole new way (and easier way) to manage lots of information head(mtcars |&gt; nest(crap=vs:carb)) ## # A tibble: 6 x 8 ## mpg cyl disp hp drat wt qsec crap ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 21 6 160 110 3.9 2.62 16.5 &lt;tibble [1 x 4]&gt; ## 2 21 6 160 110 3.9 2.88 17.0 &lt;tibble [1 x 4]&gt; ## 3 22.8 4 108 93 3.85 2.32 18.6 &lt;tibble [1 x 4]&gt; ## 4 21.4 6 258 110 3.08 3.22 19.4 &lt;tibble [1 x 4]&gt; ## 5 18.7 8 360 175 3.15 3.44 17.0 &lt;tibble [1 x 4]&gt; ## 6 18.1 6 225 105 2.76 3.46 20.2 &lt;tibble [1 x 4]&gt; 3.17.2 Combining/collapsing list levels (Reducing) demo_vars=files |&gt; map(import_spss) |&gt; reduce(left_join, by=&quot;catieid&quot;) "],["clean-data.html", "Chapter 4 Clean Data 4.1 Replace a value with NA 4.2 Replace NAs with a value 4.3 Identify columns or rows with Missing values 4.4 Find the percentage of a variable that is missing 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set", " Chapter 4 Clean Data 4.1 Replace a value with NA Use dplyr::na_if() if you have a value coded in your data (e.g., 999) that you want to convert to NA example_data=dplyr::tribble(~name, ~bday_month, &quot;Ryan&quot;, 10, &quot;Z&quot;, 3, &quot;Jen&quot;, 999, &quot;Tristin&quot;, 999, &quot;Cassidy&quot;, 6) example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen 999 ## 4 Tristin 999 ## 5 Cassidy 6 example_data$bday_month=na_if(example_data$bday_month, 999) #example doing one column at a time example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 example_data %&gt;% # can also pass the data to mutate and do it the tidyverse way mutate(bday_month=na_if(bday_month, 999)) ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 4.2 Replace NAs with a value tidyr::replace_na() is very useful if you have some NAs in your data and you want to fill them in with some value. example_data=tibble::tribble(~name, ~fav_color, ~fav_food, &quot;Ryan&quot;, &quot;green&quot;, &quot;Mexican&quot;, &quot;Cassidy&quot;, &quot;blue&quot;, NA, &quot;Z&quot;, NA, NA, &quot;Tristin&quot;, &quot;purple&quot;, NA, &quot;Tarika&quot;, NA, NA, &quot;Jen&quot;, NA, &quot;Italian&quot;) example_data ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue &lt;NA&gt; ## 3 Z &lt;NA&gt; &lt;NA&gt; ## 4 Tristin purple &lt;NA&gt; ## 5 Tarika &lt;NA&gt; &lt;NA&gt; ## 6 Jen &lt;NA&gt; Italian # replace NA&#39;s in one col tidyr::replace_na(example_data$fav_food, &quot;MISSING&quot;) ## [1] &quot;Mexican&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;Italian&quot; # replace in multiple columns example_data %&gt;% mutate(across(c(fav_color, fav_food), replace_na, &quot;MISSING&quot;)) ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue MISSING ## 3 Z MISSING MISSING ## 4 Tristin purple MISSING ## 5 Tarika MISSING MISSING ## 6 Jen MISSING Italian 4.3 Identify columns or rows with Missing values is.na() is the base R way to identify, in a TRUE/FALSE manner, whether or not there are missing values in a vector y &lt;- c(1,2,3,NA) is.na(y) # returns a vector (F F F T) ## [1] FALSE FALSE FALSE TRUE 4.4 Find the percentage of a variable that is missing Sometimes necessary to check before conducting an analysis. This requires my package, legaldmlab ?legaldmlab::count_missing mtcars %&gt;% select(hp:drat) %&gt;% legaldmlab::count_missing() ## # A tibble: 2 x 3 ## variable missing_count percent_missing ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 hp 0 0.0% ## 2 drat 0 0.0% 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set Use tidyr::drop_na() to remove rows with missing values. example_data=dplyr::tribble(~name, ~bday_month, ~car, &quot;Ryan&quot;, 10, &quot;kia&quot;, &quot;Z&quot;, NA, &quot;toyota&quot;, &quot;Jen&quot;, NA, NA, &quot;Tristin&quot;, 999, NA, &quot;Cassidy&quot;, 6, &quot;honda&quot;) knitr::kable(example_data) name bday_month car Ryan 10 kia Z NA toyota Jen NA NA Tristin 999 NA Cassidy 6 honda example_data %&gt;% drop_na() # with nothing specified, it drops ALL variables that have &gt;=1 missing value ## # A tibble: 2 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Cassidy 6 honda example_data %&gt;% drop_na(car) # drops only rows with values missing in the specified column ## # A tibble: 3 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Z NA toyota ## 3 Cassidy 6 honda "],["working-with-factors.html", "Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels 5.2 Collapse factor levels 5.3 Add levels to a factor 5.4 Drop unused levels 5.5 Change the order of a factors levels", " Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels Use forcats::fct_recode() diamonds=diamonds %&gt;% as_tibble() diamonds$cut=fct_recode(diamonds$cut, &quot;meh&quot;=&quot;Fair&quot;, &quot;Wow&quot;=&quot;Premium&quot;) summary(diamonds$cut) ## meh Good Very Good Wow Ideal ## 1610 4906 12082 13791 21551 5.2 Collapse factor levels Extremely useful command for when you have infrequent cases in one factor and need to combine it with another. Works by specifying a series of new level names, each of which contains the information from the old variables. Format is as follows: fct_collapse(dataset$variable, NewLevelA=c(&quot;OldLevel1&quot;,&quot;Oldlevel2&quot;), # NewLevelA is the new variable that contains both variables 1 and 2 NewLevelB=c(&quot;OldLevel3&quot;)) 5.3 Add levels to a factor use fct_expand() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.4 Drop unused levels Use fct_drop() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.5 Change the order of a factors levels example_data=tribble(~person, ~condition, &quot;bob&quot;, &quot;25 years&quot;, &quot;jane&quot;, &quot;5 years&quot;, &quot;jim&quot;, &quot;5 years&quot;, &quot;john&quot;, &quot;25 years&quot;) example_data$condition=factor(example_data$condition) str(example_data$condition) ## Factor w/ 2 levels &quot;25 years&quot;,&quot;5 years&quot;: 1 2 2 1 Notice that R thinks these are nominal factors, and that 25 comes before 5. To fix this and correct the level order example_data$condition =fct_relevel(example_data$condition, c(&quot;5 years&quot;, &quot;25 years&quot;)) # specify level order str(example_data$condition) ## Factor w/ 2 levels &quot;5 years&quot;,&quot;25 years&quot;: 2 1 1 2 "],["working-with-strings.html", "Chapter 6 Working with Strings 6.1 Remove a pattern from a string 6.2 Replace one pattern in a string with another 6.3 Find (i.e., filter for) all instances of a string 6.4 Drop all rows from a data set that contain a certain string 6.5 Force all letters to lower case", " Chapter 6 Working with Strings 6.1 Remove a pattern from a string price_table=tribble(~car, ~price, &quot;Corvette&quot;, &quot;$65,000&quot;, &quot;Mustang GT&quot;, &quot;$40,000&quot;) # BASE R METHOD (sub by replacing something with nothing) gsub(&quot;\\\\$&quot;, &quot;&quot;,price_table$price) # (pattern, replace with, object$column) ## [1] &quot;65,000&quot; &quot;40,000&quot; # TIDYVERSE METHOD str_remove(price_table$price, pattern = &quot;\\\\$&quot;) ## [1] &quot;65,000&quot; &quot;40,000&quot; You can remove numbers by typing \"[:digit:]\" panss_sem_data$cgi_sev=str_remove(panss_sem_data$cgi_sev, pattern = &quot;[:digit:]&quot;) 6.2 Replace one pattern in a string with another Tidyverse command: str_replace() or str_replace_all() Base R command: gsub() # base R gsub(mtcars, replacement = ) #tidyverse str_replace_all(iris$Species, pattern=c(&quot;e&quot;, &quot;a&quot;), replacement=&quot;ZZZZ&quot;) |&gt; head() str_replace(iris$Species, pattern=c(&quot;e&quot;, &quot;a&quot;), replacement=&quot;ZZZZ&quot;) |&gt; head() 6.3 Find (i.e., filter for) all instances of a string Useful for finding very specific things inside a column (e.g., one particular persons name in a roster of names; everyone with a particular last name) Tidyverse command: str_detect() Base R command: grepl() Note both must be nested inside of filter() cars_df=rownames_to_column(mtcars, var = &quot;car&quot;) # base R cars_df |&gt; filter(grepl(&quot;Firebird&quot;, car)) # tidyverse cars_df %&gt;% filter(str_detect(car,&quot;Firebird&quot;)) You can also search for multiple strings simultaneously by including the or logical operator inside the quotes. cars_df |&gt; filter(str_detect(car, &quot;Firebird|Fiat&quot;)) You can also include the negation logical operator to filter for all instances except those with the specified string. # base R cars_df |&gt; filter(!(grepl(&quot;Pontiac&quot;, car))) # tidyverse cars_df |&gt; filter(!(str_detect(car, &quot;Pontiac&quot;))) 6.4 Drop all rows from a data set that contain a certain string # Tidyverse method cars_df |&gt; filter(str_detect(car, &quot;Merc&quot;, negate = TRUE)) #including negate=TRUE will negate all rows with the matched string # base R cars_df[!grepl(&quot;Merc&quot;, cars_df$car),] 6.5 Force all letters to lower case Use stringr::str_to_lower() blah=tribble(~A, ~B, &quot;A&quot;,&quot;X&quot;, &quot;A&quot;,&quot;X&quot;) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 A X ## 2 A X blah$A=str_to_lower(blah$A) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 a X ## 2 a X "],["figures-and-graphs-with-the-ggplot-and-see-packages.html", "Chapter 7 Figures and Graphs with the ggplot and see packages 7.1 Commands for ggplot graph types 7.2 Specific Commands for Specific Types of Analysis 7.3 Highlight specific points 7.4 Add labels to data points 7.5 Plotting multiple graphs at once 7.6 Change the colors (bars; columns; dots; etc.) 7.7 Other aesthetic mappings 7.8 Adding and Customizing Text 7.9 Remove gridlines 7.10 Faceting 7.11 Log transformations 7.12 Changing the scale of the axis 7.13 Add a regression line 7.14 Save a graph or figure", " Chapter 7 Figures and Graphs with the ggplot and see packages There are three parts to a ggplot2 call: data aesthetic mapping Layer There is no piping involved in ggplot. You simply invoke ggplot, and tell it what they dataset is. Then you specify the aesthetics, and then the mapping. Lastly, include other optional stuff (e.g. expanded y-axis scale; titles and legends; etc.) Every single plot has the exact same layout that ONLY USES the above three points: ggplot(dataframe, aes(graph dimensions and variables used)) + geom_GraphType(specific graph controls) ## OR ## ggplot(dataframe) + geom_GraphType(aes(graph dimensions and variables used), specific graph controls) # mapping= aes() can go in either spot Then if you have other stuff you want to add on top of this, like axis labels, annotations, highlights, etc., you keep adding those in separate lines 7.1 Commands for ggplot graph types Graph Type Geom command Scatter geom_point() Line geom_line() Box geom_boxplot() Bar geom_bar() Column geom_col() Histogram geom_histogram() Density curve geom_density() Note that bar and column graphs look identical at first glance, but they serve two different purposes. Bar graphs are for frequency counts, and thus only take an X-axis variable; Column graphs are for showing the relationship between two variables X and Y, and display the values in the data # BAR GRAPH # height of bars is a frequency count of each level of the X variable cut bar_plot=ggplot(diamonds, aes(x=cut)) + geom_bar()+ theme_classic() # COLUMN GRAPH # height of bars represents relationship between price and cut col_plot=ggplot(diamonds, aes(x=cut, y=price)) + geom_col()+ theme_classic() see::plots(bar_plot, col_plot, n_columns = 2, tags = c(&quot;Bar&quot;, &quot;Column&quot;)) 7.2 Specific Commands for Specific Types of Analysis 7.2.1 lavaan stuff 7.2.1.1 Plotting an SEM or CFA model First lets set up a model to use. library(lavaan) ## This is lavaan 0.6-9 ## lavaan is FREE software! Please report any bugs. HS.model &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9&#39; fit1 &lt;- cfa(HS.model, data=HolzingerSwineford1939) Two options for graphing it. Option 1 is graph_sem() from the tidySEM package. tidySEM::graph_sem(fit1) Option 2 is from the easystats suite plot(parameters::parameters(fit1)) ## Using `sugiyama` as default layout 7.2.2 Bayes stuff Quick highlights here of my favorite functions from this package. See (ha) the full package overview at this link You can adjust the colors of the figures by setting them yourself (with scale_fill_manual), or by using the appropriate scale_fill command 7.2.2.1 Probability of Direction (Pd) figure Use plot(pd()) to visualize the Probability of Direction index. plot(bayestestR::pd(fit1))+ scale_fill_manual(values=c(&quot;#FFC107&quot;, &quot;#E91E63&quot;))+ theme_classic()+ theme(plot.title = element_text(hjust = 0.5, size = 14, face = &quot;italic&quot;)) 7.2.2.2 ROPE figure plot(fit1, rope_color = &quot;grey70&quot;)+ gameofthrones::scale_fill_got_d(option = &quot;white_walkers&quot;) # scale_fill_manual(values = c(&quot;gray75&quot;,&quot;red&quot;) ROPE tests are plots of distributions, and therefore use scale_fill_xyz_d commands. (the d stands for discrete). You can use any scale theme color set from any package, as long as it ends in _d values=c(#FFC107, #E91E63) is the default bayestestR theme colors from their website 7.2.2.3 Bayes factor models comparison figure plot(bayesfactor_models(Thesis_Model,discount_model))+ scale_fill_flat(palette = &quot;complement&quot; , reverse = TRUE)+ # scale color adjustment 7.2.3 Histograms and density curves Since I use these so often I figure they deserve their own special section. Basic histograms can be built with the following code: ggplot(data = mtcars, aes(x=cyl)) + geom_histogram(binwidth = .5, colour=&quot;Black&quot;, fill=&quot;green&quot;) + # histogram theme_classic() and your basic density curve with the following: ggplot(diamonds, aes(x=price)) + geom_density(alpha=.3)+ # density plot. Alpha sets the transparency level of the fill. theme_classic() You can also use the following code from bayestestR to build a really quick and nice density curve plot(bayestestR::point_estimate(diamonds, centrality=c(&quot;median&quot;,&quot;mean&quot;)))+ labs(title=&quot;Mean and Median&quot;) 7.3 Highlight specific points The gghighlight package is great for this # example 1 ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp))+ # add data labels (optional) gghighlight::gghighlight(hp &gt; 200) # add highlights, according to some criteria # example 2 diamonds_abr=diamonds %&gt;% slice(1:100) ggplot(diamonds_abr, aes(x= cut, y= price, colour=price))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = diamonds_abr, aes(label = price))+ # this line labels gghighlight::gghighlight(cut %in% c(&quot;Very Good&quot;, &quot;Ideal&quot;)) #this line highlights 7.4 Add labels to data points ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp)) ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point() + geom_text(aes(label=hp, hjust=2.5, vjust=2.5)) #geom_label(aes(label = scales::comma(n)), size = 2.5, nudge_y = 6) 7.5 Plotting multiple graphs at once see::plots() is good for this. print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.6 Change the colors (bars; columns; dots; etc.) This can be done in at least two different ways, depending on your goal. To change the fill color by factor or group, add fill = ___ within the aes() command. If you want to add color and/or fill to a continuous variable, do that within the geom_density() command. If you want to add color and make all of the (bars; dots; lines; etc.) the same color, than that is a graph-wide control and needs to be put in geom_point(). This manually sets the color for the whole graph. # add a color scale to the dots ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(color=&quot;blue&quot;) If you want to add color that changes according to a variable (e.g., by factor level), then the color needs to be specified as a variable name, in the aes mapping with the other variables. ggplot(mtcars, aes(x= mpg, y=hp, color=cyl))+ geom_point() 7.6.1 Fine-tuning colors You can change the spectrum of colors to specific colors if you want. Useful for example, when making graphs for APLS presentations; you can change the colors to be Montclair State University themed. When changing the color scale of graphs, note that scale_fill commands are used for representing nominal data, while scale_color commands are for representing continuous data. As such, you use scale_fill to fill in area on a graph that shows a whole category or distinct things; and scale_color to use gradients of color to show changes in continuous data. For figures that have solid area (e.g., density; box; bar; violin plots; etc.), use scale_fill For figures that have continuous changes (e.g., line and scatter plots), use scale_color # Set colors manually ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_manual(values=c(&quot;green&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;blue&quot;)) ggplot(mtcars, aes(x = wt, y = mpg, color=as.factor(cyl)))+ geom_point() + scale_color_manual(values=c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;)) # Use color scales from a package library(gameofthrones) # NOTICE THAT scale_fill AND scale_color STILL APPLY TO THEIR RESPECTIVE GRAPH TYPES # bar graphs ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Tully&quot;) ggplot(mtcars, aes(factor(cyl), fill=factor(vs)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Daenerys&quot;) # scatter plot ggplot(mtcars, aes(x = mpg, y = disp, colour = hp))+ geom_point(size = 2) + scale_colour_got(option = &quot;Lannister&quot;) Fill graphs also come with an extra option: Setting the outline color. You can change the outline of the bar/column/etc. by specifying the color inside geom_x() # change only the fill of the bars ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() # Change the outline of the bars by adding color inside the geom_bar() command ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar(color=&quot;black&quot;) 7.6.2 More options with the see package See this link for setting color gradients for continuous variables, or using other custom color palattes like the gameofthrones package. Check out the see package for some good color scales; the commands for which are here. Incidentally, see is great not only for regular ggplot graphs, but also Bayesian stats graphs link; effect size graphs link; correlation graphs link; and more. 7.7 Other aesthetic mappings shape() controls the shapes on the graph alpha() controls transparency size() controls size Note again that if you want it to change by variable, it goes INSIDE aes(); but if you want to set it manually for the whole graph, it goes in geom_x() # shape ggplot(mtcars, aes(x= mpg, y=hp, shape=as.factor(cyl)))+ geom_point() ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(shape=23) # transparency ggplot(mtcars, aes(x= mpg, y=hp, alpha=hp))+ geom_point() # size ggplot(mtcars, aes(x= mpg, y=hp, size=cyl))+ geom_point() 7.8 Adding and Customizing Text 7.8.1 Add a title, axis labels, and captions All three can be added with labs(). ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;) 7.8.2 Center graph title Add the line theme(plot.title = element_text(hjust = 0.5)) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 7.8.3 Use different fonts See tutorial on this web page Or, use the extrafont package, and set everything using the theme() command. # Visualize new groups library(extrafont) loadfonts(device=&quot;win&quot;) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;)) 7.9 Remove gridlines Add theme(panel.grid = element_blank()) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;))+ theme(panel.grid = element_blank()) 7.10 Faceting This is dividing one plot into subplots, in order to communicate relationships better. Again, this is just a single extra command, this time at the end of the code: facet_wrap(~columnhead) The tilde sign in R means by, as in divide (something) by this print(&quot;temp&quot;) This line produces a graph of population and life expectency, breaking it down to make a separate graph per each continent 7.11 Log transformations Sometimes when your data is really squished together on a graph it is hard to read. In this case, log transformations are really helpful, to change the scale of the data. For example, by multiplying all your points by 10x To create a log transformation of the same scatter plot above, add one extra bit: scale_x_log10() print(&quot;temp&quot;) You can also make both axis be logged by adding +scale again for y 7.12 Changing the scale of the axis Add coord_cartesian(xlim = c(lower,upper)) print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.13 Add a regression line Add the line geom_smooth(method = \"lm\", formula = y ~ x) ggplot(mtcars, aes(x= mpg, y=hp, color=mpg))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, formula = y ~ x) 7.14 Save a graph or figure Use the ggsave command ggsave(plot=better_linearReg_prior, filename = &quot;example_betterLinear_prior.png&quot;, path = here::here(&quot;textbook pics&quot;), dpi = 300, units = &quot;px&quot;, width = 750, height = 600, device = &quot;png&quot;) "],["making-tables-with-flextable.html", "Chapter 8 Making Tables with flextable 8.1 APA Table Components 8.2 Indent values 8.3 Add a Horizontal border (AKA horizontal spanner) 8.4 Change font and font size 8.5 Grouped table 8.6 Adjust line spacing 8.7 Set global options for all flextables 8.8 Complete Example 8.9 Wraper-function to create APA-style tables", " Chapter 8 Making Tables with flextable NOTES: - j refers to the column - i refers to the row number 8.1 APA Table Components 8.2 Indent values https://davidgohel.github.io/flextable/reference/padding.html https://stackoverflow.com/questions/64134725/indentation-in-the-first-column-of-a-flextable-object Use the padding function: ft &lt;- padding(ft, i=2, j=1, padding.left=20) 8.3 Add a Horizontal border (AKA horizontal spanner) hline(., i=4, j=1:2, part = &quot;body&quot;) 8.4 Change font and font size glm_table&lt;-flextable::font(glm_table, part = &quot;all&quot;, fontname = &quot;Times&quot;) %&gt;% # Font fontsize(., size = 11, part = &quot;all&quot;) # Font size 8.5 Grouped table cars=rownames_to_column(mtcars, var = &quot;Model&quot;) test=flextable::as_grouped_data(x=cars, groups = c(&quot;cyl&quot;)) 8.6 Adjust line spacing flextable::line_spacing(space = &quot;0.5&quot;) 8.7 Set global options for all flextables # set global options for all flextables flextable::set_flextable_defaults(font.family = &quot;Times New Roman&quot;, font.size = 11) # create an fp_border object to set the borders for the flextables border.test=officer::fp_border(color = &quot;black&quot;, style = &quot;solid&quot;, width = 1) 8.8 Complete Example library(flextable) ## ## Attaching package: &#39;flextable&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## compose # set global options for all flextables flextable::set_flextable_defaults(font.family = &quot;Times New Roman&quot;, font.size = 11) # create an fp_border object to set the borders for the flextables border.test=officer::fp_border(color = &quot;black&quot;, style = &quot;solid&quot;, width = 1) mtcars |&gt; slice(1:5) |&gt; flextable() |&gt; # FIX BORDERS IN THE HEADER hline_bottom(border = border.test, part = &quot;header&quot;) |&gt; hline_top(border = border.test, part = &quot;header&quot;) |&gt; # CREATE A TITLE HEADER; APPLY FORMATTING add_header_lines(values = &quot;Car Stuff&quot;) |&gt; hline_top(border = fp_border_default(width = 0), part = &quot;header&quot;) |&gt; flextable::align(align = &quot;left&quot;, part = &quot;header&quot;, i=1) |&gt; italic(part = &quot;header&quot;, i=1) |&gt; # FIX BORDER IN TABLE BODY hline_bottom(border = border.test, part = &quot;body&quot;) |&gt; # Line spacing flextable::line_spacing(space = &quot;0.5&quot;) |&gt; # Add a footer flextable::add_footer_lines(values = &quot;A test footer&quot;) |&gt; # SET COLUMN WIDTH/DIMENSIONS autofit(part = &quot;all&quot;) |&gt; width(j=1, width = 0.4, unit = &quot;in&quot;) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-c03b5d7c{}.cl-c0303a00{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:italic;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c0303a01{font-family:'Times New Roman';font-size:11pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-c0304d92{margin:0;text-align:left;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c0304d93{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-c0304d94{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 0.5;background-color:transparent;}.cl-c030af3a{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af3b{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af3c{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af3d{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af3e{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af3f{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af40{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af41{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af42{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af43{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030af44{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d672{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d673{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d674{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d675{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d676{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d677{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d678{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d679{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d67a{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d67b{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030d67c{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb62{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb63{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(255, 255, 255, 0.00);border-top: 0 solid rgba(255, 255, 255, 0.00);border-left: 0 solid rgba(255, 255, 255, 0.00);border-right: 0 solid rgba(255, 255, 255, 0.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb64{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb65{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb66{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb67{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb68{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb69{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb6a{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb6b{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c030eb6c{width:28.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0310f2a{width:30.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0310f2b{width:34pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0310f2c{width:39.5pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0310f2d{width:38.9pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0310f2e{width:45.3pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0310f2f{width:39.8pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-c0310f30{width:37pt;background-color:transparent;vertical-align: middle;border-bottom: 1pt solid rgba(0, 0, 0, 1.00);border-top: 1pt solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Car Stuffmpgcyldisphpdratwtqsecvsamgearcarb21.061601103.902.62016.46014421.061601103.902.87517.02014422.84108933.852.32018.61114121.462581103.083.21519.44103118.783601753.153.44017.020032A test footer 8.9 Wraper-function to create APA-style tables This took way too many hours. Youre welcome. APA_table=function(flextable_object, table_title, include_note){ flextable_object=flextable_object |&gt; hline_bottom(border = border.test, part = &quot;header&quot;) |&gt; hline_top(border = border.test, part = &quot;header&quot;) |&gt; # CREATE A TITLE HEADER; APPLY FORMATTING add_header_lines(values = table_title) |&gt; hline_top(border = fp_border_default(width = 0), part = &quot;header&quot;) |&gt; flextable::align(align = &quot;left&quot;, part = &quot;header&quot;, i=1) |&gt; italic(part = &quot;header&quot;, i=1) |&gt; # FIX BORDER IN TABLE BODY hline_bottom(border = border.test, part = &quot;body&quot;) |&gt; # SET FONT flextable::font(part = &quot;all&quot;, fontname = &quot;Times New Roman&quot;) |&gt; flextable::fontsize(part = &quot;all&quot;, size = 11) |&gt; # SET COLUMN WIDTH/DIMENSIONS autofit(part = &quot;all&quot;) if(include_note==FALSE) (return(flextable_object)) if(include_note!=FALSE) (flextable_object=add_footer_lines(flextable_object, values = paste0(&quot;Note.&quot;,&quot; &quot;, include_note)) |&gt; fontsize(part = &quot;footer&quot;, size = 11) |&gt; font(part = &quot;footer&quot;, fontname = &quot;Times&quot;)) return(flextable_object) } "],["misc.-stuff.html", "Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables 9.2 Read SPSS files into R 9.3 Turn numbers into percentages 9.4 Find all possible combindations of items in a vector 9.5 Download files from the internet 9.6 Print multiple things in one statement", " Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables Note. See Chapter 10s example purrr walk through for a guide on how to scrape multiple web tables simultaneously Simple example. library(rvest) ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding library(tidyverse) html=read_html(&#39;https://shop.tcgplayer.com/price-guide/pokemon/base-set&#39;) %&gt;% html_table(fill = TRUE) html ## [[1]] ## # A tibble: 101 x 6 ## PRODUCT Rarity Number `Market Price` `Listed Median` `` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abra Common 043/102 $0.39 $0.46 View ## 2 Alakazam Holo Rare 001/102 $40.64  View ## 3 Arcanine Uncommon 023/102 $2.94 $2.41 View ## 4 Beedrill Rare 017/102 $3.12 $3.20 View ## 5 Bill Common 091/102 $0.25 $0.30 View ## 6 Blastoise Holo Rare 002/102 $119.59  View ## 7 Bulbasaur Common 044/102 $1.39 $2.21 View ## 8 Caterpie Common 045/102 $0.70 $0.75 View ## 9 Chansey Holo Rare 003/102 $25.35  View ## 10 Charizard Holo Rare 004/102 $348.99  View ## # ... with 91 more rows # Saved as a list by default. Now extract your table from said list html=as_tibble(html[[1]] %&gt;% # find out which number it is in the list select(&#39;PRODUCT&#39;,&#39;Rarity&#39;,&#39;Number&#39;,&#39;Market Price&#39;)) # if needed, specify which columns you want too html ## # A tibble: 101 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abra Common 043/102 $0.39 ## 2 Alakazam Holo Rare 001/102 $40.64 ## 3 Arcanine Uncommon 023/102 $2.94 ## 4 Beedrill Rare 017/102 $3.12 ## 5 Bill Common 091/102 $0.25 ## 6 Blastoise Holo Rare 002/102 $119.59 ## 7 Bulbasaur Common 044/102 $1.39 ## 8 Caterpie Common 045/102 $0.70 ## 9 Chansey Holo Rare 003/102 $25.35 ## 10 Charizard Holo Rare 004/102 $348.99 ## # ... with 91 more rows # remove $ symbol in Price column to make it easier to work with html$`Market Price`=str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) html=html %&gt;% mutate(`Market Price`=as.numeric(`Market Price`)) # convert from string to numeric # view finished table head(html) ## # A tibble: 6 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Abra Common 043/102 0.39 ## 2 Alakazam Holo Rare 001/102 40.6 ## 3 Arcanine Uncommon 023/102 2.94 ## 4 Beedrill Rare 017/102 3.12 ## 5 Bill Common 091/102 0.25 ## 6 Blastoise Holo Rare 002/102 120. Slightly more complicated example Reading a table into R takes a few steps. Step 1 is to copy and paste the URL into the read_html() verb like below: pacman::p_load(rvest, tidyverse) exonerations_table=read_html(&quot;https://www.law.umich.edu/special/exoneration/Pages/detaillist.aspx&quot;) %&gt;% html_nodes(&quot;table.ms-listviewtable&quot;) %&gt;% html_table(fill=TRUE, header = TRUE) Sometimes if the web page is extremely basic and pretty much the only thing on it is a table, you can stop there. Most of the time though, there will be tons of other stuff on the website and you need to get more specific so R can find the table. This is the html_nodes() part of the above command; in there you specify the exact part of the web page where the table is located/what object file it is. To find this you will need to use the Developer mode in your browser. See this screenshot for an example knitr::include_graphics(here::here(&quot;pics&quot;, &quot;scrape.png&quot;)) In Firefox you open this by going to Settings &gt; More Tools &gt; Web Developer Tools (or CNTRL + Shift + I). Begin by looking through the console in the center bottom for names that look like they would be related to your table. A good place to start might be  , which contains the main body of the web page. Click on a name to expand it and see all the elements on the page contained there. Ultimately what youre looking for is what you see above: an element that, when selected, highlights ONLY the area of the web page youre looking for. To get at this you will need to keep expanding, highlighting, and clicking repeatedly.it can take some digging. Keep drilling down through page elements until you find the one that highlights the table and just the table. When you find this, look for the .ms file in that name; you should also see this in the smaller console box on the right. That is the file youll need. Write that name in the html_node command and read it into R. Thats stage 1. From here you now need to clean up the table. exonerations_table=as.data.frame(exonerations_table) # convert into a df Your table might be different, but this ones names were messed up when read in, so lets fix those first and then fix the rows and columns. # save the names to a vector table_names=exonerations_table$Last.Name[1:20] # Trim out the garbage rows and columns exonerations_table=exonerations_table %&gt;% select(Last.Name:Tags.1) %&gt;% slice(22:n()) # over-write incorrect col names with the vector of correct ones we saved above colnames(exonerations_table)=table_names # clean up names exonerations_table=exonerations_table %&gt;% janitor::clean_names() # verify structure of columns is correct # glimpse(exonerations_table) Yikes, a lot of stuff is stored incorrectly, and as a result theres some missing values that need to be addressed and other data that needs to be corrected. exonerations_table=as_tibble(exonerations_table) %&gt;% # convert to tibble mutate(across(c(dna,mwid:ild), na_if,&quot;&quot;)) %&gt;% # turn missing values into NA&#39;s mutate(across(c(dna,mwid:ild), replace_na, &quot;derp&quot;)) %&gt;% # replace NA&#39;s with a string (required for the next lines to work) mutate(dna=ifelse(dna==&quot;DNA&quot;,1,0), # change these variables from text to numeric to better facilitate analysis mwid=ifelse(mwid==&quot;MWID&quot;,1,0), fc=ifelse(fc==&quot;FC&quot;,1,0), p_fa=ifelse(p_fa==&quot;P/FA&quot;,1,0), f_mfe=ifelse(f_mfe==&quot;F/MFE&quot;,1,0)) %&gt;% mutate(across(c(st, crime, dna:f_mfe),factor)) # correct form by converting to factors And thats it! Check out final result! head(exonerations_table) ## # A tibble: 6 x 20 ## last_name first_name age race st county_of_crime tags om_tags crime sentence convicted exonerated dna mwid fc p_fa f_mfe om ild ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbott Cinque 19 Black IL Cook CIU, ~ &quot;OF, WH~ Drug~ Probati~ 2008 2022 0 0 0 1 0 OM derp ## 2 Abdal Warith Habib 43 Black NY Erie IO, SA &quot;OF, WH~ Sexu~ 20 to L~ 1983 1999 1 1 0 0 1 OM derp ## 3 Abernathy Christopher 17 White IL Cook CIU, ~ &quot;OF, WH~ Murd~ Life wi~ 1987 2015 1 0 1 1 0 OM derp ## 4 Abney Quentin 32 Black NY New York CV &quot;&quot; Robb~ 20 to L~ 2006 2012 0 1 0 0 0 derp derp ## 5 Abrego Eruby 20 Hispanic IL Cook CDC, ~ &quot;OF, WH~ Murd~ 90 years 2004 2022 0 1 1 1 0 OM derp ## 6 Acero Longino 35 Hispanic CA Santa Clara NC, P &quot;&quot; Sex ~ 2 years~ 1994 2006 0 0 0 0 0 derp ILD ## # ... with 1 more variable: count_3299 &lt;lgl&gt; Check out this page for a quick overview. 9.2 Read SPSS files into R Use foreign::read.spss spss_version=foreign::read.spss(here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;, &quot;JLWOP_RYAN.sav&quot;), to.data.frame = TRUE) Might also want to add as_tibble() on the end. 9.3 Turn numbers into percentages Use scales::percent(), which converts normal numbers into percentages and includes the percent sign (%) afterwards simple_table=tribble(~n_people, ~votes_in_favor, 25, 14) simple_table=simple_table %&gt;% mutate(percent_voted_for=scales::percent(votes_in_favor/n_people, accuracy = 0.1, scale = 100)) simple_table ## # A tibble: 1 x 3 ## n_people votes_in_favor percent_voted_for ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 25 14 56.0% Scale is what to multiple the original number by (e.g., convert 0.05 to 5% by x100) Accuracy controls how many places out the decimal goes 9.4 Find all possible combindations of items in a vector y &lt;- c(2,4,6,8) combn(c(2,4,6,8),2) # find all possible combinations of these numbers, drawn two at a time ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 2 2 2 4 4 6 ## [2,] 4 6 8 6 8 8 9.5 Download files from the internet 9.6 Print multiple things in one statement Use cat() from base R cat(&quot;The p-value dropped below 0.05 for the first time as sample size&quot;, 100) ## The p-value dropped below 0.05 for the first time as sample size 100 "],["intermediate-r-functions-loops-and-iterative-programming.html", "Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions 10.2 For-loops 10.3 purrr and Iterative Functions 10.4 Other purrr commands 10.5 Using purrr to manage many models", " Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions A function is a command that performs a specified operation and returns an output in accordance with that operation. You can literally make a function to do anything you want. General structure of a basic function: # example structure Function_name=function(argument){ Expressions return(output) } Argument is your input. It is the thing you want to perform the operation on. Expressions is the actual operation (or operations) you want to perform on the supplied argument return tells R to return the result of the Expression to you when done. This example function takes an input of numbers in the form of a vector and subtracts two from each. numbers=c(2,10,12,80) sub_2=function(x){ result= x - 2 return(result) } sub_2(numbers) ## [1] 0 8 10 78 We can also supply the function with a single number and it still works sub_2(100) ## [1] 98 Well this looks useful. So whats the bigger picture? One of the primary advantages of functions are that they can reduce a long and complex process, or a process that involves many steps, into a single line of code; thus, creating your own functions is a fast way to make your life easier down the line either at some point in the far future or even in just a few minutes, if you know you will be writing the code for some process two or more times. Take this script for instance. You can see from the circled parts that I needed to transform three different data sets in a similar way: knitr::include_graphics(here::here(&quot;pics&quot;, &quot;repeat_process.jpg&quot;)) Yes, I could have just done a copy-paste of the original code and tweak it slightly each time. But that is time consuming, produces a sloppier and longer script, and introduces a lot more room for error because of the repeated code and extra steps. Better to write a single function that could be applied to all three. In short, use functions to reduce a multi-step process or a process that youre implementing &gt;=2 times in a single script into one command. This saves you space and makes the script shorter; it saves you the trouble and effort of re-writing or adapting code from earlier sections; and importantly, reduces the chances of you making a coding error by proxy of the former two. As a quick example, I was able to replace each of the circled paragraphs of code above with a custom function that ran everything in one simple line. Now instead of 3 whole (and redundant) paragraphs, I now have 3 short lines, like so. na_zero_helpreint=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;reintegrate_&quot;) na_blank=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) na_zero=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) Limitations to your average, everyday functions. While reducing a whole process or sequence of commands is extremely useful, it still leaves a limitation. For instance, while we avoided copying and pasting whole paragraphs or processes, I still had to copy-paste the same function three times. This still leaves chances for error on the table, and it still leaves us with wasted lines that make the script longer. In general, when you want to perform some function or process multiple times on multiple items (as above where the same command is used three times on three different data frames), you need to use a for-loop or iterating function. These can reduce further unwanted redundancies by applying the function or process iteratively. Read on for more info. 10.2 For-loops A for loop is essentially a function that applies a function or given set of operations to multiple things at once, and returns an output of many items. For example, this code finds the means of every vector/column in a dataset by repeatedly applying the same code over and over to element i in the given list: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) output &lt;- vector(&quot;double&quot;, ncol(df)) # 1.Output. Create the object you want the results of the loop stored in. for (i in seq_along(df)) { # 2.Sequence of operations. &quot;For each item &#39;i&#39; along data frame&quot; output[[i]] &lt;- median(df[[i]]) # 3.Body:&quot;every individual item in &#39;output&#39; = the median of each col in df } output ## [1] 0.3771802 -0.5176346 0.4171879 0.5704655 Check out this book chapter for a great and detailed explanation of for-loops and functional coding. Although for loops are nice, they are unwieldy. R programmers typically use iterating functions instead. Examples of iterating functions are the lapply, vapply, sapply, etc. family of base R commands. But these can also be confusing and the commands are not great. The purrr package offers a better way to do iterating functions over base R; its the tidyverse way to make efficient and understandable for loops! If you have a need for a for-loop for something, see the next section instead on how to use purrr to make an iterative function. Important to understand conceptually what a for-loop is, but using them is impractical when you have purrr 10.3 purrr and Iterative Functions All notes here come from Charlotte Wickhams lecture tutorial below Part 1: https://www.youtube.com/watch?v=7UlWJWfZO9M Part 2: https://www.youtube.com/watch?v=b0ozKTUho0A&amp;t=1210s purrrs map() series of functions offer a way to apply any existing function (even functions youve made) to multiple things at once, be it lists, data frame columns, individual items in vector, etc. In short, they are for doing the same type of task repeatedly in a very quick and efficient manner. They work in much the same way as for-loops, but are far simpler to write, and can be applied in the same way to solve the same problems. How to use purrr The structure of map() commands is the same as the others in the tidyverse: #option 1 map(data, function) # option 2 data %&gt;% map(function) As a quick example and to highlight why purrr is so much more efficient and easier to use than for-loops, look at the same example from before, now using map() instead of a for: df |&gt; map_dbl(median) ## a b c d ## 0.3771802 -0.5176346 0.4171879 0.5704655 A single line is all it took to get the same results! And, it follows tidyverse grammar structure. Now lets get into how it works. map() commands work like this: For each element of x, do f. So if you pass it object x and object x is. - A vector, it will perform function f on every item in the vector - A data frame, it will perform function f on every column in the data frame - A list, it will perform function f on every level in the list Etc., etc.; the point is it applies a function repeatedly to every element in the object you supply it with. So lets walk through a case example. 10.3.1 Reproducible example: Scraping web data This is an example walk through showing how we can use purrr to speed things up dramatically and/or reduce the use of unwanted, extra code in our scripts. In this guide Ill be building a table of LPGA Tour statistics from multiple webpages. The workflow for purrr goes like this: First, you want to figure out how to do each step of your process line-by-line, for a single item. The idea is to try and walk through each step of the process and see exactly what will need to be done each each step and what the code will like, before trying to code it all at once at a higher level. Once you have each step for the first item figured out, then you make functions for each step that condense that code down to one command. Lastly, apply each function from your individual steps to all items in your list by using purr::map(). Do for One library(rvest) # STEP 1 # Figure out a line-by-line process for one item/one single web page html1=read_html(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; html_table(fill = TRUE, header=TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() head(html1) ## rank name distance ## 1 1 Maria Fassi 279.255 ## 2 2 Bianca Pagdanganan 277.052 ## 3 3 Yuka Saso 275.614 ## 4 4 Brooke Matthews 275.279 ## 5 5 A Lim Kim 274.741 ## 6 6 Emily Pedersen 274.669 # STEP 2 # create a custom function of the above to shorten and generalize the process quick_read_html=function(url){ web_page=read_html(url) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; # fortunately this node works for all four pages so it can be baked into the function html_table(fill = TRUE, header = TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() return(web_page) } # test to verify it works test=quick_read_html(url= &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;) head(test) # nice ## rank name putt_average ## 1 1 Lydia Ko 1.722 ## 2 2 Hyo Joo Kim 1.735 ## 3 3 Danielle Kang 1.738 ## 4 4 Nasa Hataoka 1.745 ## 5 5 Madelene Sagstrom 1.748 ## 6 6 Georgia Hall 1.751 DO FOR ALL. Now create the object that contains all the elements you want to iterate over, and then pass it to your generalized function with map. # Step 3a # create an object that contains ALL elements of interest URLs=c(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=06&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=12&quot;) # Step 4 # use the power of map and be amazed lpga_data= URLs |&gt; map(quick_read_html) head(lpga_data) ## [[1]] ## rank name distance ## 1 1 Maria Fassi 279.255 ## 2 2 Bianca Pagdanganan 277.052 ## 3 3 Yuka Saso 275.614 ## 4 4 Brooke Matthews 275.279 ## 5 5 A Lim Kim 274.741 ## 6 6 Emily Pedersen 274.669 ## 7 7 Madelene Sagstrom 273.399 ## 8 8 Lexi Thompson 272.842 ## 9 9 Maude-Aimee Leblanc 272.377 ## 10 10 Nelly Korda 272.280 ## 11 11 Nanna Koerstz Madsen 271.985 ## 12 12 Jessica Korda 271.827 ## 13 13 Pauline Roussin-Bouchard 271.775 ## 14 14 Rachel Rohanna 270.774 ## 15 15 Patty Tavatanakit 269.945 ## 16 16 Carlota Ciganda 269.691 ## 17 17 Alana Uriell 269.509 ## 18 18 Atthaya Thitikul 269.005 ## 19 19 Weiwei Zhang 268.636 ## 20 20 Janie Jackson 268.045 ## 21 21 Minjee Lee 267.715 ## 22 22 Amanda Doherty 267.700 ## 23 23 Cydney Clanton 267.360 ## 24 24 Yu Liu 267.199 ## 25 25 Angel Yin 267.148 ## 26 26 Brooke Henderson 267.079 ## 27 27 Perrine Delacour 266.850 ## 28 28 Charley Hull 266.445 ## 29 29 Jennifer Kupcho 266.273 ## 30 30 Alena Sharp 265.935 ## 31 31 Georgia Hall 265.679 ## 32 32 Ally Ewing 265.455 ## 33 33 Sei Young Kim 265.424 ## 34 34 Nasa Hataoka 265.242 ## 35 35 Albane Valenzuela 265.212 ## 36 36 Yealimi Noh 264.686 ## 37 37 Gaby Lopez 264.419 ## 38 38 Lilia Vu 264.006 ## 39 39 Frida Kinhult 263.857 ## 40 40 Hyejin Choi 263.734 ## 41 41 Ryann O&#39;Toole 263.613 ## 42 42 Hannah Green 263.506 ## 43 43 Celine Herbin 263.327 ## 44 44 Ruixin Liu 263.017 ## 45 45 Stephanie Meadow 263.007 ## 46 46 Daniela Darquea 263.000 ## 47 47 Katherine Perry-Hamski 262.543 ## 48 48 Sung Hyun Park 262.396 ## 49 49 Alison Lee 262.273 ## 50 50 Xiyu Lin 262.250 ## 51 51 Savannah Vilaubi 261.867 ## 52 52 Dewi Weber 261.824 ## 53 53 Pajaree Anannarukarn 261.638 ## 54 54 Peiyun Chien 261.546 ## 55 55 Gerina Mendoza 261.511 ## 56 56 Fatima Fernandez Cano 261.406 ## 57 57 Haylee Harford 261.155 ## 58 58 Amy Yang 261.154 ## 59 59 Sarah Schmelzel 260.633 ## 60 60 Amy Olson 259.643 ## 61 61 Jodi Ewart Shadoff 258.975 ## 62 62 Lauren Hartlage 258.975 ## 63 63 Lindy Duncan 258.962 ## 64 64 Lauren Coughlin 258.862 ## 65 65 Brittany Lincicome 258.719 ## 66 66 Ariya Jutanugarn 258.675 ## 67 67 Mel Reid 258.521 ## 68 68 Giulia Molinaro 258.509 ## 69 69 Katherine Kirk 258.489 ## 70 70 Jaye Marie Green 258.417 ## 71 71 Sophia Schubert 258.403 ## 72 72 Ruoning Yin 258.279 ## 73 73 Eun-Hee Ji 258.129 ## 74 74 Annie Park 258.117 ## 75 75 Stephanie Kyriacou 258.115 ## 76 76 Min Lee 257.983 ## 77 77 Lauren Stephenson 257.947 ## 78 78 Jeong Eun Lee 257.747 ## 79 79 Hinako Shibuno 257.648 ## 80 80 Gina Kim 257.613 ## 81 81 Elizabeth Nagel 257.520 ## 82 82 Mi Hyang Lee 257.506 ## 83 83 Matilda Castren 257.467 ## 84 84 Isi Gabsa 257.085 ## 85 85 Paula Reto 256.840 ## 86 86 Sophia Popov 256.762 ## 87 87 Sanna Nuutinen 256.574 ## 88 88 Elizabeth Szokol 255.967 ## 89 89 Brittany Lang 255.885 ## 90 90 Angela Stanford 255.857 ## 91 91 Megan Khang 255.679 ## 92 92 Karis Davidson 255.359 ## 93 93 Lydia Ko 255.343 ## 94 94 Hyo Joo Kim 255.325 ## 95 95 Casey Danielson 255.064 ## 96 96 Ana Belac 255.020 ## 97 97 Tiffany Chan 254.689 ## 98 98 Agathe Laisne 254.659 ## 99 99 Jenny Shin 254.644 ## 100 100 Robynn Ree 254.513 ## 101 102 Pernilla Lindberg 254.091 ## 102 103 Esther Henseleit 254.080 ## 103 104 Celine Boutier 254.000 ## 104 105 Na Rin An 253.696 ## 105 106 Jenny Coleman 253.656 ## 106 107 Yu-Sang Hou 253.643 ## 107 108 Jennifer Song 253.579 ## 108 109 Linnea Johansson 253.509 ## 109 110 Jennifer Chang 253.375 ## 110 111 Wei-Ling Hsu 253.229 ## 111 112 Allisen Corpuz 252.925 ## 112 113 Pornanong Phatlum 252.874 ## 113 114 Wichanee Meechai 252.580 ## 114 115 Kelly Tan 252.446 ## 115 116 Kaitlyn Papp 252.365 ## 116 117 Sarah Kemp 252.222 ## 117 118 Morgane Metraux 252.139 ## 118 119 Ashleigh Buhai 251.692 ## 119 120 Muni He 251.633 ## 120 121 Danielle Kang 251.172 ## 121 122 Bronte Law 250.623 ## 122 123 Moriya Jutanugarn 250.589 ## 123 124 Leona Maguire 250.306 ## 124 125 Marina Alex 250.259 ## 125 126 Lauren Kim 250.139 ## 126 127 Gemma Dryburgh 249.841 ## 127 128 Sarah Jane Smith 249.389 ## 128 129 Jin Young Ko 249.282 ## 129 130 In Gee Chun 249.077 ## 130 131 Cristie Kerr 248.966 ## 131 132 So Yeon Ryu 248.712 ## 132 133 Ayaka Furue 248.645 ## 133 134 Christina Kim 248.489 ## 134 135 Maddie Szeryk 248.324 ## 135 136 Allison Emrey 247.010 ## 136 137 Su-Hyun Oh 246.942 ## 137 138 Jasmine Suwannapura 246.813 ## 138 139 Emma Talley 246.571 ## 139 140 Caroline Masson 246.448 ## 140 141 Mariah Stackhouse 246.425 ## 141 142 Anna Nordqvist 246.409 ## 142 143 Chella Choi 246.323 ## 143 144 Mirim Lee 246.269 ## 144 145 Mina Harigae 245.803 ## 145 146 Brittany Altomare 245.701 ## 146 147 Cheyenne Knight 245.276 ## 147 148 Caroline Inglis 245.109 ## 148 149 Haeji Kang 245.081 ## 149 150 Na Yeon Choi 243.988 ## 150 151 Brianna Do 243.839 ## 151 152 Lindsey Weaver 243.211 ## 152 153 Inbee Park 242.287 ## 153 154 In-Kyung Kim 242.244 ## 154 155 Stacy Lewis 241.671 ## 155 156 Andrea Lee 241.328 ## 156 157 Marissa Steen 240.720 ## 157 158 Lizette Salas 240.642 ## 158 159 Charlotte Thomas 238.644 ## 159 160 Vivian Hou 238.543 ## 160 161 Yae Eun Hong 238.306 ## 161 162 Dana Finkelstein 235.241 ## 162 163 Aditi Ashok 235.105 ## ## [[2]] ## rank name putt_average ## 1 1 Lydia Ko 1.722 ## 2 2 Hyo Joo Kim 1.735 ## 3 3 Danielle Kang 1.738 ## 4 4 Nasa Hataoka 1.745 ## 5 5 Madelene Sagstrom 1.748 ## 6 6 Georgia Hall 1.751 ## 7 7 Yuka Saso 1.752 ## 8 8 Celine Boutier 1.753 ## 9 9 Lilia Vu 1.757 ## 10 10 Xiyu Lin 1.759 ## 11 11 Leona Maguire 1.759 ## 12 12 Andrea Lee 1.760 ## 13 13 Stephanie Kyriacou 1.761 ## 14 14 Jeong Eun Lee 1.761 ## 15 15 Karis Davidson 1.764 ## 16 16 Carlota Ciganda 1.765 ## 17 17 Nelly Korda 1.766 ## 18 18 Amanda Doherty 1.766 ## 19 19 Brooke Henderson 1.767 ## 20 20 Hannah Green 1.768 ## 21 21 Jessica Korda 1.770 ## 22 22 Atthaya Thitikul 1.771 ## 23 23 Ruoning Yin 1.772 ## 24 24 Amy Yang 1.772 ## 25 25 Hyejin Choi 1.775 ## 26 26 Charley Hull 1.777 ## 27 27 Ayaka Furue 1.778 ## 28 28 Gaby Lopez 1.780 ## 29 29 Patty Tavatanakit 1.781 ## 30 30 Cheyenne Knight 1.783 ## 31 31 Su-Hyun Oh 1.784 ## 32 32 Kaitlyn Papp 1.785 ## 33 33 Minjee Lee 1.786 ## 34 34 Jin Young Ko 1.787 ## 35 35 Sarah Jane Smith 1.787 ## 36 36 Sei Young Kim 1.788 ## 37 37 Megan Khang 1.788 ## 38 38 Angel Yin 1.789 ## 39 39 Chella Choi 1.789 ## 40 40 Frida Kinhult 1.790 ## 41 41 Eun-Hee Ji 1.791 ## 42 42 Yae Eun Hong 1.793 ## 43 43 Paula Reto 1.793 ## 44 44 Ryann O&#39;Toole 1.793 ## 45 45 Pauline Roussin-Bouchard 1.793 ## 46 46 A Lim Kim 1.794 ## 47 47 Nanna Koerstz Madsen 1.794 ## 48 48 Cristie Kerr 1.795 ## 49 49 Gemma Dryburgh 1.795 ## 50 50 Lexi Thompson 1.795 ## 51 51 Na Rin An 1.796 ## 52 52 Inbee Park 1.797 ## 53 53 Alison Lee 1.798 ## 54 54 Yu Liu 1.798 ## 55 55 Lauren Stephenson 1.799 ## 56 56 Isi Gabsa 1.799 ## 57 57 So Yeon Ryu 1.800 ## 58 58 Alena Sharp 1.801 ## 59 59 Mina Harigae 1.801 ## 60 60 In Gee Chun 1.801 ## 61 61 Morgane Metraux 1.802 ## 62 62 Allisen Corpuz 1.802 ## 63 63 Jasmine Suwannapura 1.803 ## 64 64 Haeji Kang 1.804 ## 65 65 Ashleigh Buhai 1.804 ## 66 66 Pajaree Anannarukarn 1.804 ## 67 67 Perrine Delacour 1.805 ## 68 68 Maude-Aimee Leblanc 1.805 ## 69 69 Sarah Schmelzel 1.806 ## 70 70 Brittany Altomare 1.807 ## 71 71 Aditi Ashok 1.808 ## 72 72 Lizette Salas 1.808 ## 73 73 Jennifer Chang 1.808 ## 74 74 Caroline Inglis 1.808 ## 75 75 Moriya Jutanugarn 1.808 ## 76 76 Jenny Shin 1.809 ## 77 77 Ariya Jutanugarn 1.809 ## 78 78 Stacy Lewis 1.810 ## 79 79 Kelly Tan 1.811 ## 80 80 Marina Alex 1.812 ## 81 81 Weiwei Zhang 1.813 ## 82 82 Jennifer Kupcho 1.813 ## 83 83 Emma Talley 1.814 ## 84 84 Elizabeth Szokol 1.814 ## 85 85 Linnea Johansson 1.814 ## 86 86 Allison Emrey 1.814 ## 87 87 Sung Hyun Park 1.814 ## 88 88 Esther Henseleit 1.815 ## 89 89 Hinako Shibuno 1.816 ## 90 90 Tiffany Chan 1.816 ## 91 91 Jodi Ewart Shadoff 1.816 ## 92 92 Katherine Kirk 1.818 ## 93 93 Pernilla Lindberg 1.818 ## 94 94 Alana Uriell 1.819 ## 95 95 Lindsey Weaver 1.819 ## 96 96 Katherine Perry-Hamski 1.822 ## 97 97 Caroline Masson 1.823 ## 98 98 Albane Valenzuela 1.823 ## 99 99 Mi Hyang Lee 1.823 ## 100 100 Janie Jackson 1.824 ## 101 101 Sophia Popov 1.824 ## 102 102 Wichanee Meechai 1.824 ## 103 103 Stephanie Meadow 1.825 ## 104 104 Min Lee 1.827 ## 105 105 Matilda Castren 1.827 ## 106 106 Sanna Nuutinen 1.827 ## 107 107 Annie Park 1.827 ## 108 108 Ruixin Liu 1.828 ## 109 109 Yu-Sang Hou 1.832 ## 110 110 Maria Fassi 1.834 ## 111 111 Pornanong Phatlum 1.835 ## 112 112 Amy Olson 1.836 ## 113 113 Anna Nordqvist 1.836 ## 114 114 Lindy Duncan 1.837 ## 115 115 Charlotte Thomas 1.837 ## 116 116 Wei-Ling Hsu 1.838 ## 117 117 Maddie Szeryk 1.838 ## 118 118 Sarah Kemp 1.838 ## 119 119 In-Kyung Kim 1.838 ## 120 120 Angela Stanford 1.839 ## 121 121 Sophia Schubert 1.839 ## 122 122 Daniela Darquea 1.839 ## 123 123 Gina Kim 1.839 ## 124 124 Yealimi Noh 1.840 ## 125 125 Ana Belac 1.840 ## 126 126 Gerina Mendoza 1.840 ## 127 127 Cydney Clanton 1.841 ## 128 128 Rachel Rohanna 1.841 ## 129 129 Brianna Do 1.841 ## 130 130 Celine Herbin 1.841 ## 131 131 Brittany Lang 1.842 ## 132 132 Christina Kim 1.843 ## 133 133 Dana Finkelstein 1.844 ## 134 134 Elizabeth Nagel 1.844 ## 135 135 Bianca Pagdanganan 1.844 ## 136 136 Emily Pedersen 1.845 ## 137 137 Lauren Coughlin 1.846 ## 138 138 Muni He 1.847 ## 139 140 Na Yeon Choi 1.848 ## 140 141 Agathe Laisne 1.850 ## 141 142 Brittany Lincicome 1.851 ## 142 143 Jennifer Song 1.853 ## 143 144 Ally Ewing 1.854 ## 144 145 Giulia Molinaro 1.854 ## 145 146 Peiyun Chien 1.857 ## 146 147 Dewi Weber 1.858 ## 147 148 Bronte Law 1.859 ## 148 149 Jaye Marie Green 1.860 ## 149 150 Lauren Hartlage 1.861 ## 150 151 Haylee Harford 1.862 ## 151 152 Robynn Ree 1.867 ## 152 153 Mariah Stackhouse 1.868 ## 153 154 Brooke Matthews 1.868 ## 154 155 Fatima Fernandez Cano 1.869 ## 155 156 Jenny Coleman 1.870 ## 156 157 Lauren Kim 1.874 ## 157 158 Savannah Vilaubi 1.884 ## 158 159 Mirim Lee 1.886 ## 159 160 Casey Danielson 1.897 ## 160 161 Vivian Hou 1.900 ## 161 162 Mel Reid 1.901 ## 162 163 Marissa Steen 1.916 ## ## [[3]] ## rank name greens_hit ## 1 1 Ally Ewing 77.7 ## 2 2 Lexi Thompson 77.2 ## 3 3 Jodi Ewart Shadoff 76.5 ## 4 4 Hyejin Choi 76.5 ## 5 5 Brooke Henderson 76.3 ## 6 6 Minjee Lee 76.2 ## 7 7 Xiyu Lin 75.9 ## 8 8 Megan Khang 75.4 ## 9 9 Nelly Korda 75.4 ## 10 10 Atthaya Thitikul 74.9 ## 11 11 Anna Nordqvist 74.6 ## 12 12 A Lim Kim 74.4 ## 13 13 Hannah Green 74.3 ## 14 14 Celine Boutier 73.9 ## 15 15 In Gee Chun 73.6 ## 16 16 Lilia Vu 73.5 ## 17 17 Emily Pedersen 73.5 ## 18 18 Sei Young Kim 73.5 ## 19 19 Jennifer Kupcho 73.4 ## 20 20 Daniela Darquea 73.3 ## 21 21 Matilda Castren 73.1 ## 22 22 Lauren Coughlin 73.1 ## 23 23 Peiyun Chien 73.1 ## 24 24 Charley Hull 73.0 ## 25 25 Andrea Lee 72.9 ## 26 26 Lydia Ko 72.9 ## 27 27 Allisen Corpuz 72.8 ## 28 28 Nasa Hataoka 72.6 ## 29 29 Pajaree Anannarukarn 72.6 ## 30 30 Jeong Eun Lee 72.6 ## 31 31 Sarah Schmelzel 72.5 ## 32 32 Chella Choi 72.5 ## 33 33 Hyo Joo Kim 72.4 ## 34 34 Ayaka Furue 72.4 ## 35 35 Amy Yang 72.4 ## 36 36 Marina Alex 72.3 ## 37 37 Jessica Korda 72.1 ## 38 38 Lindy Duncan 72.0 ## 39 39 Danielle Kang 71.9 ## 40 40 Wei-Ling Hsu 71.8 ## 41 41 Jin Young Ko 71.5 ## 42 42 Carlota Ciganda 71.5 ## 43 43 Perrine Delacour 71.3 ## 44 44 Ryann O&#39;Toole 70.8 ## 45 45 Yealimi Noh 70.8 ## 46 46 Lizette Salas 70.8 ## 47 47 Moriya Jutanugarn 70.7 ## 48 48 Albane Valenzuela 70.7 ## 49 49 Alison Lee 70.6 ## 50 50t Caroline Masson 70.6 ## 51 50t Lauren Stephenson 70.6 ## 52 52 Elizabeth Szokol 70.6 ## 53 53 Ariya Jutanugarn 70.5 ## 54 54 Leona Maguire 70.4 ## 55 55t Cheyenne Knight 70.4 ## 56 55t Gaby Lopez 70.4 ## 57 55t Sophia Schubert 70.4 ## 58 58 Nanna Koerstz Madsen 70.3 ## 59 59 Casey Danielson 70.2 ## 60 60 Madelene Sagstrom 70.1 ## 61 61 Jenny Shin 70.1 ## 62 62t Gina Kim 70.1 ## 63 62t Alena Sharp 70.1 ## 64 64 Dewi Weber 70.1 ## 65 65 So Yeon Ryu 70.0 ## 66 66 Haylee Harford 70.0 ## 67 67 Na Rin An 69.9 ## 68 68 Pornanong Phatlum 69.8 ## 69 69 Brittany Altomare 69.8 ## 70 70 Kelly Tan 69.7 ## 71 71 Sarah Kemp 69.6 ## 72 72 Hinako Shibuno 69.4 ## 73 73 Stephanie Kyriacou 69.4 ## 74 74 Ruixin Liu 69.4 ## 75 75 Caroline Inglis 69.2 ## 76 76 Ruoning Yin 69.1 ## 77 77 Mina Harigae 69.1 ## 78 78 Inbee Park 69.0 ## 79 79 Mi Hyang Lee 69.0 ## 80 80 Annie Park 68.9 ## 81 81 Jennifer Chang 68.9 ## 82 82 Stacy Lewis 68.9 ## 83 83 Esther Henseleit 68.7 ## 84 84 Georgia Hall 68.7 ## 85 85 Ashleigh Buhai 68.6 ## 86 86 Paula Reto 68.6 ## 87 87 Yuka Saso 68.6 ## 88 88 Patty Tavatanakit 68.6 ## 89 89 Yu Liu 68.6 ## 90 90 Maude-Aimee Leblanc 68.5 ## 91 91 Min Lee 68.4 ## 92 92t Jasmine Suwannapura 68.4 ## 93 92t Emma Talley 68.4 ## 94 94 Cydney Clanton 68.3 ## 95 95 Wichanee Meechai 68.3 ## 96 96 Jaye Marie Green 68.2 ## 97 97 Amy Olson 68.2 ## 98 98 Eun-Hee Ji 68.2 ## 99 99 Giulia Molinaro 68.2 ## 100 100 Jenny Coleman 68.1 ## 101 101 Maria Fassi 68.0 ## 102 102 Lindsey Weaver 67.9 ## 103 103 Morgane Metraux 67.9 ## 104 104 Bronte Law 67.9 ## 105 106 Sung Hyun Park 67.9 ## 106 107 Pernilla Lindberg 67.8 ## 107 108 Pauline Roussin-Bouchard 67.8 ## 108 109 Celine Herbin 67.6 ## 109 110 Frida Kinhult 67.6 ## 110 111 Isi Gabsa 67.6 ## 111 112 Haeji Kang 67.6 ## 112 113 Brittany Lincicome 67.5 ## 113 114 Weiwei Zhang 67.4 ## 114 115 Stephanie Meadow 67.2 ## 115 116 Dana Finkelstein 67.1 ## 116 117 Gemma Dryburgh 67.1 ## 117 118 Robynn Ree 67.0 ## 118 119 Gerina Mendoza 66.7 ## 119 120 Jennifer Song 66.6 ## 120 121 Na Yeon Choi 66.1 ## 121 122 Bianca Pagdanganan 66.0 ## 122 123 Janie Jackson 66.0 ## 123 124 Amanda Doherty 65.6 ## 124 125 Sanna Nuutinen 65.6 ## 125 126 Karis Davidson 65.5 ## 126 127 Alana Uriell 65.3 ## 127 128 Lauren Hartlage 65.1 ## 128 129 Muni He 65.1 ## 129 130 Mariah Stackhouse 65.0 ## 130 131 Vivian Hou 65.0 ## 131 132 Angel Yin 65.0 ## 132 133 Agathe Laisne 64.9 ## 133 134 Charlotte Thomas 64.8 ## 134 135 Rachel Rohanna 64.8 ## 135 136 Sophia Popov 64.7 ## 136 137 Ana Belac 64.7 ## 137 138 Kaitlyn Papp 64.6 ## 138 139 Christina Kim 64.3 ## 139 140 Mel Reid 64.3 ## 140 141 Brittany Lang 64.1 ## 141 142 Linnea Johansson 64.0 ## 142 143 Elizabeth Nagel 64.0 ## 143 144 Fatima Fernandez Cano 63.5 ## 144 145 Su-Hyun Oh 63.5 ## 145 146 Mirim Lee 63.5 ## 146 147 Cristie Kerr 63.4 ## 147 148 Marissa Steen 63.3 ## 148 149 Tiffany Chan 63.3 ## 149 150 Katherine Perry-Hamski 63.0 ## 150 151 Aditi Ashok 63.0 ## 151 152 In-Kyung Kim 62.8 ## 152 153 Yu-Sang Hou 62.7 ## 153 154 Brianna Do 62.5 ## 154 155 Maddie Szeryk 62.4 ## 155 156 Katherine Kirk 62.3 ## 156 157 Angela Stanford 61.5 ## 157 158 Brooke Matthews 60.8 ## 158 159 Savannah Vilaubi 60.6 ## 159 160 Allison Emrey 59.7 ## 160 161 Lauren Kim 58.6 ## 161 162 Yae Eun Hong 58.5 ## 162 163 Sarah Jane Smith 57.0 ## ## [[4]] ## rank name rounds score_average_actual ## 1 1 Lydia Ko 85 68.988 ## 2 2 Hyo Joo Kim 59 69.390 ## 3 3 Atthaya Thitikul 96 69.458 ## 4 4 Brooke Henderson 76 69.513 ## 5 5 Xiyu Lin 88 69.545 ## 6 6 Nelly Korda 50 69.660 ## 7 7 Minjee Lee 72 69.694 ## 8 8 Lexi Thompson 60 69.700 ## 9 9 Danielle Kang 61 69.721 ## 10 10 Hyejin Choi 94 69.723 ## 11 11 Hannah Green 79 69.823 ## 12 12 Celine Boutier 86 69.860 ## 13 13 Andrea Lee 67 69.925 ## 14 14 Nasa Hataoka 95 69.937 ## 15 15 Madelene Sagstrom 79 70.127 ## 16 16 Charley Hull 55 70.145 ## 17 17 Megan Khang 81 70.148 ## 18 18 Sei Young Kim 72 70.194 ## 19 19 In Gee Chun 71 70.197 ## 20 20 Lilia Vu 84 70.202 ## 21 21 Ayaka Furue 94 70.340 ## 22 22 Georgia Hall 70 70.357 ## 23 23 Jessica Korda 49 70.388 ## 24 24 Amy Yang 65 70.415 ## 25 25 Leona Maguire 80 70.450 ## 26 26 Chella Choi 84 70.464 ## 27 27 Jodi Ewart Shadoff 81 70.506 ## 28 28 A Lim Kim 101 70.535 ## 29 29 Carlota Ciganda 76 70.579 ## 30 30 Jennifer Kupcho 86 70.593 ## 31 31 Alison Lee 86 70.616 ## 32 32 Jin Young Ko 55 70.673 ## 33 33 Cheyenne Knight 78 70.692 ## 34 34 Yuka Saso 88 70.727 ## 35 35 Jeong Eun Lee 75 70.747 ## 36 36 Daniela Darquea 32 70.750 ## 37 37 Lizette Salas 69 70.754 ## 38 38 Sarah Schmelzel 79 70.759 ## 39 39 Gaby Lopez 78 70.808 ## 40 40 Marina Alex 79 70.823 ## 41 41 Allisen Corpuz 73 70.877 ## 42 42 Nanna Koerstz Madsen 65 70.892 ## 43 43 Ruoning Yin 43 70.930 ## 44 44 Inbee Park 48 70.938 ## 45 45 Na Rin An 85 70.976 ## 46 46 Eun-Hee Ji 66 70.985 ## 47 47t Ally Ewing 67 71.000 ## 48 47t Anna Nordqvist 78 71.000 ## 49 49 Frida Kinhult 70 71.014 ## 50 50 Karis Davidson 32 71.063 ## 51 51 Pajaree Anannarukarn 87 71.080 ## 52 52 Ryann O&#39;Toole 80 71.088 ## 53 53 Albane Valenzuela 78 71.090 ## 54 54 So Yeon Ryu 66 71.091 ## 55 55 Moriya Jutanugarn 88 71.114 ## 56 56 Paula Reto 82 71.122 ## 57 57 Brittany Altomare 82 71.134 ## 58 58 Ruixin Liu 60 71.167 ## 59 59 Jenny Shin 68 71.176 ## 60 60 Ashleigh Buhai 73 71.192 ## 61 61 Caroline Masson 67 71.194 ## 62 62 Jasmine Suwannapura 78 71.205 ## 63 63 Gemma Dryburgh 82 71.220 ## 64 64 Wei-Ling Hsu 72 71.236 ## 65 65 Stephanie Kyriacou 66 71.242 ## 66 66 Alena Sharp 31 71.258 ## 67 67 Emily Pedersen 73 71.301 ## 68 68 Stacy Lewis 73 71.315 ## 69 69 Ariya Jutanugarn 85 71.341 ## 70 70 Lauren Stephenson 67 71.343 ## 71 71 Mina Harigae 71 71.352 ## 72 72 Hinako Shibuno 71 71.366 ## 73 73 Lindy Duncan 26 71.385 ## 74 74t Jennifer Chang 60 71.400 ## 75 74t Perrine Delacour 50 71.400 ## 76 76 Pornanong Phatlum 88 71.455 ## 77 77 Amanda Doherty 60 71.467 ## 78 78 Matilda Castren 76 71.474 ## 79 79 Emma Talley 78 71.474 ## 80 80 Pauline Roussin-Bouchard 71 71.479 ## 81 81 Yu Liu 73 71.507 ## 82 82 Lauren Coughlin 65 71.508 ## 83 83 Caroline Inglis 46 71.543 ## 84 84 Stephanie Meadow 76 71.592 ## 85 85 Haeji Kang 62 71.613 ## 86 86 Yealimi Noh 79 71.646 ## 87 88 Sarah Kemp 63 71.651 ## 88 89 Wichanee Meechai 88 71.682 ## 89 90 Kelly Tan 74 71.743 ## 90 91 Patty Tavatanakit 64 71.750 ## 91 92 Lindsey Weaver 65 71.754 ## 92 93 Dana Finkelstein 54 71.759 ## 93 94 Peiyun Chien 60 71.783 ## 94 95 Esther Henseleit 69 71.812 ## 95 96 Elizabeth Szokol 30 71.833 ## 96 97 Angel Yin 72 71.861 ## 97 98 Isi Gabsa 59 71.881 ## 98 99 Min Lee 60 71.883 ## 99 100 Brittany Lang 39 71.923 ## 100 101 Maria Fassi 55 71.927 ## 101 102 Mi Hyang Lee 46 71.935 ## 102 103 Brittany Lincicome 32 71.938 ## 103 104 Pernilla Lindberg 55 71.945 ## 104 105 Charlotte Thomas 59 71.949 ## 105 106 Sophia Schubert 72 71.986 ## 106 107t Aditi Ashok 72 72.000 ## 107 107t Morgane Metraux 61 72.000 ## 108 107t Dewi Weber 54 72.000 ## 109 110 Bronte Law 57 72.035 ## 110 111 Gina Kim 31 72.065 ## 111 112 Amy Olson 58 72.086 ## 112 113 Annie Park 64 72.125 ## 113 114 Kaitlyn Papp 48 72.146 ## 114 115 Maude-Aimee Leblanc 65 72.215 ## 115 116 Jennifer Song 64 72.219 ## 116 117 Haylee Harford 42 72.238 ## 117 118 Alana Uriell 53 72.264 ## 118 119 Sung Hyun Park 55 72.273 ## 119 120 Gerina Mendoza 47 72.277 ## 120 121 Su-Hyun Oh 60 72.283 ## 121 122 Celine Herbin 29 72.345 ## 122 123 Cristie Kerr 29 72.379 ## 123 124 Linnea Johansson 55 72.382 ## 124 125 Tiffany Chan 31 72.419 ## 125 126 Yae Eun Hong 54 72.463 ## 126 127 Giulia Molinaro 56 72.464 ## 127 128 Jaye Marie Green 36 72.472 ## 128 129 Bianca Pagdanganan 48 72.521 ## 129 130 Muni He 49 72.531 ## 130 131 Robynn Ree 38 72.553 ## 131 132 Maddie Szeryk 34 72.559 ## 132 133 Ana Belac 50 72.620 ## 133 134 Katherine Kirk 24 72.708 ## 134 135 Weiwei Zhang 22 72.727 ## 135 136 Mariah Stackhouse 20 72.750 ## 136 137t Katherine Perry-Hamski 47 72.809 ## 137 137t Rachel Rohanna 47 72.809 ## 138 139 In-Kyung Kim 40 72.850 ## 139 140 Cydney Clanton 50 72.880 ## 140 141 Sarah Jane Smith 27 72.889 ## 141 142 Brooke Matthews 34 72.912 ## 142 143 Janie Jackson 55 72.927 ## 143 144 Lauren Hartlage 40 72.950 ## 144 145t Na Yeon Choi 42 72.952 ## 145 145t Sophia Popov 42 72.952 ## 146 147t Casey Danielson 39 73.000 ## 147 147t Yu-Sang Hou 28 73.000 ## 148 147t Christina Kim 45 73.000 ## 149 147t Agathe Laisne 41 73.000 ## 150 151 Allison Emrey 52 73.019 ## 151 152 Sanna Nuutinen 47 73.043 ## 152 153 Mel Reid 47 73.128 ## 153 154 Jenny Coleman 61 73.131 ## 154 155 Angela Stanford 42 73.143 ## 155 156 Brianna Do 28 73.464 ## 156 157 Marissa Steen 25 73.840 ## 157 158 Elizabeth Nagel 25 73.920 ## 158 159 Vivian Hou 23 74.087 ## 159 160 Savannah Vilaubi 30 74.133 ## 160 161 Fatima Fernandez Cano 32 74.188 ## 161 162 Lauren Kim 18 74.333 ## 162 163 Mirim Lee 26 74.462 All done!! And just like that, weve downloaded four different web pages, extracted the tabled info, and formatted them without copying and pasting any code. The same process for all four was only used one time to write the initial function. Just apply some final formatting to clean it up a bit and combine the separate data frames into a single, unified one. lpga_data= lpga_data %&gt;% reduce(left_join, by=&quot;name&quot;) %&gt;% # Combine all list levels into a single tibble, matching by the &quot;Name&quot; column select(-contains(&quot;rank.&quot;)) |&gt; rename(&quot;score_average&quot;=&quot;score_average_actual&quot;) # VOILA! head(lpga_data) ## name distance putt_average greens_hit rounds score_average ## 1 Maria Fassi 279.255 1.834 68.0 55 71.927 ## 2 Bianca Pagdanganan 277.052 1.844 66.0 48 72.521 ## 3 Yuka Saso 275.614 1.752 68.6 88 70.727 ## 4 Brooke Matthews 275.279 1.868 60.8 34 72.912 ## 5 A Lim Kim 274.741 1.794 74.4 101 70.535 ## 6 Emily Pedersen 274.669 1.845 73.5 73 71.301 10.3.2 Non-reproducible example (Juvenile Life Without Parole study) In the Juvenile Lifers study, there were a series of questions that participants rated on a scale of 0-100 in terms of difficulty. Part of our analysis involved taking the ratings on those variables and giving them relative rankings, so that each of the 6 variables in the series was rated from the least to most difficult, by participant. Now if we only needed to compute these rankings once this wouldnt have been any big deal; however, we needed to do it three times. Much of the same code and the same process would need to be copied and pasted, resulting in a very long, messy, harder to read script. With purrr however, we can reduce the redundancies to a minimum, saving time and reducing the chances of mistakes. Step 1. Just like before, the first step is to find a line-by-line solution for a single item, and then to generalize this into a shortcut function that can be applied to the any item i in a series of items. For the sake of brevity, Im going to skip most of that and just include the functions below. load(&quot;C:/Github Repos/Studies/JLWOP/Data and Models/jlwop_reentry_survey.RData&quot;) #### CREATE THE DATA SETS WE NEED#### na_blank=jlwop_reentry_survey # analysis 1 keeps the data as-is na_zero=jlwop_reentry_survey %&gt;% # supplementary analysis replaces the NA&#39;s with 0 mutate(across(c(barrier_housing:barrier_identification), replace_na,0)) rm(jlwop_reentry_survey) # remove old data set to avoid confusion #### Functions #### # transformation function to wrangle the data into proper formatting rotate_data=function(data, variable_prefix){ data=data %&gt;% pivot_longer( cols= starts_with(variable_prefix), # collect all the desired variables (i.e., columns).... names_to = &quot;variable&quot;, #...and put them into a new categorical variable called &quot;variable&quot; values_to = &quot;participant_score&quot;) %&gt;% # ...and store their values in a new variable called &quot;participant_score&quot; arrange(unique,participant_score) %&gt;% select(c(unique, participant_score, variable)) %&gt;% # keep only these 3 variables relocate(variable, .before = participant_score) # put the newly created variable up front return(data) } # creating the rankings for each variable; then transform data back to original structure rank_and_unpivot=function(data){ data=data %&gt;% group_by(unique) %&gt;% # group the scores so they can be ranked by participant mutate(rank1=dense_rank(participant_score), # create ranking variable rank=max(rank1,na.rm = TRUE) + 1 - rank1) %&gt;% # fix ranks by flipping to ascending order mutate(rank=factor(rank)) %&gt;% # convert rank to factor structure select(-rank1) # Pivot back to wide data=data %&gt;% pivot_wider(names_from = variable, values_from = rank:participant_score) %&gt;% ungroup() # un-group the data and delete the generated names return(data) } Step 2. Again, like before, we want to combine all elements of interest into some object. Once we have that, we then pass said object to map() and supply the map call with our custom function. dfs=list(na_blank=na_blank, na_zero=na_zero) %&gt;% # create lists map(.f=rotate_data, variable_prefix = &quot;barrier&quot;) %&gt;% # apply custom function along whole list map(rank_and_unpivot) # again!! DO IT AGAIN! With another function this time. # extract list elements to make them data frames again list2env(dfs, globalenv()) rm(dfs) #discard list. It has fulfilled its purpose. And just like that, were done! 10.3.3 Example 3: Read/Import several files at once with map() Multiple ways you can do this. ################### Option 1: read all into the global environment, keeping them as SEPERATE df&#39;s ############### legaldmlab::read_all(path=&quot;Data Repository/Stats Data Repository/JASP files&quot;, extension = &quot;.csv&quot;) # Option 1.A: Squish ALL OBJECTS in the working environment into a list # Again, note the &quot;ALL OBJECTS&quot; part; make sure there are no functions or other things in the environment when you run this. files=mget(ls()) ############# Option 2: Read in all files as a LIST of df&#39;s, then stitch in the names ##################################### files=paste0(here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;, &quot;/&quot;), list.files(path=here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;), pattern = &quot;.csv&quot;)) files_list=files |&gt; map(readr::read_csv) names(files_list)=file.path(here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;)) |&gt; #specify file path as a string list.files(pattern = &quot;.csv&quot;) |&gt; # pass the path string to list files; search in this location for files with this extension gsub(pattern=&quot;.csv&quot;, replacement = &quot;&quot;) # remove this pattern to save only the name # Option 2.A: Extract each data frame and put everything into the global environment list2env(cog_data, globalenv()) 10.4 Other purrr commands Note that map() always returns a list, and depending on the output that you want, you may need to use a variation of map(). These variations are as follows: Command Return map_lgl() logical vector map_int() integer vector map_dbl() double vector map_chr() character vector walk() only returns the side effects of a function 10.4.1 walk and walk2 Walk() is useful for when you just want to plot something or write a save file to your disk, etc. It does not give you any return to store something in the environment. You use it to write/read files, open graphics windows, and so on. Example: Writing multiple files at once Utilize purrr::walk2() to apply a function iteratively on TWO objects simultaneously. To save multiple .csv files with walk2, we need two distinct lists: 1. A list of data frames that we wish to export, 2. and the file paths, complete with the file names and extensions, for each file to be written. First create and define both list items. Then apply walk2() to pluck an element from list 1 and its corresponding element from list 2, and apply the write_csv function in for-loop fashion. # DEMO 1: Writing multiple plots at once # Create list one, the list of objects figs = list(scatter_plot=scatter_plot, multi_plot=multi_plot) # create list 2, the list of file names fig_names = figs |&gt; names() |&gt; map(paste0, &quot;.png&quot;) # pass both to purrr to use ggsave iteratively over both lists once and save all graphs with one command walk2(figs, fig_names, ~ggsave(plot = figs, filename = fig_names path=here::here(&quot;Figures and Tables&quot;), device = &quot;png&quot;, dpi = 300)) A second demo, this time using write.csv to save/export multiple CSV files at once # DEMO 2: Wrinting multiple .csv files at once ### Custom function #### # Create needed function that grabs file names and stitches them together with the correct path and extension # Included in legaldmlab package bundle_paths=function(df_list, output_location, file_type){ names=names(df_list) paths=rep(here::here(output_location), length(names)) extension=rep(c(file_type), length(names)) fixed_names=paste0(&quot;/&quot;,names) path_bundle=list(paths,fixed_names, extension) %&gt;% pmap(., paste0) return(path_bundle) } #### Exporting the .csv files for SPSS/JASP/etc. #### # Define list 1 dfs=list(na_blank=na_blank, na_zero=na_zero, na_zero_helpreint=na_zero_helpreint) # list 2 paths_csv=bundle_paths(df_list = dfs, folder_location = &quot;JLWOP/Data and Models&quot;, file_type = &quot;.sav&quot;) # Iterate over all elements in list 1 and corresponding element in list 2; # and apply the the write_csv function to each walk2(.x=dfs, .y= paths, .f=haven::write_sav) #### .RData file for R users #### # Combine multiple data frames into a single .RData file and export save(list = c(&quot;na_blank&quot;, &quot;na_zero&quot;, &quot;na_zero_helpreint&quot;), file = here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;,&quot;ranking_data.RData&quot;)) 10.4.2 map2 knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_a.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_b.png&quot;)) 10.4.3 pmap for when you have a bunch of shit This function is for iterating over three or more elements. As soon as you have &gt;2 items you have to iterate over, you need pmap(), which acts on a list object called .i instead of a list object. The list .i is a list of all the objects you want to iterate over. If you give it a list of 18 items, it iterates over all 18. If the list only has two things, it only acts on those two. She says its easiest to imagine the list as a data frame, and the columns of the data frame like the elements of that list. knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap_2.png&quot;)) 10.5 Using purrr to manage many models Below is the full script I copied from Hadley Wickhams lecture, which you can watch here pacman::p_load(dplyr,purrr,tidyverse,gapminder) #### Workflow for managing many models in R #### # 1. Nest data with {tidyr} # 2. Use {purrr} to map a modeling function # 3. Use {broom} to inspect your tidy data gapminder=gapminder %&gt;% mutate(year1950= year-1950) #the number of years it&#39;s been since 1950 #-------------------------------------------------------------------------------------------- #### Step 1. Nest the data. #### # A nested data frame has one column per country. You&#39;re essentially # creating a Russian doll; a data frame inside of a larger data frame. by_country=gapminder %&gt;% group_by(continent,country) %&gt;% # variables to keep at the top level nest() # smush everything else into a df, and store this mini-df in its own column # with this, you can have an entire table per row; a whole data frame for each country # Essentially condensing a list into a table by_country$data[[1]] #-------------------------------------------------------------------------------------------- #### Step 2. Use purrr to map stuff. #### # 12:50 country_model=function(df){ lm(lifeExp ~ year1950, data = df) } models= by_country %&gt;% mutate( mod=map(data,country_model) ) gapminder %&gt;% group_by(continent,country) %&gt;% nest() %&gt;% mutate( mod= data %&gt;% map(country_model) ) # 27:11 #-------------------------------------------------------------------------------------------- ##### Step 3. #### # This creates another nested df inside of your main data frame that has the summary stats of each model models=models %&gt;% mutate( tidy=map(mod, broom::tidy), # tidy() gives model estimates glance=map(mod,broom::glance), # glance() gives model summaries augment=map(mod,broom::augment) # model coefficients ) # What can you do with this nest of data frames? # The reverse of step 1; un-nest it to unpack everything! # 34:40 # Keeps a massive list of related information neatly organized! unnest(models,data) # back to where we started unnest(models,glance, .drop = TRUE) unnest(models,tidy) and here is a version I made of the above to manage many Latent Growth Curve models. # CONDENSED MASTER TABLE VERSION ----------------------------------------------------------------------------- # Models table that has all models condensed models_noCovs=tibble( #### Define model names #### model_name=c(&quot;Linear&quot;, &quot;Quadratic&quot;, &quot;Latent_Basis&quot;), ##### List model specifications for lavaan #### model_spec=list( linear_model= &#39; # intercept and slope with fixed coefficients i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 + 1*panss_total_3400 + 1*panss_total_5200 s =~ 0*panss_total_400 + 3*panss_total_1000 + 6*panss_total_1600 + 9*panss_total_2200 + 12*panss_total_2800 + 15*panss_total_3400 + 24*panss_total_5200 &#39;, quadratic_model= &#39; # intercept and slope with fixed coefficients i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 + 1*panss_total_3400 s =~ 0*panss_total_400 + 3*panss_total_1000 + 6*panss_total_1600 + 9*panss_total_2200 + 12*panss_total_2800 + 15*panss_total_3400 + 24*panss_total_5200 qs =~ 0*panss_total_400 + 9*panss_total_1000 + 36*panss_total_1600 + 81*panss_total_2200 + 144*panss_total_2800 + 225*panss_total_3400 + 576*panss_total_5200 &#39;, latentBasis_model= &#39; # intercept and slope with fixed coefficients i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 + 1*panss_total_3400 + 1*panss_total_5200 s =~ 0*panss_total_400 + NA*panss_total_1000 + NA*panss_total_1600 + NA*panss_total_2200 + NA*panss_total_2800 + NA*panss_total_3400 + 1*panss_total_5200 &#39; ), #### Fit all models at once with purrr #### fitted_model=model_spec |&gt; map(lavaan::growth, data=panss_sem_data, missing=&quot;FIML&quot;), ) #### Add parameter estimates and fit stats #### models_noCovs=models_noCovs |&gt; mutate(tidy_parameters=map(fitted_model, tidy, conf.int=.95), #parameter estimates global_fit=map(fitted_model, performance::model_performance)) #global fit of models #### Clean up stuff #### models_noCovs$tidy_parameters=models_noCovs$tidy_parameters |&gt; map(select,-c(std.lv:std.nox, op)) |&gt; # remove extra columns map(mutate, estimate=round(estimate, digits = 2)) |&gt; # round numbers map(mutate, (across(c(std.error:p.value, conf.low, conf.high), round, 3))) models_noCovs$global_fit=models_noCovs$global_fit |&gt; map(select, c(Chi2:p_Chi2, RMSEA:SRMR, CFI, AIC)) Note how the functions inside map take on a slightly different form, but work the same. Using this framework, you can easily drill down into any column and be sure that youre accessing the right thing. Everything is always kept together, and always acted upon in the same way. This minimizes mistakes. models_noCovs$tidy_parameters$latent_Basis_model "],["intro-to-r-markdown.html", "Chapter 11 Intro to R Markdown 11.1 Important code chunk options 11.2 Writing math equations and symbols 11.3 Including graphics/inserting pictures 11.4 Footnotes 11.5 Change the color of your text 11.6 Re-using code chunk options 11.7 Making better tables 11.8 Running in-line code", " Chapter 11 Intro to R Markdown R Markdown is a better and more organized way to write scripts. Seriously, once you learn it, theres no going back. New and dont know where to start? Read The R Markdown Cookbook. Amazing overview with tons of neat tricks and how-tos. This other source may also be of some help. Below are some quick tips for common tasks; but be sure to read the Cookbook above. 11.1 Important code chunk options cache: TRUE or FALSE. Do you want to save the output of the chunk so it doesnt have to run next time? Creates a cached folder in the directory. eval: Do you want to evaluate (i.e., run) the code in the chunk? echo: Do you want to print the code after its run? include: Do you want to include code output in the final output document? Setting to FALSE means the code does not appear in the output document, but it is still run. 11.2 Writing math equations and symbols 11.2.1 Greek symbols A few notes first: Math notation is done with dollar signs and forward slashes For Greek letters, just type the name of the letter: $\\mu$ for \\(\\mu\\) $\\sigma$ for \\(\\sigma\\) $\\alpha$ for \\(\\alpha\\) $\\pi$ for \\(\\pi\\) $\\rho$ for \\(\\rho\\) etc. 11.2.2 Math notation $\\pm$ for ± $\\ge$ for  $\\le$ for  $\\neq$ for  ${\\sim}$ for \\({\\sim}\\) $\\overline{X}$ for \\(\\overline{X}\\) $\\frac{X} {Y} for \\(\\frac{X} {Y}\\) $X_i$ for \\(X_i\\) 11.2.3 Statistics notation 11.2.4 Writing in-line code Use the funny looking symbol on the tilde key that looks like this: ` To write in line, code, put one of those symbols on either side of the code, like you would with quotation marks. Helps you write lines like: I love dplyr 11.3 Including graphics/inserting pictures The default method doesnt work for me for some reason, but you can still insert images using a combination of the here package and knitr. Use the include_graphics() command and specify both the file location and its name: knitr::include_graphics(here::here(&quot;pics&quot;,&quot;snapchat.png&quot;)) NOTE. Use 300-600 DPI to get good looking pictures. The bookdown book notes that: The syntax for controlling the image attributes is the same as when images are generated from R code. Chunk options fig.cap, out.width, and fig.show still have the same meanings. and: You can easily scale these images proportionally using the same ratio. This can be done via the dpi argument (dots per inch), which takes the value from the chunk option dpi by default If it is a numeric value and the chunk option out.width is not set, the output width of an image will be its actual width (in pixels) divided by dpi , and the unit will be inches. For example, for an image with the size 672 x 480, its output width will be 7 inches ( 7in ) when dpi=96. This feature requires the package png and/or jpeg to be installed. You can always override the automatic calculation of width in inches by providing a non-NULL value to the chunk option out.width , or use include_graphics(dpi = NA) 11.4 Footnotes To add a footnote, use the ^ symbol and put the note in brackets: You can also write footnotes1 like this. 11.5 Change the color of your text YOUR TEXT HERE 11.6 Re-using code chunk options https://yihui.org/en/2021/05/knitr-reuse/ 11.7 Making better tables https://rfortherestofus.com/2019/11/how-to-make-beautiful-tables-in-r/ 11.8 Running in-line code To run code in the middle of a sentence, you create a mini code chunk inside the sentence. For example: &gt; There are 2x2 apples in the basket Could be typed as There are 4 apples in the basket "],["statistics--and-psych-specific-stuff.html", "Chapter 12 Statistics- and Psych-specific Stuff 12.1 Create or sample from a distribution 12.2 Calculating Interrater reliability 12.3 Statistical tests and modeling with easystats", " Chapter 12 Statistics- and Psych-specific Stuff 12.1 Create or sample from a distribution If you want to generate a distribution of data to sample from, use the distribution functions that start with r. I will create a few different binomial distributions as examples. Each distribution command has parameters you set to create the distribution. The binomials has three: n, the number of samples size, the number of trials per sample prob, the probability of a success on each individual trial Note the definition of the size variable in particular. And imagine we are simulating a bunch of jury trials. #### Example 1 #### # 10 jury trials with 1 verdict each rbinom(n=10, size = 1, prob = .7) #### Example 2 #### # 10 jury trials with 2 verdicts per trial. Each participant has 2 outcomes. rbinom(n=10, size = 2, prob = .7) #### Example 3 #### # 1 jury trial with 1 verdict rbinom(n=1, size = 1, prob = .7) # or alternatively... purrr::rbernoulli(n=1, p=.7) The third example of rbinom is equivalent to creating a Bernoulli distribution, because a single binomial event is a Bernoulli event. One coin flip comes from a Bernoulli distribution.a series of 5 coin flips (i.e., trials) comes from a Binomial. Heres an example where I generate 100 fake participants, and then for each participant, simulate a single Bernoulli event (a win or a loss at trial). test=tibble(plea_rejectors=c(1:100), trial_outcomes=purrr::rbernoulli(n=length(plea_rejectors), p=.3)) |&gt; mutate(trial_outcomes=ifelse(trial_outcomes==&quot;FALSE&quot;, &quot;Loss&quot;, &quot;Win&quot;)) 12.1.1 Calculating Likelihood If you already have data (or some idea of what the parameters are for a distribution) and you want to find the likelihood, use the density functions that start with d. For likelihood of getting 6 wins out of 10 trials, and a 50% of winning on each trial, use the density function for a binomial distribution: dbinom(6, size=10, prob=0.5) ## [1] 0.2050781 12.2 Calculating Interrater reliability Cohens Kappa is useful for IRR agreement on categorical variablesUse the psych package for this see here; and read this web page for an overview of what Cohens Kappa is if you need a recap/intro. For 3+ raters on a continuous variable, use Intraclass Correlation. See this page. 12.3 Statistical tests and modeling with easystats https://easystats.github.io/easystats/ 12.3.1 Getting parameter estimates from model objects Scenario: Youve run some statistical test (like the below regression), and want a summary of the model estimates. rm(iris) model &lt;- lm(Sepal.Length ~ Species, data = iris) You have a few options when it comes to getting a summary of a model and getting the coefficient estimates: - summary() - broom::tidy() - paramters::model_paramters(), or just paramters::paramters() for short Theres no reason to use summary, generally speaking, because it sucks. It doesnt give you tidy output thats easy to manipulate or extract, its hard to read, and it cant be turned into a useful table. Skip it unless you need something specific from its output (i.e., youre using lavaan) Options two and three are pretty similar and both give you most of the same information, though parameters() prints neater to the console window. Generally I find parameters preferable. Note though that neither command will round the numbers if you store it as a table in the environment. So. If you want to manipulate ANY info in the table and/or extract info, use tidy or parameters. Both make tidy tibbles. If youre using the command to export said info in a neat, presentable MS Word table or HTML table, and you do not care about extracting/modifying/manipulating anything in it, then use parameters and pipe it to format_table() Using format_table() rounds all columns to 2 decimal places, reformats p-values to APA format, and collapses CIs into a single column. Do note though that it makes every column into a Character column! So this is for exporting-use only. Heres a comparison of brooms output (first) vs. parameters (second) when you save each in the environment. As you can see, both produce tidy tibbles And heres what parameters(model) |&gt; format_table() does to the a parameters table: Much cleaner for making a table to export to Word. 12.3.2 Getting model information and performance metrics Again, two options here. You can use either glance from the broom package, or performance from the package of the same name. These each produce slightly different output, though unlike above, I dont think one is necessarily better than the other. Use whichever one you prefer. broom::glance(model) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.619 0.614 0.515 119. 1.67e-31 2 -112. 231. 243. 39.0 147 150 performance::performance(model) ## # Indices of model performance ## ## AIC | BIC | R2 | R2 (adj.) | RMSE | Sigma ## ----------------------------------------------------- ## 231.452 | 243.494 | 0.619 | 0.614 | 0.510 | 0.515 12.3.3 Effect size info with effectsize logreg_model=glm(smoke ~ age + sex, data= legaldmlab::survey, family = &quot;binomial&quot;) logreg_model_coeff=parameters::parameters(logreg_model) logreg_model_coeff=logreg_model_coeff |&gt; dplyr::mutate(odds_ratio=exp(Coefficient)) effectsize::interpret_oddsratio(logreg_model_coeff$odds_ratio, rules = &quot;chen2010&quot;) ## [1] &quot;small&quot; &quot;very small&quot; &quot;very small&quot; ## (Rules: chen2010) 12.3.4 Quick, detailed, and automated reporting with report Check out https://easystats.github.io/report/ 12.3.5 Running correlations with correlation https://easystats.github.io/correlation/ "],["advanced-coding-tips.html", "Chapter 13 Advanced Coding Tips 13.1 Grammar and Syntax 13.2 Creating a package 13.3 Creating a bookdown", " Chapter 13 Advanced Coding Tips 13.1 Grammar and Syntax 13.1.1 Regex expressions and symbols str_remove(html$`Market Price`, pattern = &quot;$&quot;) # doesn&#39;t remove the $ sign str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) # works 13.1.2 The colon-equals (:=) operator Sometimes when making a function you need to use the colon-equals operator, rather than just the normal &lt;- or = assignment operators Specifically, when you have multiple named arguments in your functionRead my question and someones answer on this blogpost: https://community.rstudio.com/t/help-creating-simple-function/109011/2 function(df, col, newCol_name){ # drop all NA&#39;s so the function can work properly df_NAdropped=df |&gt; drop_na({{col}}) # apply the function, saving it in a small tibble with only the outlier column and a key to join by outliers_key=df_NAdropped |&gt; dplyr::mutate(outliers=dplyr::if_else(round(abs({{col}}-median({{col}}))/(1.483*mad({{col}}, constant = 1)),2)&gt;2.24,1,0)) |&gt; select(subject_id, outliers) # join back together df=df |&gt; left_join(outliers_key, by=c(&quot;subject_id&quot;)) num.outliers=df |&gt; dplyr::filter(outliers==1) |&gt; dplyr::count() df$outliers=replace_na(df$outliers, 999) df=df |&gt; rename({{newCol_name}}:=&quot;outliers&quot;) message(paste0(num.outliers, &quot; outlier(s) detected&quot;)) return(df) } 13.1.3 User-supplied expressions or named columns in functions This is when you have to put double braces around something 13.1.4 When a command requires a named column or data set, but youve already supplied it and its required a second time If youre writing a function with a pipe but the command youre using needs the data set defined in it, you specify it as .x , or simply just .; Here is an example: 13.2 Creating a package https://rstudio4edu.github.io/rstudio4edu-book/data-pkg.html 13.2.1 Documenting package meta-data https://r-pkgs.org/description.html 13.2.2 Connecting to other packages https://kbroman.org/pkg_primer/pages/depends.html 13.2.3 Linking Git and Github view this detailed guide by Jenny Bryan, and this YouTube video if you want the full guide; or just follow the TL;DR below. Quick summary of steps in YouTube video: Open project folder in Windows Explorer and click in the URL bar, then type cmd to open command prompt If there are any pre-existing git files or repository info there, remove it with the following: rd .git /S/Q Tell git to create a new repo by typing: git init Then tell it to include all files in the current place by typing: git add . Commit these files with: git commit -m \"Initial commit\" At this point youve created a git and GitHub repo each; now link them with: git remote add origin [https URL of GitHub repo] Push all these changes live with: git push -u origin master 13.3 Creating a bookdown https://www.youtube.com/watch?app=desktop&amp;v=m5D-yoH416Y&amp;feature=youtu.be 13.3.1 Rendering the book once its done Render locally with bookdown::render_book(index.Rmd) Use browseURL(\"docs/index.html\") to view your book locally (or just open index.html in a browser). If it looks good, commit and push all changed files to GitHub. "],["creating-a-simulated-data-set.html", "Chapter 14 Creating a simulated data set 14.1 Part 1: Independent samples from a normal distribution 14.2 Part 2: Creating data sets with quantitative and categorical variables 14.3 Part 3: Repeatedly simulate samples with replicate() 14.4 Part 4: repeatedly making whole data sets 14.5 Part 5: Using purrr", " Chapter 14 Creating a simulated data set From the tutorial on this page 14.1 Part 1: Independent samples from a normal distribution Consider the following first before you start doing stuff: - How many subjects are in each condition? - What are the means and standard deviations of each group? Set that shit below. # number of subjects per group A_sub_n &lt;- 50 B_sub_n &lt;- 50 # distribution parameters A_mean &lt;- 10 A_sd &lt;- 2.5 B_mean &lt;- 11 B_sd &lt;- 2.5 Now generate scores for each group A_scores &lt;- rnorm(A_sub_n, A_mean, A_sd) B_scores &lt;- rnorm(B_sub_n, B_mean, B_sd) Technically you could stop here and just analyze the data in this fashionbut its better to organize it into a table. One that looks like something you would import after real data collection. So do that next; make it look nice. dat &lt;- tibble( sub_condition = rep( c(&quot;A&quot;, &quot;B&quot;), c(A_sub_n, B_sub_n) ), score = c(A_scores, B_scores) ) head(dat) ## # A tibble: 6 x 2 ## sub_condition score ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 8.50 ## 2 A 8.26 ## 3 A 13.4 ## 4 A 11.6 ## 5 A 4.04 ## 6 A 9.68 Always perform a quality and consistency check on your data to verify that shits ok. dat %&gt;% group_by(sub_condition) %&gt;% summarise(n = n() , mean = mean(score), sd = sd(score)) ## # A tibble: 2 x 4 ## sub_condition n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 50 10.1 2.53 ## 2 B 50 10.4 2.04 14.2 Part 2: Creating data sets with quantitative and categorical variables From the web page at this link 14.2.1 2.a. DATA WITH NO DIFFERENCE AMONG GROUPS Critically important notes to know: When you use the rep() function, there are several different arguments you can specify inside it that control how stuff is repeated: using rep(x, each= ) repeats things element-wise; each element gets replicated n times, in order rep(c(&quot;A&quot;,&quot;B&quot;), each=3) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; using rep(x, times= ) repeats the sequence; the vector as a whole, as it appears, will be repeated with one sequence following the next rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), times=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; using rep(x, length.out) repeats only the number of elements you specify, in their original order rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), length.out=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; In this particular data, we want every combination of group and letter to be present ONCE. letters=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;) tibble(group = rep(letters[1:2], each = 3), factor = rep(LETTERS[3:5], times = 2), response = rnorm(n = 6, mean = 0, sd = 1) ) ## # A tibble: 6 x 3 ## group factor response ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A C -0.355 ## 2 A D 0.178 ## 3 A E 0.610 ## 4 B C -0.465 ## 5 B D 0.401 ## 6 B E -0.727 14.2.2 2.b. Data WITH A DIFFERENCE among groups What if we want data where the means are different between groups? Lets make two groups of three observations where the mean of one group is 5 and the other is 10. The two groups have a shared variance (and so standard deviation) of 1. 14.2.2.1 Some notes first Creating a difference between the two groups average score means we have to tell R to sample itteratively from distributions with different means. We do this by specifying a vector of means within rnorm, like so: response = rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 2.754076 9.609848 5.285164 9.798403 4.205527 7.954296 You can see that: 1. draw 1 is from the distribution \\((\\mu=5,\\sigma=1)\\) 2. draw 2 is from the distribution \\((\\mu=5,\\sigma=1)\\) And this process repeats a total of six times. And if you happen to also specify a vector of standard deviations (purely to demonstrate what is happening, we wont actually do this), the first mean is paired with the first SD; the second mean is paired with the second SD; and so on. rnorm(n = 6, mean = c(5, 10), sd = c(2,0.1)) ## [1] 4.987252 9.950492 4.149175 9.974044 2.077211 10.115511 14.2.2.2 Ok, back to creating the data If you want there to be differences between the groups, we need to change the way the vector of factors is replicated, in addition to specifying the vector of means. We want to ensure that the sequence of A, B in the group column matches the sequence repeated in the response column. Here we are going to use length.out so that the whole sequence of A,B is repeated exactly in line with the alternating drawing from \\(\\mu=5\\), \\(\\mu=10\\). Its often best to do this by building each thing separately, and then combining it into a tibble when you have it figured out. group=rep(letters[1:2], length.out = 6) group ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; response=rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 4.126291 8.969935 3.874072 9.828078 5.393620 9.125079 tibble(group, response) ## # A tibble: 6 x 2 ## group response ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 4.13 ## 2 B 8.97 ## 3 A 3.87 ## 4 B 9.83 ## 5 A 5.39 ## 6 B 9.13 14.2.3 2.c. Data with MULTIPLE QUANTITATIVE VARIABLES with groups 14.3 Part 3: Repeatedly simulate samples with replicate() Instead of drawing values one at a time from a distribution, we want to do it many times. This is a job for replicate(). What replicate() does is run a function repeatedly. The replicate() function will perform a given operation as many times as you tell it to. Here we tell it to generate numbers from the distribution \\(N~(\\mu=0, \\sigma=1)\\), three times (as specified in the n=3 argument in line one) replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = FALSE ) ## [[1]] ## [1] -0.51287813 0.63520496 0.91589225 0.07678404 0.14043466 ## ## [[2]] ## [1] -0.4779480 -0.9642717 0.2279148 -0.4471080 0.1064011 ## ## [[3]] ## [1] -0.7956522 0.8792829 -0.3071313 0.1094047 0.1135288 The argument simplify=FALSE tells it to return the output as a list. If you set this to TRUE it returns a matrix instead replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) ## [,1] [,2] [,3] ## [1,] 0.6787630 -1.3669316 -0.9293416 ## [2,] -0.8953333 -0.9607781 -1.9034088 ## [3,] -0.6243654 1.0369531 -1.0106750 ## [4,] 0.2827911 0.1516815 1.7664099 ## [5,] 1.6380746 -1.3831001 1.8825689 Specifying as.data.frame() with the matrix output can turn it into a data frame. replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) %&gt;% as.data.frame() %&gt;% rename(sample_a=V1, sample_b=V2, sample_c=V3) ## sample_a sample_b sample_c ## 1 0.088180361 0.6301284 -1.3532818 ## 2 1.542517588 1.5851132 -1.7461287 ## 3 -0.294668964 0.7840522 -1.2377223 ## 4 0.003613039 0.9598645 -0.2833658 ## 5 -0.697784050 1.2285486 0.9498306 14.4 Part 4: repeatedly making whole data sets This is combining parts 2 and 3 to repeatedly create and sample data sets, resulting in a list of many data sets. simlist = replicate(n = 3, expr = data.frame(group = rep(letters[1:2], each = 3), response = rnorm(n = 6, mean = 0, sd = 1) ), simplify = FALSE) simlist ## [[1]] ## group response ## 1 A 1.3681197 ## 2 A 0.1706618 ## 3 A 0.1858557 ## 4 B 2.2231589 ## 5 B 0.8016017 ## 6 B 0.6890034 ## ## [[2]] ## group response ## 1 A 1.69969710 ## 2 A 0.01731652 ## 3 A 0.91465872 ## 4 B -2.81221913 ## 5 B -0.09187946 ## 6 B -0.63539913 ## ## [[3]] ## group response ## 1 A 0.5521644 ## 2 A -0.3389562 ## 3 A -0.2929999 ## 4 B 0.2710472 ## 5 B -1.3079854 ## 6 B -1.8641999 14.5 Part 5: Using purrr See this blog post Kruschke, J. (2015). Goals, power, and sample size. In J. K. Kruschke (Ed.), Doing bayesian data analysis: A tutorial with r, jags, and stan (2nd ed., pp. 359-398). Academic Press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
