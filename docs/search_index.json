[["index.html", "Creating a simulated data set Chapter 1 A Monument to my Madness 1.1 What this book is, and what it is not", " Creating a simulated data set Ryan Schneider 2022-02-08 Chapter 1 A Monument to my Madness This book contains all my personal coding notes from the last two years. Why am I doing this? Probably because Im a glutton for punishment, and Id rather procrastinate than write my dissertation proposal. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 1.1 What this book is, and what it is not You know those absolutely amazing, comprehensive guides where you can learn everything you need to know about R? This is is not one of those guides. This book is designed as a quick reference guide for many of the most common things youll need to do in everyday data analysis and research. Think of it like a coding dictionary, as opposed to a manual or comprehensive text. If you want (or need) to learn R in-depth and/or from the ground up (i.e., youre a novice user), then you should go read Hadley Wickhams book and the tidyverse websites. Also, these slides might be a good high-level overview if youve never used the tidyverse before. That said, if youre already familiar with R and the tidyverse and just need a quick reference for what command do I need to accomplish XYZ, youve come to the right place. "],["introduction-r-basics.html", "Chapter 2 Introduction: R Basics 2.1 Importing Data 2.2 Exporting (i.e., saving) Data and Output", " Chapter 2 Introduction: R Basics For the love of God before you do anything, familiarize yourself with R Projects and the here package. These make R so much more user friendly and less of a nightmare. If you need an overview, go here: http://jenrichmond.rbind.io/post/how-to-use-the-here-package/ Now lets get stuck in. library(tidyverse) 2.1 Importing Data 2.1.1 Spreadsheets See https://nacnudus.github.io/spreadsheet-munging-strategies/index.html for more detailed and in-depth tutorials (if you need that kind of thing) 2.2 Exporting (i.e., saving) Data and Output 2.2.1 Exporting to .CSV Generally speaking, unless you have a specific reason to, dont. But if you must: write_csv() 2.2.2 Export to .RData (and load the data again later) save(obj_name, file=here::here(&quot;subfolder&quot;, &quot;save_file_name&quot;), compress = FALSE) load(here::here(&quot;folder&quot;, &quot;save_name.RData&quot;)) 2.2.3 Export to Excel library(openxlsx) #Method 1: If you only want to export 1 thing, and/or only need output document #write as object, with no formatting: write.xlsx(objectname,file = &quot;filenamehere.xlsx&quot;,colnames=TRUE, borders=&quot;columns&quot;) #write as table: write.xlsx(objectname,&quot;filename.xlsx&quot;,asTable = TRUE) #Method 2: If you want to do the above, but add multiple objects or tables to one workbook/file: ## first Create Workbook object wb &lt;- createWorkbook(&quot;AuthorName&quot;) #then add worksheets (as many as desired) addWorksheet(wb, &quot;worksheetnamehere&quot;) #then write the object to the worksheet writeData(wb, &quot;test&quot;, nameofobjectordataframe, startCol = 2, startRow = 3, rowNames = TRUE) #save excel file saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) #Method 3: exact same as method 2, but creating a more fancy tables wb &lt;- createWorkbook(&quot;Ryan&quot;) addWorksheet(wb, &quot;worksheetnamehere&quot;) writeDataTable(wb, sheetName, objectName, startCol = 1, startRow = 1, colNames = TRUE, rowNames = FALSE, tableStyle=&quot;TableStyleLight2&quot;,tableName=NULL, headerStyle = NULL,withFilter=FALSE,keepNA=TRUE,sep=&quot;, &quot;, stack = FALSE, firstColumn = FALSE, lastColumn = FALSE,bandedRows = TRUE,bandedCols = FALSE) saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) 2.2.4 Access/edit specific cell number values rainbow=tibble::tribble(~Color, &quot;red&quot;, &quot;orange&quot;, &quot;black&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) rainbow$Color[3] # access, but can&#39;t overwrite this way ## [1] &quot;black&quot; rainbow[3,&quot;Color&quot;] # access and can overwrite ## # A tibble: 1 x 1 ## Color ## &lt;chr&gt; ## 1 black rainbow[3, &quot;Color&quot;]= &quot;yellow&quot; # save this value to row 3 in column &quot;Color&quot; rainbow ## # A tibble: 6 x 1 ## Color ## &lt;chr&gt; ## 1 red ## 2 orange ## 3 yellow ## 4 green ## 5 blue ## 6 purple "],["wrangle-data.html", "Chapter 3 Wrangle Data 3.1 Joining or Splitting 3.2 Selecting/extracting specific variables 3.3 If-then and Case-when 3.4 Conditional replacement of values 3.5 Merging variables 3.6 Apply a function to multiple variables at once 3.7 Pivoting (i.e., transposing) data 3.8 Managing Many Models 3.9 Turn row names into a column/variable 3.10 How to edit/change column names 3.11 Re-order columns in a data set 3.12 Date and time variables 3.13 Reverse-code a variable 3.14 Create a relative ranking among several variables", " Chapter 3 Wrangle Data This chapter contains useful tips on wrangling (i.e., manipulating) data. If you need to know to do to things like create new variables, split one variable into multiple variables, pivot a data set from wide to long, etc., look no further. If you want a pretty good intro tutorial to the dplyr package, click here 3.1 Joining or Splitting Joining and splitting data is pretty straightforward. 3.1.1 Whole Data Sets The code below is from this excellent tutorial set.seed(2018) df1=data.frame(customer_id=c(1:10), product=sample(c(&#39;toaster&#39;,&#39;TV&#39;,&#39;Dishwasher&#39;),10,replace = TRUE)) df2=data.frame(customer_id=c(sample(df1$customer_id, 5)),state=sample(c(&#39;New York&#39;,&#39;California&#39;),5,replace = TRUE)) df1=tibble::as_tibble(df1) df2=tibble::as_tibble(df2) # df1 =left table # df2= right table Inner join - retains only rows with values that appear in both tables, and matches by keys. If youre joining two Qualtrics surveys together, this is most likely the one you want to use (e.g.Â matching by participant name, and only keeping rows in the joined data set for participants that have responses logged in both survey 1 and survey 2 df1 %&gt;% inner_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York Left join - returns everything in the left, and rows with matching keys in the right df1 %&gt;% left_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Right join - returns everything in the right, and rows with matching keys in the left df1 %&gt;% right_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York # note: example if the customer id column was named something different in the second df #df1 %&gt;% left_join(df2,by=c(&#39;customer_id&#39;=&#39;name2&#39;)) Full join - retain all rows from both tables, and join matching keys in both right and left df1 %&gt;% full_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Anti join - returns all rows in the left that do not have matching keys in the right df1 %&gt;% anti_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 2 ## customer_id product ## &lt;int&gt; &lt;chr&gt; ## 1 2 Dishwasher ## 2 4 toaster ## 3 5 TV ## 4 7 toaster ## 5 10 TV 3.1.2 Individual Columns/Variables Splitting or joining columns is much easier than doing it to whole data sets. You can use dplyr::separate() to accomplish the former, and dplyr::unite() for the latter. print(&quot;hello&quot;) ## [1] &quot;hello&quot; 3.2 Selecting/extracting specific variables Sometimes when working with a data set, you want to work with a few specific variables. For instance, maybe you want to view a graph of only reverse-coded variables (which start with the prefix r); or maybe you want to create a subset of your data that has a few specific variables removed. For this you can use dplyr::select() and its associated helper commands select() can be thought of as extract; it tells R to identify and extract a specific variable (or variables) cars=mtcars # select one column cars %&gt;% select(mpg) # select multiple columns, if they are all next to one another cars %&gt;% select(mpg:hp) # select multiple columns by name (when not next to one another) by defining them in a vector cars %&gt;% select(c(mpg, hp, wt)) # select only variables that start with a certain prefix/character/pattern/etc. cars %&gt;% select(starts_with(&quot;d&quot;)) # ...or columns that end with a certain prefix/etc. cars %&gt;% select(ends_with(&quot;t&quot;)) # ...or contains a certain pattern or string cars %&gt;% select(contains(&quot;se&quot;)) # select ALL OF the variables in a data set that match those of a pre-defined vector # first define the names in a vector vars=c(&quot;hp&quot;, &quot;drat&quot;, &quot;gear&quot;, &quot;carb&quot;) #now use helper cars %&gt;% select(all_of(vars)) # select ANY OF the variables in a pre-defined vector vars_2=c(&quot;hp&quot;, &quot;drat&quot;, &quot;watermelon&quot;, &quot;grilled_cheese&quot;) # only the first two will be in the data cars %&gt;% select(any_of(vars_2)) # only (and all of) the variables actually PRESENT in the data are pulled # select only variables of a certain class or type cars %&gt;% select(where(is.numeric)) cars %&gt;% select(where(is.character)) Other examples can be seen on THIS LINK for a simple but detailed guide. 3.3 If-then and Case-when 3.3.1 If-then The premise of an if/then or if/else statement is simple: If condition 1 is satisfied, perform x operation; if not, then do y mtcars %&gt;% mutate(power_level=ifelse(mtcars$hp&lt;350, &quot;Low&quot;, &quot;High&quot;)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb power_level ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Low ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Low ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Low ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Low ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Low ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Low This line of code effectively says: if the length in Sepal.Length is &gt;5, set new variable = to short; else, set it to long 3.3.2 Case-when When you have 3+ conditions, its easier to use case-when. This is a more simple and straightforward approach than nesting multiple if-else commands My_vector= case_when( Condition1 ~ value1, Condition2 ~ value2, Condition3 ~ value3 TRUE ~ valueForEverythingElse #catch all for things that don&#39;t meet the above conditions ) Example: mtcars %&gt;% mutate(size= case_when(cyl==4 ~ &quot;small&quot;, cyl==6 ~ &quot;medium&quot;, cyl==8 ~ &quot;large&quot;)) %&gt;% select(c(cyl,size)) %&gt;% head() ## cyl size ## Mazda RX4 6 medium ## Mazda RX4 Wag 6 medium ## Datsun 710 4 small ## Hornet 4 Drive 6 medium ## Hornet Sportabout 8 large ## Valiant 6 medium 3.4 Conditional replacement of values The following code is useful if you want to replace a value in one column, and the replacement is conditional upon the value in another column. mpg %&gt;% mutate(across(.cols = c(displ, cty, hwy), .fns = ~case_when(cyl == 4L ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 NA 1999 4 auto(l5) f NA NA p compact ## 2 audi a4 NA 1999 4 manual(m5) f NA NA p compact ## 3 audi a4 NA 2008 4 manual(m6) f NA NA p compact ## 4 audi a4 NA 2008 4 auto(av) f NA NA p compact ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact ## 7 audi a4 3.1 2008 6 auto(av) f 18 27 p compact ## 8 audi a4 quattro NA 1999 4 manual(m5) 4 NA NA p compact ## 9 audi a4 quattro NA 1999 4 auto(l5) 4 NA NA p compact ## 10 audi a4 quattro NA 2008 4 manual(m6) 4 NA NA p compact ## # ... with 224 more rows test %&gt;% mutate(across(.cols = c(rank), .fns = ~case_when(is.na(participant_score) ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) 3.5 Merging variables Sometimes youll have multiple variables and you want to collapse them into a single variable. The pmin() command is useful for this. example_data=tribble(~A,~B,~C, 1,NA,NA, 2,NA,NA, 3,NA,NA, NA,4,NA, NA,5,NA, NA,6,NA, NA,NA,7, NA,NA,8, NA,NA,9) example_data %&gt;% mutate(accept_reject = pmin(A,B,C,na.rm = TRUE)) 3.6 Apply a function to multiple variables at once You can either specify each column individually, like above, or tell R to identify columns for you based on their type or their name. This requires adding in one additional verbeither contains() or where() depending on what you want to do. Two simple examples: # turn multiple variables into factors ex_data=dplyr::tribble(~color, ~car, &quot;red&quot;, &quot;corvette&quot;, &quot;blue&quot;, &quot;chevelle&quot;, &quot;green&quot;, &quot;camaro&quot;, &quot;red&quot;, &quot;corvette&quot;, &quot;green&quot;, &quot;chevelle&quot;, &quot;yellow&quot;, &quot;gto&quot;) dplyr::glimpse(ex_data) ## Rows: 6 ## Columns: 2 ## $ color &lt;chr&gt; &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;green&quot;, &quot;yellow&quot; ## $ car &lt;chr&gt; &quot;corvette&quot;, &quot;chevelle&quot;, &quot;camaro&quot;, &quot;corvette&quot;, &quot;chevelle&quot;, &quot;gto&quot; ex_data %&gt;% mutate(across(c(color, car),factor)) ## # A tibble: 6 x 2 ## color car ## &lt;fct&gt; &lt;fct&gt; ## 1 red corvette ## 2 blue chevelle ## 3 green camaro ## 4 red corvette ## 5 green chevelle ## 6 yellow gto # round multiple columns to 1 decimal place mtcars %&gt;% mutate(across(c(disp:qsec),round,1)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.9 2.6 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.9 2.9 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.9 2.3 18.6 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.1 3.2 19.4 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.1 3.4 17.0 0 0 3 2 ## Valiant 18.1 6 225 105 2.8 3.5 20.2 1 0 3 1 3.7 Pivoting (i.e., transposing) data 3.7.1 Condense multiple rows into a single column (pivot wide to long) Rearranging data like this can make it easier to work with and analyze. Example below from my gradebook for stats (exported from Canvas), with fake names. The command structure is as follows: pivot_longer( # Transpose LENGTHWISE by.... cols = everything(), # Taking ALL variable names... names_to=&quot;variable&quot;, # ...and dumping them into this new variable/column values_to=&quot;missing_count&quot;) #...and placing their values in this other new column NOTE!!! Pivoting data from wide to long like this expands the number of rows to make a matrix so that (for example, each student now has as a row for each assignment). Therefore, you can only pivot longways (or wide) ONCE, otherwise you will make duplicates. If you need to pivot multiple columns, just include all of the columns in one single pivot; do not use two separate, back to back pivot commands. Example: gradebook=tibble::tribble( ~Student, ~Homework.1, ~Homework.2, ~Homework.3, ~Homework.4, ~Homework.5, ~Quiz.1, ~Quiz.2, ~Quiz.3, ~Quiz.4, ~Final, &quot;Bob&quot;, 19L, 0L, 13, 16, 0L, 21, 7L, 15, 17.5, 33, &quot;Jane&quot;, 17L, 19L, 16, 16.5, 25L, 21.5, 19L, 14.75, 9.5, 39.5, &quot;John&quot;, 19L, 19L, 14.5, 19.5, 25L, 21, 21L, 18.5, 17, 46.5 ) head(gradebook) ## # A tibble: 3 x 11 ## Student Homework.1 Homework.2 Homework.3 Homework.4 Homework.5 Quiz.1 Quiz.2 Quiz.3 Quiz.4 Final ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bob 19 0 13 16 0 21 7 15 17.5 33 ## 2 Jane 17 19 16 16.5 25 21.5 19 14.8 9.5 39.5 ## 3 John 19 19 14.5 19.5 25 21 21 18.5 17 46.5 gradebook=gradebook %&gt;% pivot_longer( # Transpose lengthwise by: cols = Homework.1:Final, # Taking these variables names_to=&quot;Assignment&quot;, # ...and dumping them into this new variable, storing them lengthwise values_to=&quot;Points&quot;) #...then place their values in this new column gradebook %&gt;% head() ## # A tibble: 6 x 3 ## Student Assignment Points ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bob Homework.1 19 ## 2 Bob Homework.2 0 ## 3 Bob Homework.3 13 ## 4 Bob Homework.4 16 ## 5 Bob Homework.5 0 ## 6 Bob Quiz.1 21 3.8 Managing Many Models Imagine the concept of Russian Dolls, applied to data sets. You can manage data sets more effectively my collapsing them into a single tiny, mini data frame, and stuffing that inside of another one. This is done via nesting Effectively, you smush/collapse everything down so it fits inside one column. You can unnest to expand this data back out later when you need it, and keep it collapsed when you dont. Code works like this: by_country=gapminder::gapminder %&gt;% group_by(continent,country) %&gt;% # indicate the variables to keep at the top level nest() # smush the rest into a list-column country_model=function(df){ lm(lifExp~year1950,data = df) } # Transform a list of models into a df models=by_country %&gt;% mutate(mod=map(data,country_model)) You can store anything in a data frame. You can keep the df connected to the model, which makes it very easy to manage a whole slew of related models You can use functional programming (i.e., iterative functions) to map functions or combinations of functions in new ways. Converting data into tidy data sets gives you a whole new way (and easier way) to manage lots of information Below is the full script I copied from Hadley Wickhams lecture, which you can watch here pacman::p_load(dplyr,purrr,tidyverse,gapminder) #### Workflow for managing many models in R #### # 1. Nest data with {tidyr} # 2. Use {purrr} to map a modeling function # 3. Use {broom} to inspect your tidy data gapminder=gapminder %&gt;% mutate(year1950= year-1950) #the number of years it&#39;s been since 1950 #-------------------------------------------------------------------------------------------- #### Step 1. Nest the data. #### # A nested data frame has one column per country. You&#39;re essentially # creating a Russian doll; a data frame inside of a larger data frame. by_country=gapminder %&gt;% group_by(continent,country) %&gt;% # variables to keep at the top level nest() # smush everything else into a df, and store this mini-df in its own column # with this, you can have an entire table per row; a whole data frame for each country # Essentially condensing a list into a table by_country$data[[1]] #-------------------------------------------------------------------------------------------- #### Step 2. Use purrr to map stuff. #### # 12:50 country_model=function(df){ lm(lifeExp ~ year1950, data = df) } models= by_country %&gt;% mutate( mod=map(data,country_model) ) gapminder %&gt;% group_by(continent,country) %&gt;% nest() %&gt;% mutate( mod= data %&gt;% map(country_model) ) # 27:11 #-------------------------------------------------------------------------------------------- ##### Step 3. #### # This creates another nested df inside of your main data frame that has the summary stats of each model models=models %&gt;% mutate( tidy=map(mod, broom::tidy), # tidy() gives model estimates glance=map(mod,broom::glance), # glance() gives model summaries augment=map(mod,broom::augment) # model coefficients ) # What can you do with this nest of data frames? # The reverse of step 1; un-nest it to unpack everything! # 34:40 # Keeps a massive list of related information neatly organized! unnest(models,data) # back to where we started unnest(models,glance, .drop = TRUE) unnest(models,tidy) and here is a version I made of the above to manage many Bayesian models. Admittedly, Im not really sure how useful this is though. # CONDENSED MASTER TABLE VERSION ----------------------------------------------------------------------------- # Models table that has all models condensed models=tribble(~Model_name, ~model_descrip, ~model, &quot;Thesis_Model&quot;, &quot;Discount and PTS&quot;, Thesis_Model, &quot;discount_model&quot;, &quot;Discount variable only&quot;, discount_model, &quot;PTS_model&quot;, &quot;PTS variable only&quot;, PTS_model ) # Clean up work space #rm(DiscountPrior,Priors_MEmodel,Priors_Interactionmodel) # Grab and store all model info models=models %&gt;% mutate(prior_info=map(model,describe_prior), posterior_info=map(model, describe_posterior_fancy), model_performance=map(model,performance::performance) ) # DO NOT TRY AND VIEW THE TABLE IN A WINDOW!!!! RStan objects are so large they cause R to lock up # Call the model in the console instead #### summon individual model stats #### describe_prior(models$model[[1]]) #by specific model map(models$model,describe_prior) # do for all models at once # or all info for all models at once unnest(models,posterior_info) %&gt;% select(-c(model,prior_info,model_descrip)) 3.9 Turn row names into a column/variable Use the rownames() command to turn row names into a variable cars=rownames_to_column(mtcars, var = &quot;car&quot;) as_tibble(cars) %&gt;% slice(1:6) ## # A tibble: 6 x 12 ## car mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 Hornet 4 Drive 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 3.10 How to edit/change column names TWO WAYS TO DO THIS: Use colnames() (for base R) or rename() (for tidyverse) colnames() pulls up all the column/variable names as a vector. If you want to actually change them, youll need to combine this command with something like the sub() or gsub() commands (for base R). Im going to skip this becauseits base R. To access and change the names faster via tidyverse, run use rename() rm(list=ls()) # clear R&#39;s memory iris %&gt;% rename(&quot;hurr&quot;=&quot;Sepal.Length&quot;, &quot;durr&quot;=&quot;Sepal.Width&quot;, &quot;abcdefgh&quot;=&quot;Species&quot;) %&gt;% head() ## hurr durr Petal.Length Petal.Width abcdefgh ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa If you need to do some really fancy conditional renaming (e.g., changing all variables that start with r to start with rf instead, to make it more clear that the prefix actually stands for risk factor rather than reverse coded), youll need to use rename_with(). This command has two parts to it: the data set, and the function you wish to apply to it (which you put after the ~) rename_with(iris, ~ gsub(pattern = &quot;.&quot;, replacement = &quot;_&quot;, .x, fixed = TRUE)) %&gt;% head() ## Sepal_Length Sepal_Width Petal_Length Petal_Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa The gsub() function from Base R identifies matching patterns in the data and substitutes them with what you want instead. Think of it like Rs version of Find/Replace from Microsoft Word. The above line of code thus does the following: 1. First, it checks the column names of the supplied data set (iris) for a specific pattern (specified in pattern= ) 2. Then it replaces that pattern with your input in replacement= The great thing about rename_with() is that the .fn (or ~ for short) can take ANY function as input. For example, if you want to add an element to the column names rather than replace something, (e.g., a prefix or suffix), you can change the function to: rename_with( iris, ~ paste0(.x, &quot;_text&quot;)) %&gt;% head() ## Sepal.Length_text Sepal.Width_text Petal.Length_text Petal.Width_text Species_text ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa The above line adds a suffix. You can also add a prefix in the exact same way, just by switching the order of the string and the pattern in the paste0 command. Alternative method to the above This is a second way to do the above. It may appear more simple, but its also probably not as theoretically consistent with how the packages were made..it uses the stringr package to rename the column names, and stringr is typically used for editing vectors of strings in a data set. so it works, but its a little unconventional because you call and edit the column names like you would a variable in your data set. colnames(iris)=str_replace(colnames(iris), pattern = &quot;.&quot;, replacement = &quot;_&quot;) In short: rename() and rename_with() are for renaming variables, as their names imply. The str_ verbs from the stringr package are for editing string-based variabels in your data set. Either works though with a little ingenuity. 3.11 Re-order columns in a data set Use relocate() to change column positions. If you need to move multiple columns at once, this command uses the same syntax as select(). mtcars # notice the column order ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 mtcars %&gt;% relocate(hp:wt, .after= am) %&gt;% head() ## mpg cyl disp qsec vs am hp drat wt gear carb ## Mazda RX4 21.0 6 160 16.46 0 1 110 3.90 2.620 4 4 ## Mazda RX4 Wag 21.0 6 160 17.02 0 1 110 3.90 2.875 4 4 ## Datsun 710 22.8 4 108 18.61 1 1 93 3.85 2.320 4 1 ## Hornet 4 Drive 21.4 6 258 19.44 1 0 110 3.08 3.215 3 1 ## Hornet Sportabout 18.7 8 360 17.02 0 0 175 3.15 3.440 3 2 ## Valiant 18.1 6 225 20.22 1 0 105 2.76 3.460 3 1 3.12 Date and time variables Formatting a column of dates can be extremely helpful if you need to work with time data, but also an extreme pain in the ass. It requires things to be done in two stages, and very precisely. Particularly in the first stage. First, assuming your data is already imported and is being stored as a vector of character strings, you have to tell R to adjust the formatting of dates. You cannot change it from a character-based object into a Date or DateTime one until it recognizes the correct formatting. example_date_data=tibble::tribble(~X1, ~X2, &quot;8/4/2021&quot;, -49.87, &quot;8/4/2021&quot;, -13.85, &quot;8/3/2021&quot;, -7.45, &quot;8/3/2021&quot;, -172.71, &quot;8/2/2021&quot;, -6.37, &quot;8/2/2021&quot;, -25, &quot;8/2/2021&quot;, -219.68, &quot;8/2/2021&quot;, -53.75, &quot;8/2/2021&quot;, -29.83, &quot;8/2/2021&quot;, -77.06, &quot;8/2/2021&quot;, -16.16, &quot;8/2/2021&quot;, -114.78, &quot;8/2/2021&quot;, -50, &quot;8/2/2021&quot;, -157.64) # Correct formatting example_date_data$X1=format(as.POSIXct(example_date_data$X1,format=&#39;%m/%d/%Y&#39;),format=&#39;%Y-%m-%d&#39;) head(as_tibble(example_date_data)) ## # A tibble: 6 x 2 ## X1 X2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 2021-08-04 -49.9 ## 2 2021-08-04 -13.8 ## 3 2021-08-03 -7.45 ## 4 2021-08-03 -173. ## 5 2021-08-02 -6.37 ## 6 2021-08-02 -25 In the code above, note that there are two format commands: The first one tells R how the date data is currently being stored, while the second at the end tells it how you want it to be stored. In this case, we are changing it from the way we would usually hand write a date (e.g., 10/26/1993) to a format commonly recognized and used in Excel and stats software (1993-10-26). If your column also has times in it, you also need to include that too! Second, you can now correct the objects structure. You can do this with base Rs as.Date() or tidyverses date() verbs. # Correct structure example_date_data$X1= lubridate::date(example_date_data$X1) # tidyverse # Base R version # example_date_data$X1=as.Date(example_date_data$X1) head(as_tibble(example_date_data)) ## # A tibble: 6 x 2 ## X1 X2 ## &lt;date&gt; &lt;dbl&gt; ## 1 2021-08-04 -49.9 ## 2 2021-08-04 -13.8 ## 3 2021-08-03 -7.45 ## 4 2021-08-03 -173. ## 5 2021-08-02 -6.37 ## 6 2021-08-02 -25 Notice how the object is now stored as the correct type in the table above. NOTE! This entire process has been included in the tidy_date() command in my package, legaldmlab. 3.12.1 Find the difference between two dates/times difftime(part_1$end_date[1], part_2$end_date[1], units=&quot;days&quot;) 3.13 Reverse-code a variable To reverse-score a variable, you should use car::recode() Can be done a few different ways, depending on how many variables youre looking to recode: # Recode just one variable df$column=recode(df$column,&quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;) # Recode a select bunch of variables df=df %&gt;% mutate(across(c(family_close : family_feelings), recode, &quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;)) # Recode the whole damn thing. All columns. df=df %&gt;% map_df(recode, &quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;) 3.14 Create a relative ranking among several variables If you want to create a variable that is an ordinal ranking of other variables, first you need to make sure your data is long-wise. Then, depending on the type of ranking system you want, youll might need a different ranking command. The min_rank command from dplyr works in a manner similar to base Rs rank command. It ranks things like you see in sporting events. For example, if there is a clear winner in a game but 3 people tie for second place, the ranks would look like this: 1,2,2,2,4,5. Notice that the positions are independent from the counts. Using the same example from above, if you want the ranks to have no gaps (i.e.Â 1,2,2,2,3,4), you need to use dplyrs dense_rank command. In either case, the ranks are generated from lowest to highest, so if you want to flip them around youll need to include desc() in the command. dat=tibble::tribble(~name, ~score, &quot;bob&quot;, 0, &quot;bob&quot;, 5, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, NA, &quot;alice&quot;, 70, &quot;alice&quot;, 80, &quot;alice&quot;, 90, &quot;alice&quot;, 20, &quot;alice&quot;, 20, &quot;alice&quot;, 1) dat %&gt;% mutate(ranked = dense_rank(desc(score))) ## # A tibble: 12 x 3 ## name score ranked ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 bob 0 8 ## 2 bob 5 6 ## 3 bob 50 4 ## 4 bob 50 4 ## 5 bob 50 4 ## 6 bob NA NA ## 7 alice 70 3 ## 8 alice 80 2 ## 9 alice 90 1 ## 10 alice 20 5 ## 11 alice 20 5 ## 12 alice 1 7 "],["clean-data.html", "Chapter 4 Clean Data 4.1 Replace a value with NA 4.2 Replace NAs with a value 4.3 Identify columns or rows with Missing values 4.4 Find the percentage of a variable that is missing 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set", " Chapter 4 Clean Data 4.1 Replace a value with NA Use dplyr::na_if() if you have a value coded in your data (e.g., 999) that you want to convert to NA example_data=dplyr::tribble(~name, ~bday_month, &quot;Ryan&quot;, 10, &quot;Z&quot;, 3, &quot;Jen&quot;, 999, &quot;Tristin&quot;, 999, &quot;Cassidy&quot;, 6) example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen 999 ## 4 Tristin 999 ## 5 Cassidy 6 example_data$bday_month=na_if(example_data$bday_month, 999) #example doing one column at a time example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 example_data %&gt;% # can also pass the data to mutate and do it the tidyverse way mutate(bday_month=na_if(bday_month, 999)) ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 4.2 Replace NAs with a value tidyr::replace_na() is very useful if you have some NAs in your data and you want to fill them in with some value. example_data=tibble::tribble(~name, ~fav_color, ~fav_food, &quot;Ryan&quot;, &quot;green&quot;, &quot;Mexican&quot;, &quot;Cassidy&quot;, &quot;blue&quot;, NA, &quot;Z&quot;, NA, NA, &quot;Tristin&quot;, &quot;purple&quot;, NA, &quot;Tarika&quot;, NA, NA, &quot;Jen&quot;, NA, &quot;Italian&quot;) example_data ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue &lt;NA&gt; ## 3 Z &lt;NA&gt; &lt;NA&gt; ## 4 Tristin purple &lt;NA&gt; ## 5 Tarika &lt;NA&gt; &lt;NA&gt; ## 6 Jen &lt;NA&gt; Italian # replace NA&#39;s in one col tidyr::replace_na(example_data$fav_food, &quot;MISSING&quot;) ## [1] &quot;Mexican&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;Italian&quot; # replace in multiple columns example_data %&gt;% mutate(across(c(fav_color, fav_food), replace_na, &quot;MISSING&quot;)) ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue MISSING ## 3 Z MISSING MISSING ## 4 Tristin purple MISSING ## 5 Tarika MISSING MISSING ## 6 Jen MISSING Italian 4.3 Identify columns or rows with Missing values is.na() is the base R way to identify, in a TRUE/FALSE manner, whether or not there are missing values in a vector y &lt;- c(1,2,3,NA) is.na(y) # returns a vector (F F F T) ## [1] FALSE FALSE FALSE TRUE 4.4 Find the percentage of a variable that is missing Sometimes necessary to check before conducting an analysis. This requires my package, legaldmlab ?legaldmlab::count_missing mtcars %&gt;% select(hp:drat) %&gt;% legaldmlab::count_missing() ## # A tibble: 2 x 3 ## variable missing_count percent_missing ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 hp 0 0.0% ## 2 drat 0 0.0% 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set Use tidyr::drop_na() to remove rows with missing values. example_data=dplyr::tribble(~name, ~bday_month, ~car, &quot;Ryan&quot;, 10, &quot;kia&quot;, &quot;Z&quot;, NA, &quot;toyota&quot;, &quot;Jen&quot;, NA, NA, &quot;Tristin&quot;, 999, NA, &quot;Cassidy&quot;, 6, &quot;honda&quot;) knitr::kable(example_data) name bday_month car Ryan 10 kia Z NA toyota Jen NA NA Tristin 999 NA Cassidy 6 honda example_data %&gt;% drop_na() # with nothing specified, it drops ALL variables that have &gt;=1 missing value ## # A tibble: 2 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Cassidy 6 honda example_data %&gt;% drop_na(car) # drops only rows with values missing in the specified column ## # A tibble: 3 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Z NA toyota ## 3 Cassidy 6 honda "],["working-with-factors.html", "Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels 5.2 Collapse factor levels 5.3 Add levels to a factor 5.4 Drop unused levels 5.5 Change the order of a factors levels", " Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels Use forcats::fct_recode() diamonds=diamonds %&gt;% as_tibble() diamonds$cut=fct_recode(diamonds$cut, &quot;meh&quot;=&quot;Fair&quot;, &quot;Wow&quot;=&quot;Premium&quot;) summary(diamonds$cut) ## meh Good Very Good Wow Ideal ## 1610 4906 12082 13791 21551 5.2 Collapse factor levels Extremely useful command for when you have infrequent cases in one factor and need to combine it with another. Works by specifying a series of new level names, each of which contains the information from the old variables. Format is as follows: fct_collapse(dataset$variable, NewLevelA=c(&quot;OldLevel1&quot;,&quot;Oldlevel2&quot;), # NewLevelA is the new variable that contains both variables 1 and 2 NewLevelB=c(&quot;OldLevel3&quot;)) 5.3 Add levels to a factor use fct_expand() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.4 Drop unused levels Use fct_drop() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.5 Change the order of a factors levels example_data=tribble(~person, ~condition, &quot;bob&quot;, &quot;25 years&quot;, &quot;jane&quot;, &quot;5 years&quot;, &quot;jim&quot;, &quot;5 years&quot;, &quot;john&quot;, &quot;25 years&quot;) example_data$condition=factor(example_data$condition) str(example_data$condition) ## Factor w/ 2 levels &quot;25 years&quot;,&quot;5 years&quot;: 1 2 2 1 Notice that R thinks these are nominal factors, and that 25 comes before 5. To fix this and correct the level order example_data$condition =fct_relevel(example_data$condition, c(&quot;5 years&quot;, &quot;25 years&quot;)) # specify level order str(example_data$condition) ## Factor w/ 2 levels &quot;5 years&quot;,&quot;25 years&quot;: 2 1 1 2 "],["working-with-strings.html", "Chapter 6 Working with Strings 6.1 Remove a pattern from a string 6.2 Replace one pattern in a string with another 6.3 Find (i.e., filter for) all instances of a string 6.4 Drop all rows from a data set that contain a certain string 6.5 Force all letters to lower case", " Chapter 6 Working with Strings 6.1 Remove a pattern from a string price_table=tribble(~car, ~price, &quot;Corvette&quot;, &quot;$65,000&quot;, &quot;Mustang GT&quot;, &quot;$40,000&quot;) # BASE R METHOD (sub by replacing something with nothing) gsub(&quot;\\\\$&quot;, &quot;&quot;,price_table$price) # (pattern, replace with, object$column) ## [1] &quot;65,000&quot; &quot;40,000&quot; # TIDYVERSE METHOD str_remove(price_table$price, pattern = &quot;\\\\$&quot;) ## [1] &quot;65,000&quot; &quot;40,000&quot; 6.2 Replace one pattern in a string with another Tidyverse command: str_replace() Base R command: gsub() # base R gsub(mtcars, replacement = ) #tidyverse 6.3 Find (i.e., filter for) all instances of a string Useful for finding very specific things inside a column (e.g., one particular persons name in a roster of names; everyone with a particular last name) Tidyverse command: str_detect() Base R command: grepl() Note both must be nested inside of filter() cars_df=rownames_to_column(mtcars, var = &quot;car&quot;) # base R cars_df |&gt; filter(grepl(&quot;Firebird&quot;, car)) # tidyverse cars_df %&gt;% filter(str_detect(car,&quot;Firebird&quot;)) You can also search for multiple strings simultaneously by including the or logical operator inside the quotes. cars_df |&gt; filter(str_detect(car, &quot;Firebird|Fiat&quot;)) You can also include the negation logical operator to filter for all instances except those with the specified string. # base R cars_df |&gt; filter(!(grepl(&quot;Pontiac&quot;, car))) # tidyverse cars_df |&gt; filter(!(str_detect(car, &quot;Pontiac&quot;))) 6.4 Drop all rows from a data set that contain a certain string # Tidyverse method cars_df |&gt; filter(str_detect(car, &quot;Merc&quot;, negate = TRUE)) #including negate=TRUE will negate all rows with the matched string # base R cars_df[!grepl(&quot;Merc&quot;, cars_df$car),] 6.5 Force all letters to lower case Use stringr::str_to_lower() blah=tribble(~A, ~B, &quot;A&quot;,&quot;X&quot;, &quot;A&quot;,&quot;X&quot;) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 A X ## 2 A X blah$A=str_to_lower(blah$A) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 a X ## 2 a X "],["figures-and-graphs-with-the-ggplot-and-see-packages.html", "Chapter 7 Figures and Graphs with the ggplot and see packages 7.1 Commands for ggplot graph types 7.2 Specific Commands for Specific Types of Analysis 7.3 Highlight specific points 7.4 Add labels to data points 7.5 Plotting multiple graphs at once 7.6 Change the colors (bars; columns; dots; etc.) 7.7 Other aesthetic mappings 7.8 Adding and Customizing Text 7.9 Remove gridlines 7.10 Faceting 7.11 Log transformations 7.12 Changing the scale of the axis 7.13 Add a regression line", " Chapter 7 Figures and Graphs with the ggplot and see packages There are three parts to a ggplot2 call: 1. data 2. aesthetic mapping 3. Layer There is no piping involved in ggplot. You simply invoke ggplot, and tell it what they dataset is. Then you specify the aesthetics, and then the mapping. Lastly, include other optional stuff (e.g.Â expanded y-axis scale; titles and legends; etc.) Every single plot has the exact same layout that ONLY USES the above three points: ggplot(dataframe, aes(graph dimensions and variables used)) + geom_GraphType(specific graph controls) ## OR ## ggplot(dataframe) + geom_GraphType(aes(graph dimensions and variables used), specific graph controls) # mapping= aes() can go in either spot Then if you have other stuff you want to add on top of this, like axis labels, annotations, highlights, etc., you keep adding those in separate lines 7.1 Commands for ggplot graph types Graph Type Geom command Scatter geom_point() Line geom_line() Box geom_boxplot() Bar geom_bar() Column geom_col() Histogram geom_histogram() Density curve geom_density() Note that bar and column graphs look identical at first glance, but they serve two different purposes. Bar graphs are for frequency counts, and thus only take an X-axis variable; Column graphs are for showing the relationship between two variables X and Y, and display the values in the data # BAR GRAPH # height of bars is a frequency count of each level of the X variable cut bar_plot=ggplot(diamonds, aes(x=cut)) + geom_bar()+ theme_classic() # COLUMN GRAPH # height of bars represents relationship between price and cut col_plot=ggplot(diamonds, aes(x=cut, y=price)) + geom_col()+ theme_classic() see::plots(bar_plot, col_plot, n_columns = 2, tags = c(&quot;Bar&quot;, &quot;Column&quot;)) 7.2 Specific Commands for Specific Types of Analysis 7.2.1 lavaan stuff 7.2.1.1 Plotting an SEM or CFA model First lets set up a model to use. library(lavaan) ## This is lavaan 0.6-9 ## lavaan is FREE software! Please report any bugs. HS.model &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9&#39; fit1 &lt;- cfa(HS.model, data=HolzingerSwineford1939) Two options for graphing it. Option 1 is graph_sem() from the tidySEM package. tidySEM::graph_sem(fit1) Option 2 is from the easystats suite plot(parameters::parameters(fit1)) ## Using `sugiyama` as default layout 7.2.2 Bayes stuff Quick highlights here of my favorite functions from this package. See (ha) the full package overview at this link You can adjust the colors of the figures by setting them yourself (with scale_fill_manual), or by using the appropriate scale_fill command 7.2.2.1 Probability of Direction (Pd) figure Use plot(pd()) to visualize the Probability of Direction index. plot(bayestestR::pd(fit1))+ scale_fill_manual(values=c(&quot;#FFC107&quot;, &quot;#E91E63&quot;))+ theme_classic()+ theme(plot.title = element_text(hjust = 0.5, size = 14, face = &quot;italic&quot;)) 7.2.2.2 ROPE figure plot(fit1, rope_color = &quot;grey70&quot;)+ gameofthrones::scale_fill_got_d(option = &quot;white_walkers&quot;) # scale_fill_manual(values = c(&quot;gray75&quot;,&quot;red&quot;) ROPE tests are plots of distributions, and therefore use scale_fill_xyz_d commands. (the d stands for discrete). You can use any scale theme color set from any package, as long as it ends in _d values=c(#FFC107, #E91E63) is the default bayestestR theme colors from their website 7.2.2.3 Bayes factor models comparison figure plot(bayesfactor_models(Thesis_Model,discount_model))+ scale_fill_flat(palette = &quot;complement&quot; , reverse = TRUE)+ # scale color adjustment 7.2.3 Histograms and density curves Since I use these so often I figure they deserve their own special section. Basic histograms can be built with the following code: ggplot(data = mtcars, aes(x=cyl)) + geom_histogram(binwidth = .5, colour=&quot;Black&quot;, fill=&quot;green&quot;) + # histogram theme_classic() and your basic density curve with the following: ggplot(diamonds, aes(x=price)) + geom_density(alpha=.3)+ # density plot. Alpha sets the transparency level of the fill. theme_classic() You can also use the following code from bayestestR to build a really quick and nice density curve plot(bayestestR::point_estimate(diamonds, centrality=c(&quot;median&quot;,&quot;mean&quot;)))+ labs(title=&quot;Mean and Median&quot;) 7.3 Highlight specific points The gghighlight package is great for this # example 1 ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp))+ # add data labels (optional) gghighlight::gghighlight(hp &gt; 200) # add highlights, according to some criteria # example 2 diamonds_abr=diamonds %&gt;% slice(1:100) ggplot(diamonds_abr, aes(x= cut, y= price, colour=price))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = diamonds_abr, aes(label = price))+ # this line labels gghighlight::gghighlight(cut %in% c(&quot;Very Good&quot;, &quot;Ideal&quot;)) #this line highlights 7.4 Add labels to data points ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp)) ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point() + geom_text(aes(label=hp, hjust=2.5, vjust=2.5)) #geom_label(aes(label = scales::comma(n)), size = 2.5, nudge_y = 6) 7.5 Plotting multiple graphs at once see::plots() is good for this. print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.6 Change the colors (bars; columns; dots; etc.) This can be done in at least two different ways, depending on your goal. To change the fill color by factor or group, add fill = ___ within the aes() command. If you want to add color and/or fill to a continuous variable, do that within the geom_density() command. If you want to add color and make all of the (bars; dots; lines; etc.) the same color, than that is a graph-wide control and needs to be put in geom_point(). This manually sets the color for the whole graph. # add a color scale to the dots ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(color=&quot;blue&quot;) If you want to add color that changes according to a variable (e.g., by factor level), then the color needs to be specified as a variable name, in the aes mapping with the other variables. ggplot(mtcars, aes(x= mpg, y=hp, color=cyl))+ geom_point() 7.6.1 Fine-tuning colors You can change the spectrum of colors to specific colors if you want. Useful for example, when making graphs for APLS presentations; you can change the colors to be Montclair State University themed. When changing the color scale of graphs, note that scale_fill commands are used for representing nominal data, while scale_color commands are for representing continuous data. As such, you use scale_fill to fill in area on a graph that shows a whole category or distinct things; and scale_color to use gradients of color to show changes in continuous data. For figures that have solid area (e.g., density; box; bar; violin plots; etc.), use scale_fill For figures that have continuous changes (e.g., line and scatter plots), use scale_color # Set colors manually ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_manual(values=c(&quot;green&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;blue&quot;)) ggplot(mtcars, aes(x = wt, y = mpg, color=as.factor(cyl)))+ geom_point() + scale_color_manual(values=c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;)) # Use color scales from a package library(gameofthrones) # NOTICE THAT scale_fill AND scale_color STILL APPLY TO THEIR RESPECTIVE GRAPH TYPES # bar graphs ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Tully&quot;) ggplot(mtcars, aes(factor(cyl), fill=factor(vs)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Daenerys&quot;) # scatter plot ggplot(mtcars, aes(x = mpg, y = disp, colour = hp))+ geom_point(size = 2) + scale_colour_got(option = &quot;Lannister&quot;) Fill graphs also come with an extra option: Setting the outline color. You can change the outline of the bar/column/etc. by specifying the color inside geom_x() # change only the fill of the bars ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() # Change the outline of the bars by adding color inside the geom_bar() command ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar(color=&quot;black&quot;) 7.6.2 More options with the see package See this link for setting color gradients for continuous variables, or using other custom color palattes like the gameofthrones package. Check out the see package for some good color scales; the commands for which are here. Incidentally, see is great not only for regular ggplot graphs, but also Bayesian stats graphs link; effect size graphs link; correlation graphs link; and more. 7.7 Other aesthetic mappings shape() controls the shapes on the graph alpha() controls transparency size() controls size Note again that if you want it to change by variable, it goes INSIDE aes(); but if you want to set it manually for the whole graph, it goes in geom_x() # shape ggplot(mtcars, aes(x= mpg, y=hp, shape=as.factor(cyl)))+ geom_point() ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(shape=23) # transparency ggplot(mtcars, aes(x= mpg, y=hp, alpha=hp))+ geom_point() # size ggplot(mtcars, aes(x= mpg, y=hp, size=cyl))+ geom_point() 7.8 Adding and Customizing Text 7.8.1 Add a title, axis labels, and captions All three can be added with labs(). ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;) 7.8.2 Center graph title Add the line theme(plot.title = element_text(hjust = 0.5)) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 7.8.3 Use different fonts See tutorial on this web page Or, use the extrafont package, and set everything using the theme() command. # Visualize new groups library(extrafont) loadfonts(device=&quot;win&quot;) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;)) 7.9 Remove gridlines Add theme(panel.grid = element_blank()) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;))+ theme(panel.grid = element_blank()) 7.10 Faceting This is dividing one plot into subplots, in order to communicate relationships better. Again, this is just a single extra command, this time at the end of the code: facet_wrap(~columnhead) The tilde sign in R means by, as in divide (something) by this print(&quot;temp&quot;) This line produces a graph of population and life expectency, breaking it down to make a separate graph per each continent 7.11 Log transformations Sometimes when your data is really squished together on a graph it is hard to read. In this case, log transformations are really helpful, to change the scale of the data. For example, by multiplying all your points by 10x To create a log transformation of the same scatter plot above, add one extra bit: scale_x_log10() print(&quot;temp&quot;) You can also make both axis be logged by adding +scale again for y 7.12 Changing the scale of the axis Add coord_cartesian(xlim = c(lower,upper)) print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.13 Add a regression line Add the line geom_smooth(method = \"lm\", formula = y ~ x) ggplot(mtcars, aes(x= mpg, y=hp, color=mpg))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, formula = y ~ x) "],["making-tables-with-flextable.html", "Chapter 8 Making Tables with flextable 8.1 APA Table Components 8.2 Indent values 8.3 Add a Horizontal border (AKA horizontal spanner) 8.4 Change font and font size 8.5 Grouped table 8.6 Complete Example", " Chapter 8 Making Tables with flextable NOTES: - j refers to the column - i refers to the row number 8.1 APA Table Components 8.2 Indent values https://davidgohel.github.io/flextable/reference/padding.html https://stackoverflow.com/questions/64134725/indentation-in-the-first-column-of-a-flextable-object Use the padding function: ft &lt;- padding(ft, i=2, j=1, padding.left=20) 8.3 Add a Horizontal border (AKA horizontal spanner) hline(., i=4, j=1:2, part = &quot;body&quot;) 8.4 Change font and font size glm_table&lt;-flextable::font(glm_table, part = &quot;all&quot;, fontname = &quot;Times&quot;) %&gt;% # Font fontsize(., size = 11, part = &quot;all&quot;) # Font size 8.5 Grouped table cars=rownames_to_column(mtcars, var = &quot;Model&quot;) test=flextable::as_grouped_data(x=cars, groups = c(&quot;cyl&quot;)) 8.6 Complete Example "],["misc.-stuff.html", "Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables 9.2 Read SPSS files into R 9.3 Turn numbers into percentages 9.4 Find all possible combindations of items in a vector 9.5 Download files from the internet 9.6 Print multiple things in one statement", " Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables Note. See Chapter 10s example purrr walk through for a guide on how to scrape multiple web tables simultaneously Simple example. library(rvest) ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding library(tidyverse) html=read_html(&#39;https://shop.tcgplayer.com/price-guide/pokemon/base-set&#39;) %&gt;% html_table(fill = TRUE) html ## [[1]] ## # A tibble: 101 x 6 ## PRODUCT Rarity Number `Market Price` `Listed Median` `` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abra Common 43 $0.43 $0.60 View ## 2 Alakazam Holo Rare 1 $30.58  View ## 3 Arcanine Uncommon 23 $2.23 $2.22 View ## 4 Beedrill Rare 17 $4.13 $4.00 View ## 5 Bill Common 91 $0.35 $0.47 View ## 6 Blastoise Holo Rare 2 $109.78  View ## 7 Bulbasaur Common 44 $2.33 $2.69 View ## 8 Caterpie Common 45 $0.70 $0.66 View ## 9 Chansey Holo Rare 3 $18.60  View ## 10 Charizard Holo Rare 4 $350.72  View ## # ... with 91 more rows # Saved as a list by default. Now extract your table from said list html=as_tibble(html[[1]] %&gt;% # find out which number it is in the list select(&#39;PRODUCT&#39;,&#39;Rarity&#39;,&#39;Number&#39;,&#39;Market Price&#39;)) # if needed, specify which columns you want too html ## # A tibble: 101 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Abra Common 43 $0.43 ## 2 Alakazam Holo Rare 1 $30.58 ## 3 Arcanine Uncommon 23 $2.23 ## 4 Beedrill Rare 17 $4.13 ## 5 Bill Common 91 $0.35 ## 6 Blastoise Holo Rare 2 $109.78 ## 7 Bulbasaur Common 44 $2.33 ## 8 Caterpie Common 45 $0.70 ## 9 Chansey Holo Rare 3 $18.60 ## 10 Charizard Holo Rare 4 $350.72 ## # ... with 91 more rows # remove $ symbol in Price column to make it easier to work with html$`Market Price`=str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) html=html %&gt;% mutate(`Market Price`=as.numeric(`Market Price`)) # convert from string to numeric # view finished table head(html) ## # A tibble: 6 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Abra Common 43 0.43 ## 2 Alakazam Holo Rare 1 30.6 ## 3 Arcanine Uncommon 23 2.23 ## 4 Beedrill Rare 17 4.13 ## 5 Bill Common 91 0.35 ## 6 Blastoise Holo Rare 2 110. Slightly more complicated example Reading a table into R takes a few steps. Step 1 is to copy and paste the URL into the read_html() verb like below: pacman::p_load(rvest, tidyverse) exonerations_table=read_html(&quot;https://www.law.umich.edu/special/exoneration/Pages/detaillist.aspx&quot;) %&gt;% html_nodes(&quot;table.ms-listviewtable&quot;) %&gt;% html_table(fill=TRUE, header = TRUE) Sometimes if the web page is extremely basic and pretty much the only thing on it is a table, you can stop there. Most of the time though, there will be tons of other stuff on the website and you need to get more specific so R can find the table. This is the html_nodes() part of the above command; in there you specify the exact part of the web page where the table is located/what object file it is. To find this you will need to use the Developer mode in your browser. See this screenshot for an example knitr::include_graphics(here::here(&quot;pics&quot;, &quot;scrape.png&quot;)) In Firefox you open this by going to Settings &gt; More Tools &gt; Web Developer Tools (or CNTRL + Shift + I). Begin by looking through the console in the center bottom for names that look like they would be related to your table. A good place to start might be  , which contains the main body of the web page. Click on a name to expand it and see all the elements on the page contained there. Ultimately what youre looking for is what you see above: an element that, when selected, highlights ONLY the area of the web page youre looking for. To get at this you will need to keep expanding, highlighting, and clicking repeatedly.it can take some digging. Keep drilling down through page elements until you find the one that highlights the table and just the table. When you find this, look for the .ms file in that name; you should also see this in the smaller console box on the right. That is the file youll need. Write that name in the html_node command and read it into R. Thats stage 1. From here you now need to clean up the table. exonerations_table=as.data.frame(exonerations_table) # convert into a df Your table might be different, but this ones names were messed up when read in, so lets fix those first and then fix the rows and columns. # save the names to a vector table_names=exonerations_table$Last.Name[1:20] # Trim out the garbage rows and columns exonerations_table=exonerations_table %&gt;% select(Last.Name:Tags.1) %&gt;% slice(22:n()) # over-write incorrect col names with the vector of correct ones we saved above colnames(exonerations_table)=table_names # clean up names exonerations_table=exonerations_table %&gt;% janitor::clean_names() # verify structure of columns is correct # glimpse(exonerations_table) Yikes, a lot of stuff is stored incorrectly, and as a result theres some missing values that need to be addressed and other data that needs to be corrected. exonerations_table=as_tibble(exonerations_table) %&gt;% # convert to tibble mutate(across(c(dna,mwid:ild), na_if,&quot;&quot;)) %&gt;% # turn missing values into NA&#39;s mutate(across(c(dna,mwid:ild), replace_na, &quot;derp&quot;)) %&gt;% # replace NA&#39;s with a string (required for the next lines to work) mutate(dna=ifelse(dna==&quot;DNA&quot;,1,0), # change these variables from text to numeric to better facilitate analysis mwid=ifelse(mwid==&quot;MWID&quot;,1,0), fc=ifelse(fc==&quot;FC&quot;,1,0), p_fa=ifelse(p_fa==&quot;P/FA&quot;,1,0), f_mfe=ifelse(f_mfe==&quot;F/MFE&quot;,1,0)) %&gt;% mutate(across(c(st, crime, dna:f_mfe),factor)) # correct form by converting to factors And thats it! Check out final result! head(exonerations_table) ## # A tibble: 6 x 20 ## last_name first_name age race st county_of_crime tags om_tags crime sentence convicted exonerated dna x mwid fc p_fa f_mfe om ild ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbitt Joseph 31 Black NC Forsyth CV, IO~ &quot;&quot; Child ~ Life 1995 2009 1 &quot;&quot; 1 0 0 0 derp derp ## 2 Abdal Warith Habib 43 Black NY Erie IO, SA &quot;OF, WH,~ Sexual~ 20 to Li~ 1983 1999 1 &quot;&quot; 1 0 0 1 OM derp ## 3 Abernathy Christopher 17 White IL Cook CIU, C~ &quot;OF, WH,~ Murder Life wit~ 1987 2015 1 &quot;&quot; 0 1 1 0 OM derp ## 4 Abney Quentin 32 Black NY New York CV &quot;&quot; Robbery 20 to Li~ 2006 2012 0 &quot;&quot; 1 0 0 0 derp derp ## 5 Acero Longino 35 Hispanic CA Santa Clara NC, P &quot;&quot; Sex Of~ 2 years ~ 1994 2006 0 &quot;&quot; 0 0 0 0 derp ILD ## 6 Adams Anthony 26 Hispanic CA Los Angeles H, P &quot;OF, WH,~ Mansla~ 12 years 1996 2001 0 &quot;&quot; 0 0 1 0 OM derp Check out this page for a quick overview. 9.2 Read SPSS files into R Use foreign::read.spss spss_version=foreign::read.spss(here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;, &quot;JLWOP_RYAN.sav&quot;), to.data.frame = TRUE) Might also want to add as_tibble() on the end. 9.3 Turn numbers into percentages Use scales::percent(), which converts normal numbers into percentages and includes the percent sign (%) afterwards simple_table=tribble(~n_people, ~votes_in_favor, 25, 14) simple_table=simple_table %&gt;% mutate(percent_voted_for=scales::percent(votes_in_favor/n_people, accuracy = 0.1, scale = 100)) simple_table ## # A tibble: 1 x 3 ## n_people votes_in_favor percent_voted_for ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 25 14 56.0% Scale is what to multiple the original number by (e.g., convert 0.05 to 5% by x100) Accuracy controls how many places out the decimal goes 9.4 Find all possible combindations of items in a vector y &lt;- c(2,4,6,8) combn(c(2,4,6,8),2) # find all possible combinations of these numbers, drawn two at a time ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 2 2 2 4 4 6 ## [2,] 4 6 8 6 8 8 9.5 Download files from the internet 9.6 Print multiple things in one statement Use cat() from base R cat(&quot;The p-value dropped below 0.05 for the first time as sample size&quot;, 100) ## The p-value dropped below 0.05 for the first time as sample size 100 "],["intermediate-r-functions-loops-and-iterative-programming.html", "Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions 10.2 For-loops 10.3 purrr and Iterative Functions 10.4 Other purrr commands", " Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions A function is a command that performs a specified operation and returns an output in accordance with that operation. You can literally make a function to do anything you want. General structure of a basic function: # example structure Function_name=function(argument){ Expressions return(output) } Argument is your input. It is the thing you want to perform the operation on. Expressions is the actual operation (or operations) you want to perform on the supplied argument return tells R to return the result of the Expression to you when done. This example function takes an input of numbers in the form of a vector and subtracts two from each. numbers=c(2,10,12,80) sub_2=function(x){ result= x - 2 return(result) } sub_2(numbers) ## [1] 0 8 10 78 We can also supply the function with a single number and it still works sub_2(100) ## [1] 98 Well this looks useful. So whats the bigger picture? One of the primary advantages of functions are that they can reduce a long and complex process, or a process that involves many steps, into a single line of code; thus, creating your own functions is a fast way to make your life easier down the line either at some point in the far future or even in just a few minutes, if you know you will be writing the code for some process two or more times. Take this script for instance. You can see from the circled parts that I needed to transform three different data sets in a similar way: knitr::include_graphics(here::here(&quot;pics&quot;, &quot;repeat_process.jpg&quot;)) Yes, I could have just done a copy-paste of the original code and tweak it slightly each time. But that is time consuming, produces a sloppier and longer script, and introduces a lot more room for error because of the repeated code and extra steps. Better to write a single function that could be applied to all three. In short, use functions to reduce a multi-step process or a process that youre implementing &gt;=2 times in a single script into one command. This saves you space and makes the script shorter; it saves you the trouble and effort of re-writing or adapting code from earlier sections; and importantly, reduces the chances of you making a coding error by proxy of the former two. As a quick example, I was able to replace each of the circled paragraphs of code above with a custom function that ran everything in one simple line. Now instead of 3 whole (and redundant) paragraphs, I now have 3 short lines, like so. na_zero_helpreint=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;reintegrate_&quot;) na_blank=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) na_zero=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) Limitations to your average, everyday functions. While reducing a whole process or sequence of commands is extremely useful, it still leaves a limitation. For instance, while we avoided copying and pasting whole paragraphs or processes, I still had to copy-paste the same function three times. This still leaves chances for error on the table, and it still leaves us with wasted lines that make the script longer. In general, when you want to perform some function or process multiple times on multiple items (as above where the same command is used three times on three different data frames), you need to use a for-loop or iterating function. These can reduce further unwanted redundancies by applying the function or process iteratively. Read on for more info. 10.2 For-loops A for loop is essentially a function that applies a function or given set of operations to multiple things at once, and returns an output of many items. For example, this code finds the means of every vector/column in a dataset by repeatedly applying the same code over and over to element i in the given list: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) output &lt;- vector(&quot;double&quot;, ncol(df)) # 1.Output. Create the object you want the results of the loop stored in. for (i in seq_along(df)) { # 2.Sequence of operations. &quot;For each item &#39;i&#39; along data frame&quot; output[[i]] &lt;- median(df[[i]]) # 3.Body:&quot;every individual item in &#39;output&#39; = the median of each col in df } output ## [1] 0.3771802 -0.5176346 0.4171879 0.5704655 Check out this book chapter for a great and detailed explanation of for-loops and functional coding. Although for loops are nice, they are unwieldy. R programmers typically use iterating functions instead. Examples of iterating functions are the lapply, vapply, sapply, etc. family of base R commands. But these can also be confusing and the commands are not great. The purrr package offers a better way to do iterating functions over base R; its the tidyverse way to make efficient and understandable for loops! If you have a need for a for-loop for something, see the next section instead on how to use purrr to make an iterative function. Important to understand conceptually what a for-loop is, but using them is impractical when you have purrr 10.3 purrr and Iterative Functions All notes here come from Charlotte Wickhams lecture tutorial below Part 1: https://www.youtube.com/watch?v=7UlWJWfZO9M Part 2: https://www.youtube.com/watch?v=b0ozKTUho0A&amp;t=1210s purrrs map() series of functions offer a way to apply any existing function (even functions youve made) to multiple things at once, be it lists, data frame columns, individual items in vector, etc. In short, they are for doing the same type of task repeatedly in a very quick and efficient manner. They work in much the same way as for-loops, but are far simpler to write, and can be applied in the same way to solve the same problems. How to use purrr The structure of map() commands is the same as the others in the tidyverse: #option 1 map(data, function) # option 2 data %&gt;% map(function) As a quick example and to highlight why purrr is so much more efficient and easier to use than for-loops, look at the same example from before, now using map() instead of a for: df |&gt; map_dbl(median) ## a b c d ## 0.3771802 -0.5176346 0.4171879 0.5704655 A single line is all it took to get the same results! And, it follows tidyverse grammar structure. Now lets get into how it works. map() commands work like this: For each element of x, do f. So if you pass it object x and object x is. - A vector, it will perform function f on every item in the vector - A data frame, it will perform function f on every column in the data frame - A list, it will perform function f on every level in the list Etc., etc.; the point is it applies a function repeatedly to every element in the object you supply it with. So lets walk through a case example. 10.3.1 Reproducible example: Scraping web data This is an example walk through showing how we can use purrr to speed things up dramatically and/or reduce the use of unwanted, extra code in our scripts. In this guide Ill be building a table of LPGA Tour statistics from multiple webpages. The workflow for purrr goes like this: First, you want to figure out how to do each step of your process line-by-line, for a single item. The idea is to try and walk through each step of the process and see exactly what will need to be done each each step and what the code will like, before trying to code it all at once at a higher level. Once you have each step for the first item figured out, then you make functions for each step that condense that code down to one command. Lastly, apply each function from your individual steps to all items in your list by using purr::map(). Do for One library(rvest) # STEP 1 # Figure out a line-by-line process for one item/one single web page html1=read_html(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; html_table(fill = TRUE, header=TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() head(html1) ## rank name distance ## 1 1 Bianca Pagdanganan 284.000 ## 2 2 Emily Pedersen 276.750 ## 3 3 Lexi Thompson 276.286 ## 4 4 Charley Hull 275.714 ## 5 5 Alana Uriell 274.000 ## 6 6 Frida Kinhult 273.750 # STEP 2 # create a custom function of the above to shorten and generalize the process quick_read_html=function(url){ web_page=read_html(url) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; # fortunately this node works for all four pages so it can be baked into the function html_table(fill = TRUE, header = TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() return(web_page) } # test to verify it works test=quick_read_html(url= &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;) head(test) # nice ## rank name putt_average ## 1 1 Leona Maguire 1.655 ## 2 2 Danielle Kang 1.661 ## 3 3 Brooke Henderson 1.701 ## 4 4 Su-Hyun Oh 1.707 ## 5 5 Carlota Ciganda 1.711 ## 6 6 Madelene Sagstrom 1.711 DO FOR ALL. Now create the object that contains all the elements you want to iterate over, and then pass it to your generalized function with map. # Step 3a # create an object that contains ALL elements of interest URLs=c(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=06&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=12&quot;) # Step 4 # use the power of map and be amazed lpga_data= URLs |&gt; map(quick_read_html) head(lpga_data) ## [[1]] ## rank name distance ## 1 1 Bianca Pagdanganan 284.000 ## 2 2 Emily Pedersen 276.750 ## 3 3 Lexi Thompson 276.286 ## 4 4 Charley Hull 275.714 ## 5 5 Alana Uriell 274.000 ## 6 6 Frida Kinhult 273.750 ## 7 7 Pauline Roussin-Bouchard 273.071 ## 8 8 Janie Jackson 270.600 ## 9 9 Jennifer Kupcho 270.333 ## 10 10 Nanna Koerstz Madsen 270.300 ## 11 11 Maria Fassi 270.125 ## 12 12 Yuka Saso 270.050 ## 13 13 Patty Tavatanakit 269.773 ## 14 14 Brooke Henderson 268.727 ## 15 15 Nelly Korda 268.636 ## 16 16 Luna Sobron 268.500 ## 17 17 Carlota Ciganda 268.143 ## 18 18 Amanda Doherty 267.750 ## 19 19 Atthaya Thitikul 267.429 ## 20 20t Cydney Clanton 266.714 ## 21 20t Rachel Rohanna 266.714 ## 22 22 Sarah Kemp 265.750 ## 23 23 Peiyun Chien 265.375 ## 24 24t Fatima Fernandez Cano 265.200 ## 25 24t Yealimi Noh 265.200 ## 26 26 Amy Yang 265.071 ## 27 27 Madelene Sagstrom 265.045 ## 28 28t Agathe Laisne 264.500 ## 29 28t Giulia Molinaro 264.500 ## 30 30 Xiyu Lin 264.286 ## 31 31 Jaye Marie Green 263.917 ## 32 32 A Lim Kim 263.909 ## 33 33 Alison Lee 263.900 ## 34 34 Gerina Mendoza 263.417 ## 35 35 Laura Davies 263.400 ## 36 36 Maude-Aimee Leblanc 262.833 ## 37 37 Allisen Corpuz 262.333 ## 38 38 Stephanie Meadow 262.250 ## 39 39 Sophia Schubert 261.917 ## 40 40 Hyejin Choi 261.429 ## 41 41 Gemma Dryburgh 261.167 ## 42 42 Jennifer Song 261.125 ## 43 43 Lauren Coughlin 260.625 ## 44 44 Lilia Vu 260.571 ## 45 45 Brittany Lincicome 260.071 ## 46 46 Jessica Korda 259.875 ## 47 47 Isi Gabsa 259.833 ## 48 48 Katherine Perry-Hamski 259.800 ## 49 49 Katherine Kirk 259.625 ## 50 50 Ryann O&#39;Toole 259.591 ## 51 51 Annie Park 259.500 ## 52 52 Ally Ewing 259.300 ## 53 53 Ariya Jutanugarn 258.889 ## 54 54 Yu Liu 258.375 ## 55 55 Jeong Eun Lee 258.357 ## 56 56 Georgia Hall 258.273 ## 57 57 Lydia Ko 258.063 ## 58 58 Leona Maguire 257.500 ## 59 59 Ruixin Liu 257.200 ## 60 60 Angela Stanford 257.000 ## 61 61 Ana Belac 256.875 ## 62 62 Nasa Hataoka 256.727 ## 63 63 Esther Henseleit 256.500 ## 64 64 Perrine Delacour 256.429 ## 65 65 Sophia Popov 256.375 ## 66 66 Lauren Kim 256.000 ## 67 67 Linnea Johansson 255.929 ## 68 68t Casey Danielson 255.375 ## 69 68t Cristie Kerr 255.375 ## 70 70 Gaby Lopez 255.300 ## 71 71 Jeongeun Lee 255.250 ## 72 72 Lauren Stephenson 254.786 ## 73 73 Jodi Ewart Shadoff 254.714 ## 74 74 Mel Reid 254.250 ## 75 75 Jennifer Chang 254.000 ## 76 76 Pernilla Lindberg 253.800 ## 77 77 Paula Reto 253.700 ## 78 78 Cheyenne Knight 253.600 ## 79 79 Elizabeth Szokol 253.250 ## 80 80 Sarah Schmelzel 253.143 ## 81 81 Angel Yin 253.000 ## 82 82 Jenny Shin 252.929 ## 83 83 Amy Olson 252.786 ## 84 84 Jasmine Suwannapura 252.571 ## 85 85 Pajaree Anannarukarn 252.409 ## 86 86 Danielle Kang 252.313 ## 87 87 Na Rin An 252.071 ## 88 88 Austin Ernst 252.056 ## 89 89 Wichanee Meechai 252.000 ## 90 90 Brittany Altomare 251.900 ## 91 91 Jenny Coleman 251.875 ## 92 92 Albane Valenzuela 251.500 ## 93 93 Muni He 251.250 ## 94 94 Megan Khang 250.929 ## 95 95 Dewi Weber 250.500 ## 96 96 Hee Young Park 250.400 ## 97 97 Celine Boutier 250.364 ## 98 98 Pornanong Phatlum 250.214 ## 99 99 Andrea Lee 249.750 ## 100 100 In Gee Chun 249.286 ## 101 101 Caroline Masson 249.200 ## 102 102 Su-Hyun Oh 249.071 ## 103 103 Kelly Tan 248.929 ## 104 104 Mina Harigae 248.875 ## 105 105 Morgane Metraux 248.500 ## 106 106 Brittany Lang 248.417 ## 107 107 Anna Nordqvist 248.227 ## 108 108 Bronte Law 247.643 ## 109 109 Ayaka Furue 246.714 ## 110 110 Marina Alex 246.571 ## 111 111 Yae Eun Hong 246.500 ## 112 112 Lindsey Weaver 246.357 ## 113 113 Moriya Jutanugarn 245.389 ## 114 114 Ashleigh Buhai 245.125 ## 115 115 Matilda Castren 244.833 ## 116 116 Marissa Steen 244.200 ## 117 117 Heaji Kang 243.875 ## 118 118 Na Yeon Choi 242.750 ## 119 119 Emma Talley 242.000 ## 120 120 Karrie Webb 241.625 ## 121 121 Christina Kim 240.083 ## 122 122 Stacy Lewis 238.682 ## 123 123 Mirim Lee 238.563 ## 124 124 Allison Emrey 237.083 ## 125 125 Aditi Ashok 236.429 ## 126 126 Michelle Wie West 235.875 ## 127 127 Inbee Park 235.056 ## 128 128 Dana Finkelstein 234.786 ## 129 129 Beatriz Recari 228.250 ## ## [[2]] ## rank name putt_average ## 1 1 Leona Maguire 1.655 ## 2 2 Danielle Kang 1.661 ## 3 3 Brooke Henderson 1.701 ## 4 4 Su-Hyun Oh 1.707 ## 5 5 Carlota Ciganda 1.711 ## 6 6 Madelene Sagstrom 1.711 ## 7 7 Georgia Hall 1.715 ## 8 8 Lauren Stephenson 1.718 ## 9 9 Gemma Dryburgh 1.719 ## 10 10 Inbee Park 1.719 ## 11 11 Patty Tavatanakit 1.719 ## 12 12 Nanna Koerstz Madsen 1.721 ## 13 13 Jeong Eun Lee 1.723 ## 14 14 Lexi Thompson 1.724 ## 15 15t Yae Eun Hong 1.726 ## 16 15t Pauline Roussin-Bouchard 1.726 ## 17 17 Celine Boutier 1.729 ## 18 18 Caroline Masson 1.731 ## 19 19 Pernilla Lindberg 1.737 ## 20 20 Sarah Schmelzel 1.739 ## 21 21 Jessica Korda 1.745 ## 22 22 Amanda Doherty 1.746 ## 23 23 Morgane Metraux 1.746 ## 24 24 Nelly Korda 1.750 ## 25 25 Stacy Lewis 1.752 ## 26 26 Linnea Johansson 1.753 ## 27 27 Cheyenne Knight 1.754 ## 28 28 Perrine Delacour 1.756 ## 29 29 Isi Gabsa 1.756 ## 30 30 Yealimi Noh 1.758 ## 31 31 Lilia Vu 1.761 ## 32 32 Charley Hull 1.765 ## 33 33 Yuka Saso 1.767 ## 34 34 Bronte Law 1.767 ## 35 35t Ayaka Furue 1.769 ## 36 35t Hee Young Park 1.769 ## 37 37t Aditi Ashok 1.773 ## 38 37t Lindsey Weaver 1.773 ## 39 39 Lydia Ko 1.774 ## 40 40 Angela Stanford 1.774 ## 41 41 Rachel Rohanna 1.775 ## 42 42 Xiyu Lin 1.776 ## 43 43 Brittany Lincicome 1.776 ## 44 44t Brittany Altomare 1.778 ## 45 44t Heaji Kang 1.778 ## 46 46 Janie Jackson 1.780 ## 47 47 Laura Davies 1.781 ## 48 48t Megan Khang 1.783 ## 49 48t Amy Yang 1.783 ## 50 50 Ryann O&#39;Toole 1.786 ## 51 51 Marina Alex 1.789 ## 52 52t Allisen Corpuz 1.791 ## 53 52t Ally Ewing 1.791 ## 54 52t Gaby Lopez 1.791 ## 55 55t Hyejin Choi 1.792 ## 56 55t Andrea Lee 1.792 ## 57 57 Nasa Hataoka 1.792 ## 58 58 Maude-Aimee Leblanc 1.795 ## 59 59 Muni He 1.797 ## 60 60 Na Rin An 1.798 ## 61 61 Amy Olson 1.802 ## 62 62 Pajaree Anannarukarn 1.806 ## 63 63 A Lim Kim 1.807 ## 64 64 Sarah Kemp 1.808 ## 65 65 Fatima Fernandez Cano 1.810 ## 66 66 Christina Kim 1.813 ## 67 67 Sophia Schubert 1.816 ## 68 68t Cydney Clanton 1.816 ## 69 68t Jenny Shin 1.816 ## 70 70 Dana Finkelstein 1.817 ## 71 71t Agathe Laisne 1.818 ## 72 71t Gerina Mendoza 1.818 ## 73 71t Dewi Weber 1.818 ## 74 74 Atthaya Thitikul 1.819 ## 75 75 Jennifer Chang 1.821 ## 76 76 In Gee Chun 1.822 ## 77 77 Jennifer Kupcho 1.825 ## 78 78 Moriya Jutanugarn 1.826 ## 79 79t Maria Fassi 1.830 ## 80 79t Yu Liu 1.830 ## 81 81 Ruixin Liu 1.831 ## 82 82 Na Yeon Choi 1.833 ## 83 83 Katherine Perry-Hamski 1.844 ## 84 84 Stephanie Meadow 1.847 ## 85 85 Kelly Tan 1.849 ## 86 86 Sophia Popov 1.851 ## 87 87t Frida Kinhult 1.852 ## 88 87t Alison Lee 1.852 ## 89 89 Jodi Ewart Shadoff 1.853 ## 90 90 Annie Park 1.855 ## 91 91 Lauren Kim 1.857 ## 92 92 Lauren Coughlin 1.860 ## 93 93 Jasmine Suwannapura 1.860 ## 94 94 Emma Talley 1.860 ## 95 95 Ariya Jutanugarn 1.863 ## 96 96 Pornanong Phatlum 1.864 ## 97 97 Matilda Castren 1.864 ## 98 98t Giulia Molinaro 1.867 ## 99 98t Karrie Webb 1.867 ## 100 100t Elizabeth Szokol 1.870 ## 101 100t Alana Uriell 1.870 ## 102 102 Mirim Lee 1.871 ## 103 103 Esther Henseleit 1.872 ## 104 104 Luna Sobron 1.875 ## 105 105 Marissa Steen 1.881 ## 106 106 Peiyun Chien 1.882 ## 107 107 Angel Yin 1.886 ## 108 108 Cristie Kerr 1.886 ## 109 109 Michelle Wie West 1.889 ## 110 110 Anna Nordqvist 1.891 ## 111 111 Mel Reid 1.891 ## 112 112 Albane Valenzuela 1.892 ## 113 113 Paula Reto 1.896 ## 114 114 Ashleigh Buhai 1.900 ## 115 115 Casey Danielson 1.902 ## 116 116 Brittany Lang 1.903 ## 117 117t Mina Harigae 1.905 ## 118 117t Beatriz Recari 1.905 ## 119 119 Austin Ernst 1.909 ## 120 120 Jenny Coleman 1.913 ## 121 121 Ana Belac 1.915 ## 122 122 Bianca Pagdanganan 1.921 ## 123 123 Katherine Kirk 1.930 ## 124 124 Allison Emrey 1.932 ## 125 125 Jeongeun Lee 1.935 ## 126 126 Jaye Marie Green 1.940 ## 127 127 Jennifer Song 1.959 ## 128 128 Wichanee Meechai 1.966 ## 129 129 Emily Pedersen 2.000 ## ## [[3]] ## rank name greens_hit ## 1 1 Charley Hull 81.0 ## 2 2 Brittany Altomare 80.0 ## 3 3 Allisen Corpuz 79.6 ## 4 4 Celine Boutier 78.3 ## 5 5t Brooke Henderson 77.8 ## 6 5t Xiyu Lin 77.8 ## 7 5t Lexi Thompson 77.8 ## 8 8 Maude-Aimee Leblanc 76.9 ## 9 9 Hyejin Choi 76.2 ## 10 10 Isi Gabsa 75.9 ## 11 11 Danielle Kang 75.7 ## 12 12 Nanna Koerstz Madsen 75.6 ## 13 13 Jodi Ewart Shadoff 75.4 ## 14 14t Matilda Castren 75.0 ## 15 14t Frida Kinhult 75.0 ## 16 16t Caroline Masson 74.4 ## 17 16t Paula Reto 74.4 ## 18 16t Marissa Steen 74.4 ## 19 19 Jennifer Kupcho 74.1 ## 20 20 Yuka Saso 73.9 ## 21 21 Jasmine Suwannapura 73.8 ## 22 22 Lydia Ko 73.6 ## 23 23 Yealimi Noh 73.3 ## 24 24t Megan Khang 73.0 ## 25 24t Amy Yang 73.0 ## 26 26 Pajaree Anannarukarn 72.7 ## 27 27t Jennifer Chang 72.2 ## 28 27t Ayaka Furue 72.2 ## 29 27t Ariya Jutanugarn 72.2 ## 30 27t Sarah Kemp 72.2 ## 31 27t Ruixin Liu 72.2 ## 32 27t Emily Pedersen 72.2 ## 33 33t Ally Ewing 71.7 ## 34 33t Gaby Lopez 71.7 ## 35 35t Marina Alex 71.4 ## 36 35t In Gee Chun 71.4 ## 37 35t Perrine Delacour 71.4 ## 38 38 Jeongeun Lee 71.3 ## 39 39 Stacy Lewis 71.2 ## 40 40 Katherine Perry-Hamski 71.1 ## 41 41t Peiyun Chien 70.8 ## 42 41t Casey Danielson 70.8 ## 43 41t Jessica Korda 70.8 ## 44 44 Na Rin An 70.6 ## 45 45t Inbee Park 70.4 ## 46 45t Sophia Schubert 70.4 ## 47 47 Patty Tavatanakit 70.2 ## 48 48 Sophia Popov 70.1 ## 49 49t Fatima Fernandez Cano 70.0 ## 50 49t Bianca Pagdanganan 70.0 ## 51 51t Aditi Ashok 69.8 ## 52 51t Pornanong Phatlum 69.8 ## 53 51t Sarah Schmelzel 69.8 ## 54 51t Lilia Vu 69.8 ## 55 55t Ashleigh Buhai 69.4 ## 56 55t Lauren Coughlin 69.4 ## 57 57t Cydney Clanton 69.0 ## 58 57t Wichanee Meechai 69.0 ## 59 57t Jenny Shin 69.0 ## 60 60 Annie Park 68.9 ## 61 61 Nelly Korda 68.7 ## 62 62 Albane Valenzuela 68.5 ## 63 63t Bronte Law 68.3 ## 64 63t Amy Olson 68.3 ## 65 63t Kelly Tan 68.3 ## 66 66 A Lim Kim 68.2 ## 67 67 Jennifer Song 68.1 ## 68 68 Austin Ernst 67.9 ## 69 69t Brittany Lincicome 67.5 ## 70 69t Lauren Stephenson 67.5 ## 71 71 Moriya Jutanugarn 67.3 ## 72 72t Yae Eun Hong 66.7 ## 73 72t Andrea Lee 66.7 ## 74 72t Leona Maguire 66.7 ## 75 72t Stephanie Meadow 66.7 ## 76 72t Giulia Molinaro 66.7 ## 77 72t Pauline Roussin-Bouchard 66.7 ## 78 72t Luna Sobron 66.7 ## 79 79 Ryann O&#39;Toole 66.2 ## 80 80t Carlota Ciganda 65.9 ## 81 80t Jeong Eun Lee 65.9 ## 82 80t Atthaya Thitikul 65.9 ## 83 83 Morgane Metraux 65.7 ## 84 84t Georgia Hall 65.7 ## 85 84t Nasa Hataoka 65.7 ## 86 86 Janie Jackson 65.6 ## 87 87t Ana Belac 65.3 ## 88 87t Maria Fassi 65.3 ## 89 87t Esther Henseleit 65.3 ## 90 87t Yu Liu 65.3 ## 91 91t Dana Finkelstein 65.1 ## 92 91t Su-Hyun Oh 65.1 ## 93 93t Anna Nordqvist 64.6 ## 94 93t Madelene Sagstrom 64.6 ## 95 95 Linnea Johansson 64.3 ## 96 96t Jenny Coleman 63.9 ## 97 96t Elizabeth Szokol 63.9 ## 98 96t Alana Uriell 63.9 ## 99 99 Rachel Rohanna 63.5 ## 100 100t Cheyenne Knight 63.3 ## 101 100t Pernilla Lindberg 63.3 ## 102 102 Karrie Webb 62.5 ## 103 103 Jaye Marie Green 62.0 ## 104 104t Cristie Kerr 61.1 ## 105 104t Agathe Laisne 61.1 ## 106 104t Gerina Mendoza 61.1 ## 107 104t Mel Reid 61.1 ## 108 104t Dewi Weber 61.1 ## 109 109 Alison Lee 60.0 ## 110 110t Katherine Kirk 59.7 ## 111 110t Emma Talley 59.7 ## 112 112t Laura Davies 59.3 ## 113 112t Gemma Dryburgh 59.3 ## 114 112t Muni He 59.3 ## 115 112t Christina Kim 59.3 ## 116 116 Mirim Lee 59.0 ## 117 117t Amanda Doherty 58.3 ## 118 117t Mina Harigae 58.3 ## 119 117t Lauren Kim 58.3 ## 120 117t Beatriz Recari 58.3 ## 121 121 Hee Young Park 57.8 ## 122 122t Brittany Lang 57.4 ## 123 122t Angela Stanford 57.4 ## 124 124 Allison Emrey 54.6 ## 125 125 Lindsey Weaver 52.4 ## 126 126t Na Yeon Choi 50.0 ## 127 126t Heaji Kang 50.0 ## 128 126t Michelle Wie West 50.0 ## 129 129 Angel Yin 48.6 ## ## [[4]] ## rank name rounds score_average_actual ## 1 1 Danielle Kang 8 68.375 ## 2 2t Charley Hull 7 68.857 ## 3 2t Lexi Thompson 7 68.857 ## 4 4 Brooke Henderson 11 69.182 ## 5 5 Celine Boutier 11 69.273 ## 6 6 Leona Maguire 7 69.286 ## 7 7 Allisen Corpuz 3 69.667 ## 8 8 Hyejin Choi 7 69.714 ## 9 9 Lydia Ko 8 69.750 ## 10 10 Brittany Altomare 5 69.800 ## 11 11t Pauline Roussin-Bouchard 7 69.857 ## 12 11t Sarah Schmelzel 7 69.857 ## 13 13 Yuka Saso 10 69.900 ## 14 14t Aditi Ashok 7 70.000 ## 15 14t Jennifer Chang 3 70.000 ## 16 14t Isi Gabsa 3 70.000 ## 17 14t Nelly Korda 11 70.000 ## 18 18t Marina Alex 7 70.143 ## 19 18t Xiyu Lin 7 70.143 ## 20 20 Patty Tavatanakit 11 70.364 ## 21 21t Cheyenne Knight 5 70.400 ## 22 21t Nanna Koerstz Madsen 5 70.400 ## 23 23 Amy Yang 7 70.429 ## 24 24 Stacy Lewis 11 70.455 ## 25 25t In Gee Chun 7 70.571 ## 26 25t Ayaka Furue 7 70.571 ## 27 25t Megan Khang 7 70.571 ## 28 28 Georgia Hall 11 70.636 ## 29 29 Inbee Park 9 70.667 ## 30 30t Perrine Delacour 7 70.714 ## 31 30t Atthaya Thitikul 7 70.714 ## 32 32 Jessica Korda 8 70.750 ## 33 33 Caroline Masson 5 70.800 ## 34 34t Yae Eun Hong 7 70.857 ## 35 34t Bronte Law 7 70.857 ## 36 36t Gemma Dryburgh 3 71.000 ## 37 36t Jeong Eun Lee 7 71.000 ## 38 38 Gaby Lopez 10 71.100 ## 39 39t Na Rin An 7 71.143 ## 40 39t Jodi Ewart Shadoff 7 71.143 ## 41 39t Su-Hyun Oh 7 71.143 ## 42 39t Jasmine Suwannapura 7 71.143 ## 43 39t Kelly Tan 7 71.143 ## 44 44 Maude-Aimee Leblanc 6 71.167 ## 45 45t Pajaree Anannarukarn 11 71.273 ## 46 45t Madelene Sagstrom 11 71.273 ## 47 47 Pornanong Phatlum 7 71.286 ## 48 48t Jenny Shin 7 71.429 ## 49 48t Lilia Vu 7 71.429 ## 50 50 Ryann O&#39;Toole 11 71.455 ## 51 51 Nasa Hataoka 11 71.545 ## 52 52t Carlota Ciganda 7 71.571 ## 53 52t Dana Finkelstein 7 71.571 ## 54 54t Janie Jackson 5 71.600 ## 55 54t Ruixin Liu 5 71.600 ## 56 56 A Lim Kim 11 71.636 ## 57 57 Matilda Castren 6 71.667 ## 58 58t Brittany Lincicome 7 71.714 ## 59 58t Lauren Stephenson 7 71.714 ## 60 60t Yealimi Noh 5 71.800 ## 61 60t Bianca Pagdanganan 5 71.800 ## 62 60t Paula Reto 5 71.800 ## 63 63 Sophia Schubert 6 71.833 ## 64 64 Moriya Jutanugarn 9 71.889 ## 65 65 Ally Ewing 10 71.900 ## 66 66t Cydney Clanton 7 72.000 ## 67 66t Fatima Fernandez Cano 5 72.000 ## 68 66t Linnea Johansson 7 72.000 ## 69 66t Alison Lee 5 72.000 ## 70 66t Andrea Lee 2 72.000 ## 71 66t Wichanee Meechai 7 72.000 ## 72 66t Marissa Steen 5 72.000 ## 73 73 Morgane Metraux 6 72.333 ## 74 74 Katherine Perry-Hamski 5 72.400 ## 75 75 Amy Olson 7 72.429 ## 76 76t Sarah Kemp 2 72.500 ## 77 76t Frida Kinhult 2 72.500 ## 78 76t Jennifer Kupcho 6 72.500 ## 79 76t Agathe Laisne 2 72.500 ## 80 76t Alana Uriell 2 72.500 ## 81 81 Ariya Jutanugarn 9 72.556 ## 82 82t Rachel Rohanna 7 72.571 ## 83 82t Lindsey Weaver 7 72.571 ## 84 84 Albane Valenzuela 6 72.667 ## 85 85 Ana Belac 4 72.750 ## 86 86 Angela Stanford 9 72.778 ## 87 87t Pernilla Lindberg 5 72.800 ## 88 87t Giulia Molinaro 5 72.800 ## 89 87t Annie Park 5 72.800 ## 90 90t Jeongeun Lee 6 72.833 ## 91 90t Stephanie Meadow 6 72.833 ## 92 92 Anna Nordqvist 11 72.909 ## 93 93t Sophia Popov 8 73.000 ## 94 93t Elizabeth Szokol 2 73.000 ## 95 93t Emma Talley 4 73.000 ## 96 93t Karrie Webb 4 73.000 ## 97 97 Dewi Weber 6 73.167 ## 98 98 Casey Danielson 4 73.250 ## 99 99 Laura Davies 3 73.333 ## 100 100t Ashleigh Buhai 4 73.500 ## 101 100t Muni He 6 73.500 ## 102 100t Cristie Kerr 4 73.500 ## 103 100t Christina Kim 6 73.500 ## 104 100t Lauren Kim 2 73.500 ## 105 100t Emily Pedersen 2 73.500 ## 106 100t Mel Reid 10 73.500 ## 107 100t Luna Sobron 2 73.500 ## 108 108 Austin Ernst 9 73.556 ## 109 109t Lauren Coughlin 4 73.750 ## 110 109t Maria Fassi 4 73.750 ## 111 109t Mina Harigae 4 73.750 ## 112 112 Hee Young Park 10 73.800 ## 113 113 Amanda Doherty 6 73.833 ## 114 114t Peiyun Chien 4 74.000 ## 115 114t Katherine Kirk 4 74.000 ## 116 114t Yu Liu 4 74.000 ## 117 117 Jennifer Song 4 74.250 ## 118 118t Esther Henseleit 4 74.500 ## 119 118t Brittany Lang 6 74.500 ## 120 118t Gerina Mendoza 6 74.500 ## 121 121 Jenny Coleman 4 74.750 ## 122 122 Jaye Marie Green 6 74.833 ## 123 123 Mirim Lee 8 75.375 ## 124 124 Heaji Kang 4 75.500 ## 125 125 Allison Emrey 6 76.000 ## 126 126t Beatriz Recari 2 77.000 ## 127 126t Michelle Wie West 4 77.000 ## 128 128 Na Yeon Choi 4 78.500 ## 129 129 Angel Yin 4 78.750 All done!! And just like that, weve downloaded four different web pages, extracted the tabled info, and formatted them without copying and pasting any code. The same process for all four was only used one time to write the initial function. Just apply some final formatting to clean it up a bit and combine the separate data frames into a single, unified one. lpga_data= lpga_data %&gt;% reduce(left_join, by=&quot;name&quot;) %&gt;% # Combine all list levels into a single tibble, matching by the &quot;Name&quot; column select(-contains(&quot;rank.&quot;)) |&gt; rename(&quot;score_average&quot;=&quot;score_average_actual&quot;) # VOILA! head(lpga_data) ## name distance putt_average greens_hit rounds score_average ## 1 Bianca Pagdanganan 284.000 1.921 70.0 5 71.800 ## 2 Emily Pedersen 276.750 2.000 72.2 2 73.500 ## 3 Lexi Thompson 276.286 1.724 77.8 7 68.857 ## 4 Charley Hull 275.714 1.765 81.0 7 68.857 ## 5 Alana Uriell 274.000 1.870 63.9 2 72.500 ## 6 Frida Kinhult 273.750 1.852 75.0 2 72.500 10.3.2 Non-reproducible example (Juvenile Life Without Parole study) In the Juvenile Lifers study, there were a series of questions that participants rated on a scale of 0-100 in terms of difficulty. Part of our analysis involved taking the ratings on those variables and giving them relative rankings, so that each of the 6 variables in the series was rated from the least to most difficult, by participant. Now if we only needed to compute these rankings once this wouldnt have been any big deal; however, we needed to do it three times. Much of the same code and the same process would need to be copied and pasted, resulting in a very long, messy, harder to read script. With purrr however, we can reduce the redundancies to a minimum, saving time and reducing the chances of mistakes. Step 1. Just like before, the first step is to find a line-by-line solution for a single item, and then to generalize this into a shortcut function that can be applied to the any item i in a series of items. For the sake of brevity, Im going to skip most of that and just include the functions below. load(&quot;C:/Github Repos/Studies/JLWOP/Data and Models/jlwop_reentry_survey.RData&quot;) #### CREATE THE DATA SETS WE NEED#### na_blank=jlwop_reentry_survey # analysis 1 keeps the data as-is na_zero=jlwop_reentry_survey %&gt;% # supplementary analysis replaces the NA&#39;s with 0 mutate(across(c(barrier_housing:barrier_identification), replace_na,0)) rm(jlwop_reentry_survey) # remove old data set to avoid confusion #### Functions #### # transformation function to wrangle the data into proper formatting rotate_data=function(data, variable_prefix){ data=data %&gt;% pivot_longer( cols= starts_with(variable_prefix), # collect all the desired variables (i.e., columns).... names_to = &quot;variable&quot;, #...and put them into a new categorical variable called &quot;variable&quot; values_to = &quot;participant_score&quot;) %&gt;% # ...and store their values in a new variable called &quot;participant_score&quot; arrange(unique,participant_score) %&gt;% select(c(unique, participant_score, variable)) %&gt;% # keep only these 3 variables relocate(variable, .before = participant_score) # put the newly created variable up front return(data) } # creating the rankings for each variable; then transform data back to original structure rank_and_unpivot=function(data){ data=data %&gt;% group_by(unique) %&gt;% # group the scores so they can be ranked by participant mutate(rank1=dense_rank(participant_score), # create ranking variable rank=max(rank1,na.rm = TRUE) + 1 - rank1) %&gt;% # fix ranks by flipping to ascending order mutate(rank=factor(rank)) %&gt;% # convert rank to factor structure select(-rank1) # Pivot back to wide data=data %&gt;% pivot_wider(names_from = variable, values_from = rank:participant_score) %&gt;% ungroup() # un-group the data and delete the generated names return(data) } Step 2. Again, like before, we want to combine all elements of interest into some object. Once we have that, we then pass said object to map() and supply the map call with our custom function. dfs=list(na_blank=na_blank, na_zero=na_zero) %&gt;% # create lists map(.f=rotate_data, variable_prefix = &quot;barrier&quot;) %&gt;% # apply custom function along whole list map(rank_and_unpivot) # again!! DO IT AGAIN! With another function this time. # extract list elements to make them data frames again list2env(dfs, globalenv()) rm(dfs) #discard list. It has fulfilled its purpose. And just like that, were done! 10.3.3 Example 3: Read/Import several files at once with map() Note. This reads many .csv files and combines them into a SINGLE, unified data frame. If you want to import many files at once but keep them separated, youll need a different command. ######### Step-by-step version ######### file_path &lt;- &quot;JLWOP/Data and Models/&quot; # 1. view all files; make sure you see everything you want/need file_path %&gt;% list.files() # 2. save only file names with the desired extension csv_file_names=file_path %&gt;% list.files() %&gt;% .[str_detect(., &quot;.csv&quot;)] # 3. Load everything into the Global Environment csv_file_names %&gt;% purrr::map(function(file_name){ # iterate through each file name assign(x = str_remove(file_name, &quot;.csv&quot;), # Remove file extension &quot;.csv&quot; value = read_csv(paste0(file_path, file_name)), envir = .GlobalEnv) }) ######### Combining all steps into a single, one-line function ########## read_all_excel=function(path){ file_path &lt;- path # save only file names with the desired extension file_names=file_path %&gt;% list.files() %&gt;% .[str_detect(., &quot;.xlsx&quot;)] file_names %&gt;% purrr::map(function(file_name){ # iterate through each file name assign(x = str_remove(file_name, &quot;.xlsx&quot;), # Remove file extension &quot;.csv&quot; value = readxl::read_excel(paste0(file_path, file_name)), envir = .GlobalEnv) }) } read_all_excel(path = &quot;JLWOP/Data and Models/&quot;) 10.4 Other purrr commands Note that map() always returns a list, and depending on the output that you want, you may need to use a variation of map(). These variations are as follows: Command Return map_lgl() logical vector map_int() integer vector map_dbl() double vector map_chr() character vector walk() only returns the side effects of a function 10.4.1 The walk and walk2 commands Walk() is useful for when you just want to plot something or write a save file to your disk, etc. It does not give you any return to store something in the environment. You use it to write/read files, open graphics windows, and so on. Example: Exporting multiple .csv files at once Utilize purrr::walk2() to apply a function iteratively on TWO objects simultaneously. To save multiple .csv files with walk2, we need two distinct lists: 1. A list of data frames that we wish to export, 2. and the file paths, complete with the file names and extensions, for each file to be written First create and define both list items. Then apply walk2() to pluck an element from list 1 and its corresponding element from list 2, and apply the write_csv function in for-loop fashion. ### Custom function #### bundle_paths=function(df_list, folder_location){ names=names(df_list) paths=rep(here::here(folder_location), length(names)) extension=rep(c(&quot;.csv&quot;), length(names)) fixed_names=paste0(&quot;/&quot;,names) path_bundle=list(paths,fixed_names, extension) %&gt;% pmap(., paste0) return(path_bundle) } #### Exporting the .csv files for SPSS/JASP/etc. #### # Define list 1 dfs=list(na_blank=na_blank, na_zero=na_zero, na_zero_helpreint=na_zero_helpreint) # list 2 paths_csv=bundle_paths(df_list = dfs, folder_location = &quot;JLWOP/Data and Models&quot;) # Iterate over all elements in list 1 and corresponding element in list 2; # and apply the the write_csv function to each walk2(.x=dfs, .y= paths, .f=write_csv) #### .RData file for R users #### # Combine multiple data frames into a single .RData file and export save(list = c(&quot;na_blank&quot;, &quot;na_zero&quot;, &quot;na_zero_helpreint&quot;), file = here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;,&quot;ranking_data.RData&quot;)) 10.4.2 map2 (and walk2) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_a.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_b.png&quot;)) 10.4.3 pmap for when you have a bunch of shit This function is for iterating over three or more elements. As soon as you have &gt;2 items you have to iterate over, you need pmap().It works similar to to map and map2, but instead of iterating over a single object x or two objects x and y, it acts on a list object called .l The list is a list of all the objects you want to iterate over. If you give it a list of 18 items, it iterates over all 18. If the list only has two things, it only acts on those two. She says its easiest to imagine the list as a data frame, and the columns of the data frame like the elements of that list. knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap_2.png&quot;)) "],["intro-to-r-markdown.html", "Chapter 11 Intro to R Markdown 11.1 Important code chunk options 11.2 Writing math equations and symbols 11.3 Including graphics/inserting pictures 11.4 Footnotes 11.5 Change the color of your text 11.6 Re-using code chunk options 11.7 Making better tables 11.8 Running in-line code", " Chapter 11 Intro to R Markdown R Markdown is a better and more organized way to write scripts. Seriously, once you learn it, theres no going back. New and dont know where to start? Read The R Markdown Cookbook. Amazing overview with tons of neat tricks and how-tos. This other source may also be of some help. Below are some quick tips for common tasks; but be sure to read the Cookbook above. 11.1 Important code chunk options cache: TRUE or FALSE. Do you want to save the output of the chunk so it doesnt have to run next time? Creates a cached folder in the directory. eval: Do you want to evaluate (i.e., run) the code in the chunk? echo: Do you want to print the code after its run? include: Do you want to include code output in the final output document? Setting to FALSE means the code does not appear in the output document, but it is still run. 11.2 Writing math equations and symbols 11.2.1 Greek symbols A few notes first: Math notation is done with dollar signs and forward slashes For Greek letters, just type the name of the letter: $\\mu$ for \\(\\mu\\) $\\sigma$ for \\(\\sigma\\) $\\alpha$ for \\(\\alpha\\) $\\pi$ for \\(\\pi\\) $\\rho$ for \\(\\rho\\) 11.2.2 Math notation $\\pm$ for Â± $\\ge$ for  $\\le$ for  $\\neq$ for  11.2.3 Statistics notation 11.2.4 Writing in-line code Use the funny looking symbol on the tilde key that looks like this: ` To write in line, code, put one of those symbols on either side of the code, like you would with quotation marks. Helps you write lines like: I love dplyr 11.3 Including graphics/inserting pictures The default method doesnt work for me for some reason, but you can still insert images using a combination of the here package and knitr. Use the include_graphics() command and specify both the file location and its name: knitr::include_graphics(here::here(&quot;pics&quot;,&quot;snapchat.png&quot;)) NOTE. Use 3000-600 DPI to get good looking pictures. The bookdown book notes that: The syntax for controlling the image attributes is the same as when images are generated from R code. Chunk options fig.cap, out.width, and fig.show still have the same meanings. and: You can easily scale these images proportionally using the same ratio. This can be done via the dpi argument (dots per inch), which takes the value from the chunk option dpi by default If it is a numeric value and the chunk option out.width is not set, the output width of an image will be its actual width (in pixels) divided by dpi , and the unit will be inches. For example, for an image with the size 672 x 480, its output width will be 7 inches ( 7in ) when dpi=96. This feature requires the package png and/or jpeg to be installed. You can always override the automatic calculation of width in inches by providing a non-NULL value to the chunk option out.width , or use include_graphics(dpi = NA) 11.4 Footnotes To add a footnote, use the ^ symbol and put the note in brackets: You can also write footnotes1 like this. 11.5 Change the color of your text YOUR TEXT HERE 11.6 Re-using code chunk options https://yihui.org/en/2021/05/knitr-reuse/ 11.7 Making better tables https://rfortherestofus.com/2019/11/how-to-make-beautiful-tables-in-r/ 11.8 Running in-line code To run code in the middle of a sentence, you create a mini code chunk inside the sentence. For example: &gt; There are 2x2 apples in the basket Could be typed as There are 4 apples in the basket Kruschke, J. (2015). Goals, power, and sample size. In J. K. Kruschke (Ed.), Doing bayesian data analysis: A tutorial with r, jags, and stan (2nd ed., pp.Â 359-398). Academic Press. "],["statistics-and-psych-specific-stuff.html", "Chapter 12 Statistics and Psych-specific Stuff 12.1 Create or sample from a distribution 12.2 Find Cohens Kappa (Interrater reliability) 12.3 Statistical tests and modeling with easystats", " Chapter 12 Statistics and Psych-specific Stuff 12.1 Create or sample from a distribution Creating a binomial distribution When you do this, you are setting the true population parameter; you are in control of the Data Generating Process and the true distribution In a binomial distribution, the parameter is normally distributed, and can take any value from 0.0 to 1.0 But the data that this process generates is not normal rbinom(n= 1000, size= 1, prob = 0.5) ## [1] 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 ## [81] 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 ## [161] 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 ## [241] 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 ## [321] 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 ## [401] 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 ## [481] 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 ## [561] 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 ## [641] 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 ## [721] 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 ## [801] 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 ## [881] 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 ## [961] 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 rnorm(n=2500,mean=500, sd=100) ## [1] 690.7642 503.0016 446.5626 484.7566 514.2144 558.1073 372.0421 486.2422 445.8801 507.1753 582.8456 556.2646 505.3560 457.7520 552.2014 443.2911 516.2216 ## [18] 507.5774 496.2267 700.0870 486.4038 544.2314 610.0555 490.9881 564.8317 588.5202 438.1985 365.3048 467.5639 521.1525 617.3123 554.5895 489.1160 512.8746 ## [35] 711.0196 692.8460 519.8707 324.4048 441.7998 534.3156 490.2324 585.4221 430.4960 460.9629 703.4337 510.0744 519.9681 468.5821 527.3318 413.0651 460.6879 ## [52] 664.7801 354.3382 370.0553 489.8274 326.0638 343.5181 669.8929 410.4743 259.5956 581.9515 479.6523 434.0253 461.1315 584.3208 423.5085 556.7683 466.5056 ## [69] 433.8107 588.9122 358.9104 296.3319 490.2643 374.4722 508.8257 550.2508 413.3520 491.6750 450.5777 637.9321 418.8566 530.1711 558.6032 531.9450 422.4871 ## [86] 509.5001 436.4969 609.5397 390.5833 573.7023 332.9137 355.7153 459.6057 416.6499 435.9233 393.1971 450.7034 370.8988 325.2500 560.9520 538.7495 596.3238 ## [103] 456.8505 378.8018 389.1940 546.0309 464.9669 437.3562 595.2106 358.9524 488.1387 577.6982 451.5795 555.9834 455.0614 424.9773 512.1967 333.4369 349.7942 ## [120] 497.6879 461.4079 579.9255 570.4377 496.8774 468.6098 553.1056 693.8511 668.4453 418.9909 571.5872 626.5654 682.8749 381.1959 380.5799 529.1706 495.0200 ## [137] 602.4397 488.4812 481.2003 480.3037 517.4619 430.3098 410.7283 383.9239 507.2224 677.0574 551.9872 743.8753 447.8830 553.9661 660.0133 426.8833 378.0482 ## [154] 586.2658 608.6438 414.7819 467.3578 443.9643 505.8476 582.4859 515.8577 579.6892 452.8202 320.2918 429.6083 496.7879 444.7553 546.7475 461.6923 495.1302 ## [171] 305.6665 481.9163 570.0053 411.7289 659.6989 319.8401 577.8611 325.9026 545.4157 460.2025 534.0228 406.5574 532.8106 611.0717 470.5804 470.7250 399.0586 ## [188] 376.7103 432.2565 510.2988 501.0330 311.5054 613.5306 491.6324 592.4899 497.2093 253.9098 540.8763 524.1655 570.1845 568.8148 649.2078 357.9979 505.1117 ## [205] 560.0656 488.2058 512.4326 429.8364 369.7317 562.7482 380.4301 691.9237 552.0891 642.8095 381.2714 610.1536 393.5623 353.8832 557.3873 478.6679 677.2078 ## [222] 493.7744 381.9982 534.3512 519.5267 354.7870 427.7809 492.3991 527.7539 236.7285 634.5613 611.5118 573.7486 494.7876 447.2701 450.1239 447.2279 528.8756 ## [239] 495.6735 432.1601 480.7788 407.7178 519.6089 533.9601 478.7551 535.1155 502.5280 342.3943 446.9967 523.1670 562.8475 462.3585 525.2291 523.7450 532.5428 ## [256] 435.9035 458.1338 661.8657 463.5148 520.1895 497.6584 427.7127 590.0779 563.6812 527.4667 580.2521 450.2358 609.7054 687.3053 315.5045 381.4677 354.9862 ## [273] 621.2616 445.6282 493.3062 557.0388 535.6489 391.4441 517.1641 643.9891 462.9767 456.7823 594.6212 679.8447 469.5304 585.8058 495.7101 515.8654 585.2954 ## [290] 475.4714 245.2662 573.7207 571.2908 510.1705 489.8945 427.6127 381.9606 549.0644 731.1598 624.7910 351.4736 516.0527 502.4989 575.3473 508.8300 598.5068 ## [307] 626.5723 394.5084 551.1109 481.5339 602.6052 490.7534 524.8933 594.8019 511.1983 451.8964 493.0614 628.4617 477.1235 576.7203 469.4596 528.6711 536.4981 ## [324] 466.4876 529.7832 514.9179 594.0735 475.0174 379.3909 551.3659 416.0800 653.6031 349.7248 498.9691 603.6682 541.4988 497.6942 419.0618 461.9743 433.0824 ## [341] 291.6662 454.9965 429.0149 266.4133 580.8652 417.8723 625.2045 507.4710 391.1100 654.7852 627.8170 398.9321 384.7853 501.1054 675.6450 476.3180 440.5276 ## [358] 509.3056 599.2357 592.1269 651.7668 524.3816 421.1368 452.8244 529.8621 644.1803 478.4175 508.7750 595.8007 558.5413 571.2349 555.1023 559.2593 489.7140 ## [375] 493.0326 669.2377 469.0042 347.3663 475.9720 421.2802 645.9101 453.0382 541.1887 411.5055 666.8346 494.7872 397.1767 497.7115 486.7867 534.5902 439.6220 ## [392] 480.8484 554.9179 368.8868 460.8274 489.0922 711.1718 501.9165 418.5724 718.6710 423.1709 435.8882 532.8551 649.3885 451.9812 587.7714 516.2318 511.8405 ## [409] 380.5151 573.8564 462.3429 510.8129 275.7200 535.5461 623.0225 456.8365 490.8698 594.5922 653.8893 516.6514 332.8932 612.6280 524.1005 538.2043 487.1793 ## [426] 524.3440 497.0323 502.0975 587.9191 448.9578 476.7128 602.9146 469.7850 376.6824 499.8641 650.7851 615.8094 493.3007 570.9333 608.2727 559.6828 587.6076 ## [443] 477.4335 535.3579 459.4781 448.0090 531.2777 522.1706 543.7536 606.8745 463.3416 491.5422 684.6416 437.6934 476.8128 499.2137 527.9584 599.6644 396.7951 ## [460] 477.0358 532.5132 478.9701 610.1375 570.7250 458.6793 490.3939 492.9727 548.0784 491.5192 534.6258 592.4548 429.9015 622.2405 514.6087 482.5179 631.1590 ## [477] 548.9731 476.7655 550.6699 458.7896 500.6046 489.9678 521.7070 376.1594 455.4748 478.8103 500.6990 450.0641 705.1408 493.3444 340.2326 450.3086 565.5474 ## [494] 510.5845 607.4647 525.2464 601.9312 393.1189 581.0030 537.7657 453.3838 318.0283 508.4975 531.2463 373.1126 548.7601 623.4519 440.0869 652.5000 437.5633 ## [511] 506.5816 415.4071 516.0706 544.6454 487.4728 544.8934 566.9092 617.6771 548.2616 631.0511 473.4739 571.3268 368.3856 501.6164 522.8763 629.5952 457.4299 ## [528] 527.2467 326.8235 513.6585 420.4447 447.2285 441.3478 517.2391 292.0001 494.4801 527.8838 542.8061 566.1641 578.9908 357.2008 473.6306 677.2587 535.5803 ## [545] 301.3727 403.7398 414.6190 625.7101 433.4926 513.9790 290.1241 249.8849 590.1190 487.6803 571.0442 601.2068 469.5465 475.7392 432.6426 496.2470 418.4045 ## [562] 303.4358 726.0839 609.2849 500.5518 578.1233 408.7615 380.4873 594.1007 499.7439 531.1113 368.6508 455.1496 609.9466 458.9388 604.4078 402.9088 415.8239 ## [579] 448.0483 462.1295 522.6474 487.6938 456.0863 577.8719 340.5999 633.7612 443.4922 454.3063 610.4940 435.0683 599.6636 397.2973 374.6455 325.8588 321.3024 ## [596] 482.7620 374.4408 425.1393 510.4970 531.1596 645.9354 448.5209 531.6261 348.1992 476.0654 562.8976 379.9974 532.0484 445.4427 634.4011 368.7167 672.7004 ## [613] 565.7535 491.0597 731.4451 604.8716 421.3364 300.1127 440.5535 527.3207 420.5020 543.1092 499.6028 422.6449 742.7753 436.8232 530.8857 546.1302 647.6666 ## [630] 448.8163 366.9691 396.1402 562.1381 741.5654 426.0017 551.1103 478.2167 600.2266 532.1149 436.3339 660.6611 354.6999 488.2334 632.6080 390.4946 389.2055 ## [647] 641.5392 502.8989 586.2955 447.9865 349.9972 500.9418 283.4726 583.2403 523.0787 475.2691 539.1305 525.1959 503.8490 297.1299 485.4389 480.3741 525.1787 ## [664] 659.0466 519.6565 513.0671 360.0704 637.1515 586.5342 580.8653 435.4805 624.2765 464.9148 409.2928 565.2380 556.4711 392.7148 588.4800 616.1251 523.3135 ## [681] 495.8529 359.6408 597.8614 557.3185 547.4084 554.3561 469.9123 471.6370 552.7903 457.7323 509.1782 484.3411 310.7837 349.0889 469.9034 736.6610 482.3783 ## [698] 423.4637 607.1870 525.9129 456.5308 358.0275 605.7850 646.5580 477.9809 377.2324 403.8891 601.9674 613.8450 408.5394 510.2801 372.3809 693.2728 609.3073 ## [715] 704.5412 441.4024 623.5082 511.3888 539.7074 465.6184 663.6630 699.4078 444.3459 485.8474 463.9139 436.9269 583.1504 435.3909 584.0109 687.2442 626.6060 ## [732] 504.6432 497.2372 306.4394 430.5330 566.2029 441.3459 484.8510 524.0242 440.1531 554.9754 572.8674 558.5630 621.3363 635.4167 549.3009 548.0824 469.9470 ## [749] 728.0815 691.3732 650.7351 311.2510 528.0012 513.0207 443.2585 495.4862 494.3729 577.6868 511.6519 387.7507 500.9570 528.0799 392.6372 575.0573 649.4153 ## [766] 424.9899 445.2550 589.0665 456.6779 417.0215 494.1086 466.9487 507.0484 617.9516 408.3503 486.6273 519.3648 367.0132 603.6742 357.5127 459.2164 414.9186 ## [783] 477.9580 375.7157 227.5575 447.0915 552.5945 541.2205 477.3557 511.3888 366.3676 541.8846 641.1269 493.1544 444.6010 571.4874 507.8334 376.4408 729.4874 ## [800] 607.8687 453.6202 377.1565 284.5644 514.1725 582.8606 423.0604 485.5145 430.1117 616.0109 367.8656 393.5513 450.6351 475.9432 513.4465 620.5404 560.5519 ## [817] 450.3671 520.9705 493.8261 507.4381 665.0495 325.3849 508.1991 343.4316 380.3415 424.6257 606.1888 579.1084 462.3792 389.8099 490.8874 591.3056 465.1908 ## [834] 624.4178 423.0522 253.6473 462.7211 452.0061 365.4780 334.9249 430.0378 310.6141 587.8560 435.5059 352.7550 366.2667 496.9301 464.1797 426.3634 524.6137 ## [851] 575.3314 462.3060 354.1794 577.5041 288.6520 467.3059 478.7292 635.0938 467.7270 610.1464 442.0070 583.9132 363.9433 530.5671 549.5344 468.3479 466.9145 ## [868] 655.7451 512.8609 504.2036 538.2761 475.7039 458.7146 418.8669 606.1192 448.4299 387.2663 474.5578 576.6278 502.9695 509.4259 505.9550 557.8631 606.7899 ## [885] 546.4254 517.9671 456.8737 534.7139 532.2812 506.9922 437.5018 354.5535 547.5040 546.1719 496.7769 594.6566 566.8912 651.1099 569.2072 399.9619 499.5997 ## [902] 453.0542 503.6737 479.9079 603.6589 588.4268 480.5298 719.0702 476.4992 463.0849 515.4054 557.1812 480.2085 517.2196 429.5344 664.7642 565.2157 489.8956 ## [919] 502.2124 514.0178 534.4487 468.6748 477.4474 428.4724 342.0031 591.6231 361.7524 579.8610 503.2338 531.1024 378.8424 436.3192 708.5300 524.2979 451.3790 ## [936] 368.0187 343.0128 649.4367 593.2486 716.6372 566.0013 563.3512 323.0505 712.7242 283.4797 593.6304 368.4733 586.7725 402.3331 557.0522 511.9797 398.3025 ## [953] 623.5914 529.3349 515.1055 495.3027 594.1564 578.8444 386.4326 402.7863 526.0448 507.9440 334.2873 398.9222 523.8656 365.6481 488.9171 459.0526 286.3321 ## [970] 337.5755 613.3931 478.8902 514.2966 745.8776 475.3705 517.7410 544.9654 630.6918 565.4086 335.1642 531.3661 436.6194 427.9133 425.6029 557.2813 501.2163 ## [987] 547.3963 370.5879 490.5978 447.3090 512.0111 506.0593 444.2074 446.8414 401.0092 463.4028 441.7573 460.8381 494.1673 437.6966 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1500 entries ] 12.2 Find Cohens Kappa (Interrater reliability) Useful for IRR agreement on categorical variables Going to use the psych package for this: https://www.rdocumentation.org/packages/psych/versions/2.1.6/topics/cohen.kappa See here for an overview of what Cohens Kappa is if you need a recap/intro. 12.3 Statistical tests and modeling with easystats https://easystats.github.io/easystats/ 12.3.1 Getting parameter estimates from model objects Scenario: Youve run some statistical test (like the below regression), and want a summary of the model estimates. rm(iris) model &lt;- lm(Sepal.Length ~ Species, data = iris) You have a few options when it comes to getting a summary of a model and getting the coefficient estimates: - summary() - broom::tidy() - paramters::model_paramters(), or just paramters::paramters() for short Theres no reason to use summary, generally speaking, because it sucks. It doesnt give you tidy output thats easy to manipulate or extract, its hard to read, and it cant be turned into a useful table. Skip it unless you need something specific from its output (i.e., youre using lavaan) Options two and three are pretty similar and both give you most of the same information, though parameters() prints neater to the console window. Generally I find parameters preferable. Note though that neither command will round the numbers if you store it as a table in the environment. So. If you want to manipulate ANY info in the table and/or extract info, just use tidy or parameters. If youre using the command to export said info in a neat table, or you want to view it in a more readable fashion and do not care about extracting/modifying/manipulating anything in it, then use parameters and pipe it to format_table() Using format_table() rounds all columns to 2 decimal places, reformats p-values to APA format, and collapses CIs into a single column. Do note though that it makes every column into a Character column! So this is for exporting-use only. Heres a comparison of brooms output (first) vs.Â parameters (second) when you save each in the environment. As you can see, both produce tidy tibbles And heres what parameters(model) |&gt; format_table() does to the a parameters table: Much cleaner for making a table to export to Word. 12.3.2 Getting model information and performance metrics Again, two options here. You can use either glance from the broom package, or performance from the package of the same name. These each produce slightly different output, though unlike above, I dont think one is necessarily better than the other. Use whichever one you prefer. broom::glance(model) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.619 0.614 0.515 119. 1.67e-31 2 -112. 231. 243. 39.0 147 150 performance::performance(model) ## # Indices of model performance ## ## AIC | BIC | R2 | R2 (adj.) | RMSE | Sigma ## ----------------------------------------------------- ## 231.452 | 243.494 | 0.619 | 0.614 | 0.510 | 0.515 12.3.3 Effect size info with effectsize logreg_model=glm(smoke ~ age + sex, data= legaldmlab::survey, family = &quot;binomial&quot;) logreg_model_coeff=parameters::parameters(logreg_model) logreg_model_coeff=logreg_model_coeff |&gt; dplyr::mutate(odds_ratio=exp(Coefficient)) effectsize::interpret_oddsratio(logreg_model_coeff$odds_ratio, rules = &quot;chen2010&quot;) ## [1] &quot;small&quot; &quot;very small&quot; &quot;very small&quot; ## (Rules: chen2010) 12.3.4 Quick, detailed, and automated reporting with report Check out https://easystats.github.io/report/ 12.3.5 Running correlations with correlation https://easystats.github.io/correlation/ "],["coding-tips-and-tricks.html", "Chapter 13 Coding Tips and Tricks 13.1 Grammar stuff in base R 13.2 Tidyverse stuff 13.3 Function-related stuff 13.4 Creating a package 13.5 Creating a bookdown", " Chapter 13 Coding Tips and Tricks 13.1 Grammar stuff in base R 13.1.1 Regex expressions and symbols str_remove(html$`Market Price`, pattern = &quot;$&quot;) # doesn&#39;t remove the $ sign str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) # works 13.1.2 The new pipe (Base R) Good reading material/stuff to know: https://www.r-bloggers.com/2021/05/the-new-r-pipe/?__twitter_impression=true&amp; Not really any functional differences from the tidyverse pipe, but using this instead puts one less dependency in your code. So its probably worth changing. You can set this in the global options in RStudio. 13.2 Tidyverse stuff 13.2.1 Sometimes when making a function you need to use the colon-equals operator, rather than just the normal &lt;- or = assignment operators Specifically, when you have multiple named arguments in your function Read my question and someones answer on this blogpost: https://community.rstudio.com/t/help-creating-simple-function/109011/2 13.3 Function-related stuff 13.3.1 User-supplied expressions or named columns in functions 13.3.2 When a command requires a named column or data set, but youve already supplied it and its required a second time If youre writing a function with a pipe but the command youre using needs the data set defined in it, you specify it as .x Here is an example: 13.3.3 Formulas within functions Generally when you see .fn inside a function (e.g., map(x, .fn)), that means function. You put whatever formula or function you want in there. You may also see the tilde used instead, which does the same thing. 13.4 Creating a package https://rstudio4edu.github.io/rstudio4edu-book/data-pkg.html 13.4.1 Documenting pacakge meta-data https://r-pkgs.org/description.html 13.4.2 Connecting to other packages https://kbroman.org/pkg_primer/pages/depends.html 13.4.3 Linking Git and Github view this detailed guide by Jenny Bryan, and this YouTube video. Quick summary of steps in YouTube video: Open project folder in Windows Explorer and click in the URL bar, then type cmd to open command prompt If there are any pre-existing git files or repository info there, remove it with the following: rd .git /S/Q Tell git to create a new repo by typing: git init Then tell it to include all files in the current place by typing: git add . Commit these files with: git commit -m \"Initial commit\" At this point youve created a git and GitHub repo each; now link them with: git remote add origin [https URL of GitHub repo] Push all these changes live with: git push -u origin master 13.5 Creating a bookdown https://www.youtube.com/watch?app=desktop&amp;v=m5D-yoH416Y&amp;feature=youtu.be 13.5.1 Rendering the book once its done Render locally with bookdown::render_book(index.Rmd) Use browseURL(\"docs/index.html\") to view your book locally (or just open index.html in a browser). If it looks good, commit and push all changed files to GitHub. "],["creating-a-simulated-data-set.html", "Chapter 14 Creating a simulated data set 14.1 Part 1: Independent samples from a normal distribution 14.2 Part 2: Creating data sets with quantitative and categorical variables 14.3 Part 3: Repeatedly simulate samples with replicate() 14.4 Part 4: repeatedly making whole data sets 14.5 Part 5: Using purrr", " Chapter 14 Creating a simulated data set From the tutorial on this page 14.1 Part 1: Independent samples from a normal distribution Consider the following first before you start doing stuff: - How many subjects are in each condition? - What are the means and standard deviations of each group? Set that shit below. # number of subjects per group A_sub_n &lt;- 50 B_sub_n &lt;- 50 # distribution parameters A_mean &lt;- 10 A_sd &lt;- 2.5 B_mean &lt;- 11 B_sd &lt;- 2.5 Now generate scores for each group A_scores &lt;- rnorm(A_sub_n, A_mean, A_sd) B_scores &lt;- rnorm(B_sub_n, B_mean, B_sd) Technically you could stop here and just analyze the data in this fashionbut its better to organize it into a table. One that looks like something you would import after real data collection. So do that next; make it look nice. dat &lt;- tibble( sub_condition = rep( c(&quot;A&quot;, &quot;B&quot;), c(A_sub_n, B_sub_n) ), score = c(A_scores, B_scores) ) head(dat) ## # A tibble: 6 x 2 ## sub_condition score ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 11.4 ## 2 A 12.0 ## 3 A 12.5 ## 4 A 8.84 ## 5 A 6.84 ## 6 A 11.0 Always perform a quality and consistency check on your data to verify that shits ok. dat %&gt;% group_by(sub_condition) %&gt;% summarise(n = n() , mean = mean(score), sd = sd(score)) ## # A tibble: 2 x 4 ## sub_condition n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 50 9.98 2.78 ## 2 B 50 10.9 3.02 14.2 Part 2: Creating data sets with quantitative and categorical variables From the web page at this link 14.2.1 2.a. DATA WITH NO DIFFERENCE AMONG GROUPS Critically important notes to know: When you use the rep() function, there are several different arguments you can specify inside it that control how stuff is repeated: using rep(x, each= ) repeats things element-wise; each element gets replicated n times, in order rep(c(&quot;A&quot;,&quot;B&quot;), each=3) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; using rep(x, times= ) repeats the sequence; the vector as a whole, as it appears, will be repeated with one sequence following the next rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), times=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; using rep(x, length.out) repeats only the number of elements you specify, in their original order rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), length.out=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; In this particular data, we want every combination of group and letter to be present ONCE. letters=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;) tibble(group = rep(letters[1:2], each = 3), factor = rep(LETTERS[3:5], times = 2), response = rnorm(n = 6, mean = 0, sd = 1) ) ## # A tibble: 6 x 3 ## group factor response ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A C -0.559 ## 2 A D 0.0378 ## 3 A E -1.14 ## 4 B C -1.10 ## 5 B D 0.509 ## 6 B E -0.304 14.2.2 2.b. Data WITH A DIFFERENCE among groups What if we want data where the means are different between groups? Lets make two groups of three observations where the mean of one group is 5 and the other is 10. The two groups have a shared variance (and so standard deviation) of 1. 14.2.2.1 Some notes first Creating a difference between the two groups average score means we have to tell R to sample itteratively from distributions with different means. We do this by specifying a vector of means within rnorm, like so: response = rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 4.066141 9.831952 4.307055 11.059641 4.808349 10.009015 You can see that: 1. draw 1 is from the distribution \\((\\mu=5,\\sigma=1)\\) 2. draw 2 is from the distribution \\((\\mu=5,\\sigma=1)\\) And this process repeats a total of six times. And if you happen to also specify a vector of standard deviations (purely to demonstrate what is happening, we wont actually do this), the first mean is paired with the first SD; the second mean is paired with the second SD; and so on. rnorm(n = 6, mean = c(5, 10), sd = c(2,0.1)) ## [1] 5.744356 10.043787 7.378609 9.952296 8.064520 10.119440 14.2.2.2 Ok, back to creating the data If you want there to be differences between the groups, we need to change the way the vector of factors is replicated, in addition to specifying the vector of means. We want to ensure that the sequence of A, B in the group column matches the sequence repeated in the response column. Here we are going to use length.out so that the whole sequence of A,B is repeated exactly in line with the alternating drawing from \\(\\mu=5\\), \\(\\mu=10\\). Its often best to do this by building each thing separately, and then combining it into a tibble when you have it figured out. group=rep(letters[1:2], length.out = 6) group ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; response=rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 4.948867 10.118024 3.109300 8.589811 4.047384 10.121841 tibble(group, response) ## # A tibble: 6 x 2 ## group response ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 4.95 ## 2 B 10.1 ## 3 A 3.11 ## 4 B 8.59 ## 5 A 4.05 ## 6 B 10.1 14.2.3 2.c. Data with MULTIPLE QUANTITATIVE VARIABLES with groups 14.3 Part 3: Repeatedly simulate samples with replicate() Instead of drawing values one at a time from a distribution, we want to do it many times. This is a job for replicate(). What replicate() does is run a function repeatedly. The replicate() function will perform a given operation as many times as you tell it to. Here we tell it to generate numbers from the distribution \\(N~(\\mu=0, \\sigma=1)\\), three times (as specified in the n=3 argument in line one) replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = FALSE ) ## [[1]] ## [1] 1.3203910 0.2040899 -0.7970636 -1.2431823 -0.4844866 ## ## [[2]] ## [1] -1.2322047 0.1063858 0.7922842 0.4413982 -0.8931153 ## ## [[3]] ## [1] 1.7023983 0.1654943 0.7787761 -0.1186586 -0.2111680 The argument simplify=FALSE tells it to return the output as a list. If you set this to TRUE it returns a matrix instead replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) ## [,1] [,2] [,3] ## [1,] 0.72115635 0.7402300 0.1134487 ## [2,] 0.92810115 -1.1035296 -1.0247406 ## [3,] 0.53107386 0.0286275 -0.3009290 ## [4,] -0.01083094 0.5085364 1.7663764 ## [5,] -0.51711197 0.3824463 -1.0787283 Specifying as.data.frame() with the matrix output can turn it into a data frame. replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) %&gt;% as.data.frame() %&gt;% rename(sample_a=V1, sample_b=V2, sample_c=V3) ## sample_a sample_b sample_c ## 1 0.56415036 -1.7060196 1.1146509 ## 2 -0.24011787 1.8521680 1.4152690 ## 3 -0.09948105 1.9130434 0.1568660 ## 4 -1.76657069 -0.2574811 -1.0662388 ## 5 0.67864934 0.5891597 -0.1166993 14.4 Part 4: repeatedly making whole data sets This is combining parts 2 and 3 to repeatedly create and sample data sets, resulting in a list of many data sets. simlist = replicate(n = 3, expr = data.frame(group = rep(letters[1:2], each = 3), response = rnorm(n = 6, mean = 0, sd = 1) ), simplify = FALSE) simlist ## [[1]] ## group response ## 1 A -1.1482417 ## 2 A -0.5806775 ## 3 A 0.3688638 ## 4 B 0.3304444 ## 5 B 0.3483946 ## 6 B -1.1257075 ## ## [[2]] ## group response ## 1 A 0.8829318 ## 2 A -1.3660498 ## 3 A -0.7142761 ## 4 B -0.2700098 ## 5 B -0.2419871 ## 6 B -0.4623374 ## ## [[3]] ## group response ## 1 A -0.6018124 ## 2 A 1.0289143 ## 3 A 0.1779939 ## 4 B 1.7056511 ## 5 B -0.3255277 ## 6 B -0.7897194 14.5 Part 5: Using purrr See this blog post "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
