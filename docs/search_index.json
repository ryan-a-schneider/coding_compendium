[["index.html", "Creating a simulated data set Chapter 1 A Monument to my Madness 1.1 What this book is, and what it is not", " Creating a simulated data set Ryan Schneider 2022-06-08 Chapter 1 A Monument to my Madness This book contains all my personal coding notes from the last two years. Why am I doing this? Probably because Im a glutton for punishment, and Id rather procrastinate than write my dissertation proposal. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 1.1 What this book is, and what it is not You know those absolutely amazing, comprehensive guides where you can learn everything you need to know about R? This is is not one of those guides. This book is designed as a quick reference guide for many of the most common things youll need to do in everyday data analysis and research. Think of it like a coding dictionary, as opposed to a manual or comprehensive text. If you want (or need) to learn R in-depth and/or from the ground up (i.e., youre a novice user), then you should go read Hadley Wickhams book and the tidyverse websites. Also, these slides might be a good high-level overview if youve never used the tidyverse before. That said, if youre already familiar with R and the tidyverse and just need a quick reference for what command do I need to accomplish XYZ, youve come to the right place. "],["introduction-r-basics.html", "Chapter 2 Introduction: R Basics 2.1 Importing Data 2.2 Exporting (i.e., saving) Data and Output", " Chapter 2 Introduction: R Basics For the love of God before you do anything, familiarize yourself with R Projects and the here package. These make R so much more user friendly and less of a nightmare. If you need an overview, go here: http://jenrichmond.rbind.io/post/how-to-use-the-here-package/ Now lets get stuck in. library(tidyverse) 2.1 Importing Data 2.1.1 Spreadsheets See https://nacnudus.github.io/spreadsheet-munging-strategies/index.html for more detailed and in-depth tutorials (if you need that kind of thing) 2.2 Exporting (i.e., saving) Data and Output 2.2.1 Exporting to .CSV Generally speaking, unless you have a specific reason to, dont. But if you must: write_csv() 2.2.2 Export to .RData (and load the data again later) save(obj_name, file=here::here(&quot;subfolder&quot;, &quot;save_file_name&quot;), compress = FALSE) load(here::here(&quot;folder&quot;, &quot;save_name.RData&quot;)) 2.2.3 Export to Excel library(openxlsx) #Method 1: If you only want to export 1 thing, and/or only need output document #write as object, with no formatting: write.xlsx(objectname,file = &quot;filenamehere.xlsx&quot;,colnames=TRUE, borders=&quot;columns&quot;) #write as table: write.xlsx(objectname,&quot;filename.xlsx&quot;,asTable = TRUE) #Method 2: If you want to do the above, but add multiple objects or tables to one workbook/file: ## first Create Workbook object wb &lt;- createWorkbook(&quot;AuthorName&quot;) #then add worksheets (as many as desired) addWorksheet(wb, &quot;worksheetnamehere&quot;) #then write the object to the worksheet writeData(wb, &quot;test&quot;, nameofobjectordataframe, startCol = 2, startRow = 3, rowNames = TRUE) #save excel file saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) #Method 3: exact same as method 2, but creating a more fancy tables wb &lt;- createWorkbook(&quot;Ryan&quot;) addWorksheet(wb, &quot;worksheetnamehere&quot;) writeDataTable(wb, sheetName, objectName, startCol = 1, startRow = 1, colNames = TRUE, rowNames = FALSE, tableStyle=&quot;TableStyleLight2&quot;,tableName=NULL, headerStyle = NULL,withFilter=FALSE,keepNA=TRUE,sep=&quot;, &quot;, stack = FALSE, firstColumn = FALSE, lastColumn = FALSE,bandedRows = TRUE,bandedCols = FALSE) saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) 2.2.4 Access/edit specific cell number values rainbow=tibble::tribble(~Color, &quot;red&quot;, &quot;orange&quot;, &quot;black&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) rainbow$Color[3] # access, but can&#39;t overwrite this way ## [1] &quot;black&quot; rainbow[3,&quot;Color&quot;] # access and can overwrite ## # A tibble: 1 x 1 ## Color ## &lt;chr&gt; ## 1 black rainbow[3, &quot;Color&quot;]= &quot;yellow&quot; # save this value to row 3 in column &quot;Color&quot; rainbow ## # A tibble: 6 x 1 ## Color ## &lt;chr&gt; ## 1 red ## 2 orange ## 3 yellow ## 4 green ## 5 blue ## 6 purple "],["wrangle-data.html", "Chapter 3 Wrangle Data 3.1 Joining or Splitting 3.2 Selecting/extracting specific variables 3.3 If-then and Case-when 3.4 Conditional replacement of values 3.5 Merging variables 3.6 Apply a function to multiple variables at once 3.7 Pivoting (i.e., transposing) data 3.8 Turn row names into a column/variable 3.9 How to edit/change column names 3.10 Re-order columns in a data set 3.11 Date and time variables 3.12 Reverse-code a variable 3.13 Dummy coding (the very fast and easy way) 3.14 Create a relative ranking among several variables 3.15 Manipulating the working environment and many things at once 3.16 Wrangling Lists", " Chapter 3 Wrangle Data This chapter contains useful tips on wrangling (i.e., manipulating) data. If you need to know to do to things like create new variables, split one variable into multiple variables, pivot a data set from wide to long, etc., look no further. If you want a pretty good intro tutorial to the dplyr package, click here 3.1 Joining or Splitting Joining and splitting data is pretty straightforward. 3.1.1 Whole Data Sets The code below is from this excellent tutorial set.seed(2018) df1=data.frame(customer_id=c(1:10), product=sample(c(&#39;toaster&#39;,&#39;TV&#39;,&#39;Dishwasher&#39;),10,replace = TRUE)) df2=data.frame(customer_id=c(sample(df1$customer_id, 5)),state=sample(c(&#39;New York&#39;,&#39;California&#39;),5,replace = TRUE)) df1=tibble::as_tibble(df1) df2=tibble::as_tibble(df2) # df1 =left table # df2= right table Inner join - retains only rows with values that appear in both tables, and matches by keys. If youre joining two Qualtrics surveys together, this is most likely the one you want to use (e.g. matching by participant name, and only keeping rows in the joined data set for participants that have responses logged in both survey 1 and survey 2 df1 %&gt;% inner_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York Left join - returns everything in the left, and rows with matching keys in the right df1 %&gt;% left_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Right join - returns everything in the right, and rows with matching keys in the left df1 %&gt;% right_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York # note: example if the customer id column was named something different in the second df #df1 %&gt;% left_join(df2,by=c(&#39;customer_id&#39;=&#39;name2&#39;)) Full join - retain all rows from both tables, and join matching keys in both right and left df1 %&gt;% full_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Anti join - returns all rows in the left that do not have matching keys in the right df1 %&gt;% anti_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 2 ## customer_id product ## &lt;int&gt; &lt;chr&gt; ## 1 2 Dishwasher ## 2 4 toaster ## 3 5 TV ## 4 7 toaster ## 5 10 TV 3.1.2 Individual Columns/Variables Splitting or joining columns is much easier than doing it to whole data sets. You can use dplyr::separate() to accomplish the former, and dplyr::unite() for the latter. print(&quot;hello&quot;) ## [1] &quot;hello&quot; 3.2 Selecting/extracting specific variables Sometimes when working with a data set, you want to work with a few specific variables. For instance, maybe you want to view a graph of only reverse-coded variables (which start with the prefix r); or maybe you want to create a subset of your data that has a few specific variables removed. For this you can use dplyr::select() and its associated helper commands select() can be thought of as extract; it tells R to identify and extract a specific variable (or variables) cars=mtcars # select one column cars %&gt;% select(mpg) # select multiple columns, if they are all next to one another cars %&gt;% select(mpg:hp) # select multiple columns by name (when not next to one another) by defining them in a vector cars %&gt;% select(c(mpg, hp, wt)) # select only variables that start with a certain prefix/character/pattern/etc. cars %&gt;% select(starts_with(&quot;d&quot;)) # ...or columns that end with a certain prefix/etc. cars %&gt;% select(ends_with(&quot;t&quot;)) # ...or contains a certain pattern or string cars %&gt;% select(contains(&quot;se&quot;)) # select ALL OF the variables in a data set that match those of a pre-defined vector # first define the names in a vector vars=c(&quot;hp&quot;, &quot;drat&quot;, &quot;gear&quot;, &quot;carb&quot;) #now use helper cars %&gt;% select(all_of(vars)) # select ANY OF the variables in a pre-defined vector vars_2=c(&quot;hp&quot;, &quot;drat&quot;, &quot;watermelon&quot;, &quot;grilled_cheese&quot;) # only the first two will be in the data cars %&gt;% select(any_of(vars_2)) # only (and all of) the variables actually PRESENT in the data are pulled # select only variables of a certain class or type cars %&gt;% select(where(is.numeric)) cars %&gt;% select(where(is.character)) Other examples can be seen on THIS LINK for a simple but detailed guide. 3.3 If-then and Case-when 3.3.1 If-then The premise of an if/then or if/else statement is simple: If condition 1 is satisfied, perform x operation; if not, then do y mtcars %&gt;% mutate(power_level=ifelse(mtcars$hp&lt;350, &quot;Low&quot;, &quot;High&quot;)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb power_level ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Low ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Low ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Low ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Low ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Low ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Low This line of code effectively says: if the length in Sepal.Length is &gt;5, set new variable = to short; else, set it to long 3.3.2 Case-when When you have 3+ conditions, its easier to use case-when. This is a more simple and straightforward approach than nesting multiple if-else commands My_vector= case_when( Condition1 ~ value1, Condition2 ~ value2, Condition3 ~ value3 TRUE ~ valueForEverythingElse #catch all for things that don&#39;t meet the above conditions ) Example: mtcars %&gt;% mutate(size= case_when(cyl==4 ~ &quot;small&quot;, cyl==6 ~ &quot;medium&quot;, cyl==8 ~ &quot;large&quot;)) %&gt;% select(c(cyl,size)) %&gt;% head() ## cyl size ## Mazda RX4 6 medium ## Mazda RX4 Wag 6 medium ## Datsun 710 4 small ## Hornet 4 Drive 6 medium ## Hornet Sportabout 8 large ## Valiant 6 medium 3.4 Conditional replacement of values The following code is useful if you want to replace a value in one column, and the replacement is conditional upon the value in another column. mpg %&gt;% mutate(across(.cols = c(displ, cty, hwy), .fns = ~case_when(cyl == 4L ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 NA 1999 4 auto(l5) f NA NA p compact ## 2 audi a4 NA 1999 4 manual(m5) f NA NA p compact ## 3 audi a4 NA 2008 4 manual(m6) f NA NA p compact ## 4 audi a4 NA 2008 4 auto(av) f NA NA p compact ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact ## 7 audi a4 3.1 2008 6 auto(av) f 18 27 p compact ## 8 audi a4 quattro NA 1999 4 manual(m5) 4 NA NA p compact ## 9 audi a4 quattro NA 1999 4 auto(l5) 4 NA NA p compact ## 10 audi a4 quattro NA 2008 4 manual(m6) 4 NA NA p compact ## # ... with 224 more rows test %&gt;% mutate(across(.cols = c(rank), .fns = ~case_when(is.na(participant_score) ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) 3.5 Merging variables Sometimes youll have multiple variables and you want to collapse them into a single variable. The pmin() command is useful for this. example_data=tribble(~A,~B,~C, 1,NA,NA, 2,NA,NA, 3,NA,NA, NA,4,NA, NA,5,NA, NA,6,NA, NA,NA,7, NA,NA,8, NA,NA,9) example_data %&gt;% mutate(accept_reject = pmin(A,B,C,na.rm = TRUE)) 3.6 Apply a function to multiple variables at once You can either specify each column individually, like above, or tell R to identify columns for you based on their type or their name. This requires adding in one additional verbeither contains() or where() depending on what you want to do. Two simple examples: # turn multiple variables into factors ex_data=dplyr::tribble(~color, ~car, &quot;red&quot;, &quot;corvette&quot;, &quot;blue&quot;, &quot;chevelle&quot;, &quot;green&quot;, &quot;camaro&quot;, &quot;red&quot;, &quot;corvette&quot;, &quot;green&quot;, &quot;chevelle&quot;, &quot;yellow&quot;, &quot;gto&quot;) dplyr::glimpse(ex_data) ## Rows: 6 ## Columns: 2 ## $ color &lt;chr&gt; &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;green&quot;, &quot;yellow&quot; ## $ car &lt;chr&gt; &quot;corvette&quot;, &quot;chevelle&quot;, &quot;camaro&quot;, &quot;corvette&quot;, &quot;chevelle&quot;, &quot;gto&quot; ex_data %&gt;% mutate(across(c(color, car),factor)) ## # A tibble: 6 x 2 ## color car ## &lt;fct&gt; &lt;fct&gt; ## 1 red corvette ## 2 blue chevelle ## 3 green camaro ## 4 red corvette ## 5 green chevelle ## 6 yellow gto # round multiple columns to 1 decimal place mtcars %&gt;% mutate(across(c(disp:qsec),round,1)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.9 2.6 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.9 2.9 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.9 2.3 18.6 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.1 3.2 19.4 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.1 3.4 17.0 0 0 3 2 ## Valiant 18.1 6 225 105 2.8 3.5 20.2 1 0 3 1 3.7 Pivoting (i.e., transposing) data 3.7.1 Condense multiple rows into a single column (pivot wide to long) Rearranging data like this can make it easier to work with and analyze. Example below from my gradebook for stats (exported from Canvas), with fake names. The command structure is as follows: pivot_longer( # Transpose LENGTHWISE by.... cols = everything(), # Taking ALL variable names... names_to=&quot;variable&quot;, # ...and dumping them into this new variable/column values_to=&quot;missing_count&quot;) #...and placing their values in this other new column NOTE!!! Pivoting data from wide to long like this expands the number of rows to make a matrix so that (for example, each student now has as a row for each assignment). Therefore, you can only pivot longways (or wide) ONCE, otherwise you will make duplicates. If you need to pivot multiple columns, just include all of the columns in one single pivot; do not use two separate, back to back pivot commands. Example: gradebook=tibble::tribble( ~Student, ~Homework.1, ~Homework.2, ~Homework.3, ~Homework.4, ~Homework.5, ~Quiz.1, ~Quiz.2, ~Quiz.3, ~Quiz.4, ~Final, &quot;Bob&quot;, 19L, 0L, 13, 16, 0L, 21, 7L, 15, 17.5, 33, &quot;Jane&quot;, 17L, 19L, 16, 16.5, 25L, 21.5, 19L, 14.75, 9.5, 39.5, &quot;John&quot;, 19L, 19L, 14.5, 19.5, 25L, 21, 21L, 18.5, 17, 46.5 ) head(gradebook) ## # A tibble: 3 x 11 ## Student Homework.1 Homework.2 Homework.3 Homework.4 Homework.5 Quiz.1 Quiz.2 Quiz.3 Quiz.4 Final ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bob 19 0 13 16 0 21 7 15 17.5 33 ## 2 Jane 17 19 16 16.5 25 21.5 19 14.8 9.5 39.5 ## 3 John 19 19 14.5 19.5 25 21 21 18.5 17 46.5 gradebook=gradebook %&gt;% pivot_longer( # Transpose lengthwise by: cols = Homework.1:Final, # Taking these variables names_to=&quot;Assignment&quot;, # ...and dumping them into this new variable, storing them lengthwise values_to=&quot;Points&quot;) #...then place their values in this new column gradebook %&gt;% head() ## # A tibble: 6 x 3 ## Student Assignment Points ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bob Homework.1 19 ## 2 Bob Homework.2 0 ## 3 Bob Homework.3 13 ## 4 Bob Homework.4 16 ## 5 Bob Homework.5 0 ## 6 Bob Quiz.1 21 3.8 Turn row names into a column/variable Use the rownames() command to turn row names into a variable cars=rownames_to_column(mtcars, var = &quot;car&quot;) as_tibble(cars) %&gt;% slice(1:6) ## # A tibble: 6 x 12 ## car mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 Hornet 4 Drive 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 3.9 How to edit/change column names TWO WAYS TO DO THIS: Use colnames() (for base R) or rename() (for tidyverse) colnames() pulls up all the column/variable names as a vector. If you want to actually change them, youll need to combine this command with something like the sub() or gsub() commands (for base R). Im going to skip this becauseits base R. To access and change the names faster via tidyverse, run use rename() rm(list=ls()) # clear R&#39;s memory iris %&gt;% rename(&quot;hurr&quot;=&quot;Sepal.Length&quot;, &quot;durr&quot;=&quot;Sepal.Width&quot;, &quot;abcdefgh&quot;=&quot;Species&quot;) %&gt;% head() ## hurr durr Petal.Length Petal.Width abcdefgh ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa If you need to do some really fancy conditional renaming (e.g., changing all variables that start with r to start with rf instead, to make it more clear that the prefix actually stands for risk factor rather than reverse coded), youll need to use rename_with(). This command has two parts to it: the data set, and the function you wish to apply to it (which you put after the ~) rename_with(iris, ~ gsub(pattern = &quot;.&quot;, replacement = &quot;_&quot;, .x, fixed = TRUE)) %&gt;% head() ## Sepal_Length Sepal_Width Petal_Length Petal_Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa The gsub() function from Base R identifies matching patterns in the data and substitutes them with what you want instead. Think of it like Rs version of Find/Replace from Microsoft Word. The above line of code thus does the following: 1. First, it checks the column names of the supplied data set (iris) for a specific pattern (specified in pattern= ) 2. Then it replaces that pattern with your input in replacement= The great thing about rename_with() is that the .fn (or ~ for short) can take ANY function as input. For example, if you want to add an element to the column names rather than replace something, (e.g., a prefix or suffix), you can change the function to: rename_with( iris, ~ paste0(.x, &quot;_text&quot;)) %&gt;% head() ## Sepal.Length_text Sepal.Width_text Petal.Length_text Petal.Width_text Species_text ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa The above line adds a suffix. You can also add a prefix in the exact same way, just by switching the order of the string and the pattern in the paste0 command. Alternative method to the above This is a second way to do the above. It may appear more simple, but its also probably not as theoretically consistent with how the packages were made..it uses the stringr package to rename the column names, and stringr is typically used for editing vectors of strings in a data set. so it works, but its a little unconventional because you call and edit the column names like you would a variable in your data set. colnames(iris)=str_replace(colnames(iris), pattern = &quot;.&quot;, replacement = &quot;_&quot;) In short: rename() and rename_with() are for renaming variables, as their names imply. The str_ verbs from the stringr package are for editing string-based variabels in your data set. Either works though with a little ingenuity. 3.10 Re-order columns in a data set Use relocate() to change column positions. If you need to move multiple columns at once, this command uses the same syntax as select(). mtcars # notice the column order ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 mtcars %&gt;% relocate(hp:wt, .after= am) %&gt;% head() ## mpg cyl disp qsec vs am hp drat wt gear carb ## Mazda RX4 21.0 6 160 16.46 0 1 110 3.90 2.620 4 4 ## Mazda RX4 Wag 21.0 6 160 17.02 0 1 110 3.90 2.875 4 4 ## Datsun 710 22.8 4 108 18.61 1 1 93 3.85 2.320 4 1 ## Hornet 4 Drive 21.4 6 258 19.44 1 0 110 3.08 3.215 3 1 ## Hornet Sportabout 18.7 8 360 17.02 0 0 175 3.15 3.440 3 2 ## Valiant 18.1 6 225 20.22 1 0 105 2.76 3.460 3 1 3.11 Date and time variables Formatting a column of dates can be extremely helpful if you need to work with time data, but also an extreme pain in the ass if its not stored correctly. This tutorial will be divided into two parts to cover both scenarios that you could encounter. It requires things to be done in two stages, and very precisely. 3.11.1 Date-time objects If youre lucky enough to have a vector of date-times, like what Qualtrics gives you, this will be brainless. Just do the following: example_datetime_data=tibble::tribble(~datetime, &quot;2010-08-03 00:50:50&quot;, &quot;2010-08-04 01:40:50&quot;, &quot;2010-08-07 21:50:50&quot;) head(example_datetime_data) # stored as character string ## # A tibble: 3 x 1 ## datetime ## &lt;chr&gt; ## 1 2010-08-03 00:50:50 ## 2 2010-08-04 01:40:50 ## 3 2010-08-07 21:50:50 # Tidyverse lubridate::as_date(example_datetime_data$datetime) ## [1] &quot;2010-08-03&quot; &quot;2010-08-04&quot; &quot;2010-08-07&quot; 3.11.2 Date-only objects If youre unlucky enough to have only dates, and said dates are written in the traditional x/x/xxxx format, this will be an annoyance that has to be done in two stages. First, assuming your data is already imported and is being stored as a vector of character strings, you have to tell R to adjust the formatting of dates. You cannot change it from a character-based object into a Date or DateTime one until it recognizes the correct formatting. example_date_data=tibble::tribble(~X1, ~X2, &quot;8/4/2021&quot;, -49.87, &quot;8/4/2021&quot;, -13.85, &quot;8/3/2021&quot;, -7.45, &quot;8/3/2021&quot;, -172.71) # Correct formatting example_date_data$X1=format(as.POSIXct(example_date_data$X1,format=&#39;%m/%d/%Y&#39;),format=&#39;%Y-%m-%d&#39;) head(as_tibble(example_date_data)) ## # A tibble: 4 x 2 ## X1 X2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 2021-08-04 -49.9 ## 2 2021-08-04 -13.8 ## 3 2021-08-03 -7.45 ## 4 2021-08-03 -173. In the code above, note that there are two format commands: The first one tells R how the date data is currently being stored, while the second at the end tells it how you want it to be stored. In this case, we are changing it from the way we would usually hand write a date (e.g., 10/26/1993) to a format commonly recognized and used in Excel and stats software (1993-10-26). If your column also has times in it, you also need to include that too! Second, you can now correct the objects structure. You can do this with base Rs as.Date() or tidyverses date() verbs. # Tidyverse example_date_data$X1= lubridate::date(example_date_data$X1) # Base R version example_date_data$X1=as.Date(example_date_data$X1) Notice how the object is now stored as the correct type in the table above. NOTE! This entire process has been included in the tidy_date() command in my package, legaldmlab. 3.11.3 Find the difference between two dates/times difftime(part_1$end_date[1], part_2$end_date[1], units=&quot;days&quot;) 3.12 Reverse-code a variable To reverse-score a variable, you should use car::recode() Can be done a few different ways, depending on how many variables youre looking to recode: # Recode just one variable df$column=recode(df$column,&quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;) # Recode a select bunch of variables df=df %&gt;% mutate(across(c(family_close : family_feelings), recode, &quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;)) # Recode the whole damn thing. All columns. df=df %&gt;% map_df(recode, &quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;) 3.13 Dummy coding (the very fast and easy way) Use dplyrs pivot_wider in conjunction with mutate to very quickly and automatically dummy code a column with any number of unique values. The middle part of the code below is what you needjust copy and paste it, and tweak the specifics library(tidyverse) mtcars |&gt; mutate(car=rownames(mtcars)) |&gt; dplyr::mutate(n=1) |&gt; tidyr::pivot_wider(names_from = cyl, values_from = n, names_prefix = &quot;number_cyl&quot;, values_fill = list(n=0)) |&gt; select(car, starts_with(&quot;number_&quot;)) |&gt; head() #truncate output for easier reading ## # A tibble: 6 x 4 ## car number_cyl6 number_cyl4 number_cyl8 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 1 0 0 ## 2 Mazda RX4 Wag 1 0 0 ## 3 Datsun 710 0 1 0 ## 4 Hornet 4 Drive 1 0 0 ## 5 Hornet Sportabout 0 0 1 ## 6 Valiant 1 0 0 3.14 Create a relative ranking among several variables If you want to create a variable that is an ordinal ranking of other variables, first you need to make sure your data is long-wise. Then, depending on the type of ranking system you want, youll might need a different ranking command. The min_rank command from dplyr works in a manner similar to base Rs rank command. It ranks things like you see in sporting events. For example, if there is a clear winner in a game but 3 people tie for second place, the ranks would look like this: 1,2,2,2,4,5. Notice that the positions are independent from the counts. Using the same example from above, if you want the ranks to have no gaps (i.e. 1,2,2,2,3,4), you need to use dplyrs dense_rank command. In either case, the ranks are generated from lowest to highest, so if you want to flip them around youll need to include desc() in the command. dat=tibble::tribble(~name, ~score, &quot;bob&quot;, 0, &quot;bob&quot;, 5, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, NA, &quot;alice&quot;, 70, &quot;alice&quot;, 80, &quot;alice&quot;, 90, &quot;alice&quot;, 20, &quot;alice&quot;, 20, &quot;alice&quot;, 1) dat %&gt;% mutate(ranked = dense_rank(desc(score))) ## # A tibble: 12 x 3 ## name score ranked ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 bob 0 8 ## 2 bob 5 6 ## 3 bob 50 4 ## 4 bob 50 4 ## 5 bob 50 4 ## 6 bob NA NA ## 7 alice 70 3 ## 8 alice 80 2 ## 9 alice 90 1 ## 10 alice 20 5 ## 11 alice 20 5 ## 12 alice 1 7 3.15 Manipulating the working environment and many things at once 3.15.1 Stuff the WHOLE working environment into a list files=mget(ls()) 3.15.2 Extract everything from a list into the environment list2env(cog_data, globalenv()) 3.15.3 Delete everything in the entire environment, except for one item rm(list=setdiff(ls(), &quot;cog_data&quot;)) # delete everything in the local environment except the final data set 3.16 Wrangling Lists 3.16.1 Nesting Imagine the concept of Russian Dolls, applied to data sets. You can manage data sets more effectively my collapsing them into a single tiny, mini data frame, and stuffing that inside of another one. This is done via nesting Effectively, you smush/collapse everything down so it fits inside one column. You can unnest to expand this data back out later when you need it, and keep it collapsed when you dont. This works because a vector/column in a data frame is a list of a defined length; and a data frame is thus simply a collection of lists that are all the same length. You can store anything in a data frame. You can keep the df connected to the model, which makes it very easy to manage a whole slew of related models You can use functional programming (i.e., iterative functions) to map functions or combinations of functions in new ways. Moreover and more importantly, when you use purrr to map functions onto multiple models or objects simultaneously, youre doing it to all of them at once with a single command, and the objects are kept together while you do it. This limits the mistakes you can make (e.g., copying and pasting code and forgetting to tweak something important; applying a function to the wrong object or set of objects by accident), and also reduces unnecessary code in your script. Converting data into tidy data sets gives you a whole new way (and easier way) to manage lots of information head(mtcars |&gt; nest(crap=vs:carb)) ## # A tibble: 6 x 8 ## mpg cyl disp hp drat wt qsec crap ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; ## 1 21 6 160 110 3.9 2.62 16.5 &lt;tibble [1 x 4]&gt; ## 2 21 6 160 110 3.9 2.88 17.0 &lt;tibble [1 x 4]&gt; ## 3 22.8 4 108 93 3.85 2.32 18.6 &lt;tibble [1 x 4]&gt; ## 4 21.4 6 258 110 3.08 3.22 19.4 &lt;tibble [1 x 4]&gt; ## 5 18.7 8 360 175 3.15 3.44 17.0 &lt;tibble [1 x 4]&gt; ## 6 18.1 6 225 105 2.76 3.46 20.2 &lt;tibble [1 x 4]&gt; 3.16.2 Combining/collapsing list levels (Reducing) demo_vars=files |&gt; map(import_spss) |&gt; reduce(left_join, by=&quot;catieid&quot;) "],["clean-data.html", "Chapter 4 Clean Data 4.1 Replace a value with NA 4.2 Replace NAs with a value 4.3 Identify columns or rows with Missing values 4.4 Find the percentage of a variable that is missing 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set", " Chapter 4 Clean Data 4.1 Replace a value with NA Use dplyr::na_if() if you have a value coded in your data (e.g., 999) that you want to convert to NA example_data=dplyr::tribble(~name, ~bday_month, &quot;Ryan&quot;, 10, &quot;Z&quot;, 3, &quot;Jen&quot;, 999, &quot;Tristin&quot;, 999, &quot;Cassidy&quot;, 6) example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen 999 ## 4 Tristin 999 ## 5 Cassidy 6 example_data$bday_month=na_if(example_data$bday_month, 999) #example doing one column at a time example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 example_data %&gt;% # can also pass the data to mutate and do it the tidyverse way mutate(bday_month=na_if(bday_month, 999)) ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 4.2 Replace NAs with a value tidyr::replace_na() is very useful if you have some NAs in your data and you want to fill them in with some value. example_data=tibble::tribble(~name, ~fav_color, ~fav_food, &quot;Ryan&quot;, &quot;green&quot;, &quot;Mexican&quot;, &quot;Cassidy&quot;, &quot;blue&quot;, NA, &quot;Z&quot;, NA, NA, &quot;Tristin&quot;, &quot;purple&quot;, NA, &quot;Tarika&quot;, NA, NA, &quot;Jen&quot;, NA, &quot;Italian&quot;) example_data ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue &lt;NA&gt; ## 3 Z &lt;NA&gt; &lt;NA&gt; ## 4 Tristin purple &lt;NA&gt; ## 5 Tarika &lt;NA&gt; &lt;NA&gt; ## 6 Jen &lt;NA&gt; Italian # replace NA&#39;s in one col tidyr::replace_na(example_data$fav_food, &quot;MISSING&quot;) ## [1] &quot;Mexican&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;Italian&quot; # replace in multiple columns example_data %&gt;% mutate(across(c(fav_color, fav_food), replace_na, &quot;MISSING&quot;)) ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue MISSING ## 3 Z MISSING MISSING ## 4 Tristin purple MISSING ## 5 Tarika MISSING MISSING ## 6 Jen MISSING Italian 4.3 Identify columns or rows with Missing values is.na() is the base R way to identify, in a TRUE/FALSE manner, whether or not there are missing values in a vector y &lt;- c(1,2,3,NA) is.na(y) # returns a vector (F F F T) ## [1] FALSE FALSE FALSE TRUE 4.4 Find the percentage of a variable that is missing Sometimes necessary to check before conducting an analysis. This requires my package, legaldmlab ?legaldmlab::count_missing mtcars %&gt;% select(hp:drat) %&gt;% legaldmlab::count_missing() ## # A tibble: 2 x 3 ## variable missing_count percent_missing ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 hp 0 0.0% ## 2 drat 0 0.0% 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set Use tidyr::drop_na() to remove rows with missing values. example_data=dplyr::tribble(~name, ~bday_month, ~car, &quot;Ryan&quot;, 10, &quot;kia&quot;, &quot;Z&quot;, NA, &quot;toyota&quot;, &quot;Jen&quot;, NA, NA, &quot;Tristin&quot;, 999, NA, &quot;Cassidy&quot;, 6, &quot;honda&quot;) knitr::kable(example_data) name bday_month car Ryan 10 kia Z NA toyota Jen NA NA Tristin 999 NA Cassidy 6 honda example_data %&gt;% drop_na() # with nothing specified, it drops ALL variables that have &gt;=1 missing value ## # A tibble: 2 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Cassidy 6 honda example_data %&gt;% drop_na(car) # drops only rows with values missing in the specified column ## # A tibble: 3 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Z NA toyota ## 3 Cassidy 6 honda "],["working-with-factors.html", "Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels 5.2 Collapse factor levels 5.3 Add levels to a factor 5.4 Drop unused levels 5.5 Change the order of a factors levels", " Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels Use forcats::fct_recode() diamonds=diamonds %&gt;% as_tibble() diamonds$cut=fct_recode(diamonds$cut, &quot;meh&quot;=&quot;Fair&quot;, &quot;Wow&quot;=&quot;Premium&quot;) summary(diamonds$cut) ## meh Good Very Good Wow Ideal ## 1610 4906 12082 13791 21551 5.2 Collapse factor levels Extremely useful command for when you have infrequent cases in one factor and need to combine it with another. Works by specifying a series of new level names, each of which contains the information from the old variables. Format is as follows: fct_collapse(dataset$variable, NewLevelA=c(&quot;OldLevel1&quot;,&quot;Oldlevel2&quot;), # NewLevelA is the new variable that contains both variables 1 and 2 NewLevelB=c(&quot;OldLevel3&quot;)) 5.3 Add levels to a factor use fct_expand() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.4 Drop unused levels Use fct_drop() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.5 Change the order of a factors levels example_data=tribble(~person, ~condition, &quot;bob&quot;, &quot;25 years&quot;, &quot;jane&quot;, &quot;5 years&quot;, &quot;jim&quot;, &quot;5 years&quot;, &quot;john&quot;, &quot;25 years&quot;) example_data$condition=factor(example_data$condition) str(example_data$condition) ## Factor w/ 2 levels &quot;25 years&quot;,&quot;5 years&quot;: 1 2 2 1 Notice that R thinks these are nominal factors, and that 25 comes before 5. To fix this and correct the level order example_data$condition =fct_relevel(example_data$condition, c(&quot;5 years&quot;, &quot;25 years&quot;)) # specify level order str(example_data$condition) ## Factor w/ 2 levels &quot;5 years&quot;,&quot;25 years&quot;: 2 1 1 2 "],["working-with-strings.html", "Chapter 6 Working with Strings 6.1 Remove a pattern from a string 6.2 Replace one pattern in a string with another 6.3 Find (i.e., filter for) all instances of a string 6.4 Drop all rows from a data set that contain a certain string 6.5 Force all letters to lower case", " Chapter 6 Working with Strings 6.1 Remove a pattern from a string price_table=tribble(~car, ~price, &quot;Corvette&quot;, &quot;$65,000&quot;, &quot;Mustang GT&quot;, &quot;$40,000&quot;) # BASE R METHOD (sub by replacing something with nothing) gsub(&quot;\\\\$&quot;, &quot;&quot;,price_table$price) # (pattern, replace with, object$column) ## [1] &quot;65,000&quot; &quot;40,000&quot; # TIDYVERSE METHOD str_remove(price_table$price, pattern = &quot;\\\\$&quot;) ## [1] &quot;65,000&quot; &quot;40,000&quot; 6.2 Replace one pattern in a string with another Tidyverse command: str_replace() Base R command: gsub() # base R gsub(mtcars, replacement = ) #tidyverse 6.3 Find (i.e., filter for) all instances of a string Useful for finding very specific things inside a column (e.g., one particular persons name in a roster of names; everyone with a particular last name) Tidyverse command: str_detect() Base R command: grepl() Note both must be nested inside of filter() cars_df=rownames_to_column(mtcars, var = &quot;car&quot;) # base R cars_df |&gt; filter(grepl(&quot;Firebird&quot;, car)) # tidyverse cars_df %&gt;% filter(str_detect(car,&quot;Firebird&quot;)) You can also search for multiple strings simultaneously by including the or logical operator inside the quotes. cars_df |&gt; filter(str_detect(car, &quot;Firebird|Fiat&quot;)) You can also include the negation logical operator to filter for all instances except those with the specified string. # base R cars_df |&gt; filter(!(grepl(&quot;Pontiac&quot;, car))) # tidyverse cars_df |&gt; filter(!(str_detect(car, &quot;Pontiac&quot;))) 6.4 Drop all rows from a data set that contain a certain string # Tidyverse method cars_df |&gt; filter(str_detect(car, &quot;Merc&quot;, negate = TRUE)) #including negate=TRUE will negate all rows with the matched string # base R cars_df[!grepl(&quot;Merc&quot;, cars_df$car),] 6.5 Force all letters to lower case Use stringr::str_to_lower() blah=tribble(~A, ~B, &quot;A&quot;,&quot;X&quot;, &quot;A&quot;,&quot;X&quot;) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 A X ## 2 A X blah$A=str_to_lower(blah$A) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 a X ## 2 a X "],["figures-and-graphs-with-the-ggplot-and-see-packages.html", "Chapter 7 Figures and Graphs with the ggplot and see packages 7.1 Commands for ggplot graph types 7.2 Specific Commands for Specific Types of Analysis 7.3 Highlight specific points 7.4 Add labels to data points 7.5 Plotting multiple graphs at once 7.6 Change the colors (bars; columns; dots; etc.) 7.7 Other aesthetic mappings 7.8 Adding and Customizing Text 7.9 Remove gridlines 7.10 Faceting 7.11 Log transformations 7.12 Changing the scale of the axis 7.13 Add a regression line 7.14 Save a graph or figure", " Chapter 7 Figures and Graphs with the ggplot and see packages There are three parts to a ggplot2 call: 1. data 2. aesthetic mapping 3. Layer There is no piping involved in ggplot. You simply invoke ggplot, and tell it what they dataset is. Then you specify the aesthetics, and then the mapping. Lastly, include other optional stuff (e.g. expanded y-axis scale; titles and legends; etc.) Every single plot has the exact same layout that ONLY USES the above three points: ggplot(dataframe, aes(graph dimensions and variables used)) + geom_GraphType(specific graph controls) ## OR ## ggplot(dataframe) + geom_GraphType(aes(graph dimensions and variables used), specific graph controls) # mapping= aes() can go in either spot Then if you have other stuff you want to add on top of this, like axis labels, annotations, highlights, etc., you keep adding those in separate lines 7.1 Commands for ggplot graph types Graph Type Geom command Scatter geom_point() Line geom_line() Box geom_boxplot() Bar geom_bar() Column geom_col() Histogram geom_histogram() Density curve geom_density() Note that bar and column graphs look identical at first glance, but they serve two different purposes. Bar graphs are for frequency counts, and thus only take an X-axis variable; Column graphs are for showing the relationship between two variables X and Y, and display the values in the data # BAR GRAPH # height of bars is a frequency count of each level of the X variable cut bar_plot=ggplot(diamonds, aes(x=cut)) + geom_bar()+ theme_classic() # COLUMN GRAPH # height of bars represents relationship between price and cut col_plot=ggplot(diamonds, aes(x=cut, y=price)) + geom_col()+ theme_classic() see::plots(bar_plot, col_plot, n_columns = 2, tags = c(&quot;Bar&quot;, &quot;Column&quot;)) 7.2 Specific Commands for Specific Types of Analysis 7.2.1 lavaan stuff 7.2.1.1 Plotting an SEM or CFA model First lets set up a model to use. library(lavaan) HS.model &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9&#39; fit1 &lt;- cfa(HS.model, data=HolzingerSwineford1939) Two options for graphing it. Option 1 is graph_sem() from the tidySEM package. tidySEM::graph_sem(fit1) Option 2 is from the easystats suite plot(parameters::parameters(fit1)) ## Using `sugiyama` as default layout 7.2.2 Bayes stuff Quick highlights here of my favorite functions from this package. See (ha) the full package overview at this link You can adjust the colors of the figures by setting them yourself (with scale_fill_manual), or by using the appropriate scale_fill command 7.2.2.1 Probability of Direction (Pd) figure Use plot(pd()) to visualize the Probability of Direction index. plot(bayestestR::pd(fit1))+ scale_fill_manual(values=c(&quot;#FFC107&quot;, &quot;#E91E63&quot;))+ theme_classic()+ theme(plot.title = element_text(hjust = 0.5, size = 14, face = &quot;italic&quot;)) 7.2.2.2 ROPE figure plot(fit1, rope_color = &quot;grey70&quot;)+ gameofthrones::scale_fill_got_d(option = &quot;white_walkers&quot;) # scale_fill_manual(values = c(&quot;gray75&quot;,&quot;red&quot;) ROPE tests are plots of distributions, and therefore use scale_fill_xyz_d commands. (the d stands for discrete). You can use any scale theme color set from any package, as long as it ends in _d values=c(#FFC107, #E91E63) is the default bayestestR theme colors from their website 7.2.2.3 Bayes factor models comparison figure plot(bayesfactor_models(Thesis_Model,discount_model))+ scale_fill_flat(palette = &quot;complement&quot; , reverse = TRUE)+ # scale color adjustment 7.2.3 Histograms and density curves Since I use these so often I figure they deserve their own special section. Basic histograms can be built with the following code: ggplot(data = mtcars, aes(x=cyl)) + geom_histogram(binwidth = .5, colour=&quot;Black&quot;, fill=&quot;green&quot;) + # histogram theme_classic() and your basic density curve with the following: ggplot(diamonds, aes(x=price)) + geom_density(alpha=.3)+ # density plot. Alpha sets the transparency level of the fill. theme_classic() You can also use the following code from bayestestR to build a really quick and nice density curve plot(bayestestR::point_estimate(diamonds, centrality=c(&quot;median&quot;,&quot;mean&quot;)))+ labs(title=&quot;Mean and Median&quot;) 7.3 Highlight specific points The gghighlight package is great for this # example 1 ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp))+ # add data labels (optional) gghighlight::gghighlight(hp &gt; 200) # add highlights, according to some criteria # example 2 diamonds_abr=diamonds %&gt;% slice(1:100) ggplot(diamonds_abr, aes(x= cut, y= price, colour=price))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = diamonds_abr, aes(label = price))+ # this line labels gghighlight::gghighlight(cut %in% c(&quot;Very Good&quot;, &quot;Ideal&quot;)) #this line highlights 7.4 Add labels to data points ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp)) ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point() + geom_text(aes(label=hp, hjust=2.5, vjust=2.5)) #geom_label(aes(label = scales::comma(n)), size = 2.5, nudge_y = 6) 7.5 Plotting multiple graphs at once see::plots() is good for this. print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.6 Change the colors (bars; columns; dots; etc.) This can be done in at least two different ways, depending on your goal. To change the fill color by factor or group, add fill = ___ within the aes() command. If you want to add color and/or fill to a continuous variable, do that within the geom_density() command. If you want to add color and make all of the (bars; dots; lines; etc.) the same color, than that is a graph-wide control and needs to be put in geom_point(). This manually sets the color for the whole graph. # add a color scale to the dots ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(color=&quot;blue&quot;) If you want to add color that changes according to a variable (e.g., by factor level), then the color needs to be specified as a variable name, in the aes mapping with the other variables. ggplot(mtcars, aes(x= mpg, y=hp, color=cyl))+ geom_point() 7.6.1 Fine-tuning colors You can change the spectrum of colors to specific colors if you want. Useful for example, when making graphs for APLS presentations; you can change the colors to be Montclair State University themed. When changing the color scale of graphs, note that scale_fill commands are used for representing nominal data, while scale_color commands are for representing continuous data. As such, you use scale_fill to fill in area on a graph that shows a whole category or distinct things; and scale_color to use gradients of color to show changes in continuous data. For figures that have solid area (e.g., density; box; bar; violin plots; etc.), use scale_fill For figures that have continuous changes (e.g., line and scatter plots), use scale_color # Set colors manually ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_manual(values=c(&quot;green&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;blue&quot;)) ggplot(mtcars, aes(x = wt, y = mpg, color=as.factor(cyl)))+ geom_point() + scale_color_manual(values=c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;)) # Use color scales from a package library(gameofthrones) # NOTICE THAT scale_fill AND scale_color STILL APPLY TO THEIR RESPECTIVE GRAPH TYPES # bar graphs ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Tully&quot;) ggplot(mtcars, aes(factor(cyl), fill=factor(vs)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Daenerys&quot;) # scatter plot ggplot(mtcars, aes(x = mpg, y = disp, colour = hp))+ geom_point(size = 2) + scale_colour_got(option = &quot;Lannister&quot;) Fill graphs also come with an extra option: Setting the outline color. You can change the outline of the bar/column/etc. by specifying the color inside geom_x() # change only the fill of the bars ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() # Change the outline of the bars by adding color inside the geom_bar() command ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar(color=&quot;black&quot;) 7.6.2 More options with the see package See this link for setting color gradients for continuous variables, or using other custom color palattes like the gameofthrones package. Check out the see package for some good color scales; the commands for which are here. Incidentally, see is great not only for regular ggplot graphs, but also Bayesian stats graphs link; effect size graphs link; correlation graphs link; and more. 7.7 Other aesthetic mappings shape() controls the shapes on the graph alpha() controls transparency size() controls size Note again that if you want it to change by variable, it goes INSIDE aes(); but if you want to set it manually for the whole graph, it goes in geom_x() # shape ggplot(mtcars, aes(x= mpg, y=hp, shape=as.factor(cyl)))+ geom_point() ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(shape=23) # transparency ggplot(mtcars, aes(x= mpg, y=hp, alpha=hp))+ geom_point() # size ggplot(mtcars, aes(x= mpg, y=hp, size=cyl))+ geom_point() 7.8 Adding and Customizing Text 7.8.1 Add a title, axis labels, and captions All three can be added with labs(). ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;) 7.8.2 Center graph title Add the line theme(plot.title = element_text(hjust = 0.5)) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 7.8.3 Use different fonts See tutorial on this web page Or, use the extrafont package, and set everything using the theme() command. # Visualize new groups library(extrafont) loadfonts(device=&quot;win&quot;) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;)) 7.9 Remove gridlines Add theme(panel.grid = element_blank()) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;))+ theme(panel.grid = element_blank()) 7.10 Faceting This is dividing one plot into subplots, in order to communicate relationships better. Again, this is just a single extra command, this time at the end of the code: facet_wrap(~columnhead) The tilde sign in R means by, as in divide (something) by this print(&quot;temp&quot;) This line produces a graph of population and life expectency, breaking it down to make a separate graph per each continent 7.11 Log transformations Sometimes when your data is really squished together on a graph it is hard to read. In this case, log transformations are really helpful, to change the scale of the data. For example, by multiplying all your points by 10x To create a log transformation of the same scatter plot above, add one extra bit: scale_x_log10() print(&quot;temp&quot;) You can also make both axis be logged by adding +scale again for y 7.12 Changing the scale of the axis Add coord_cartesian(xlim = c(lower,upper)) print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.13 Add a regression line Add the line geom_smooth(method = \"lm\", formula = y ~ x) ggplot(mtcars, aes(x= mpg, y=hp, color=mpg))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, formula = y ~ x) 7.14 Save a graph or figure Use the ggsave command ggsave( &quot;panss_total_scores.png&quot;, plot = scatter_plot, device = &quot;png&quot;, path = here::here(&quot;Figures and Tables&quot;), scale = 1, width = NA, height = NA, units = c(&quot;in&quot;, &quot;cm&quot;, &quot;mm&quot;, &quot;px&quot;), dpi = 300, limitsize = TRUE, bg = NULL, ) "],["making-tables-with-flextable.html", "Chapter 8 Making Tables with flextable 8.1 APA Table Components 8.2 Indent values 8.3 Add a Horizontal border (AKA horizontal spanner) 8.4 Change font and font size 8.5 Grouped table 8.6 Complete Example", " Chapter 8 Making Tables with flextable NOTES: - j refers to the column - i refers to the row number 8.1 APA Table Components 8.2 Indent values https://davidgohel.github.io/flextable/reference/padding.html https://stackoverflow.com/questions/64134725/indentation-in-the-first-column-of-a-flextable-object Use the padding function: ft &lt;- padding(ft, i=2, j=1, padding.left=20) 8.3 Add a Horizontal border (AKA horizontal spanner) hline(., i=4, j=1:2, part = &quot;body&quot;) 8.4 Change font and font size glm_table&lt;-flextable::font(glm_table, part = &quot;all&quot;, fontname = &quot;Times&quot;) %&gt;% # Font fontsize(., size = 11, part = &quot;all&quot;) # Font size 8.5 Grouped table cars=rownames_to_column(mtcars, var = &quot;Model&quot;) test=flextable::as_grouped_data(x=cars, groups = c(&quot;cyl&quot;)) 8.6 Complete Example "],["misc.-stuff.html", "Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables 9.2 Read SPSS files into R 9.3 Turn numbers into percentages 9.4 Find all possible combindations of items in a vector 9.5 Download files from the internet 9.6 Print multiple things in one statement", " Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables Note. See Chapter 10s example purrr walk through for a guide on how to scrape multiple web tables simultaneously Simple example. library(rvest) library(tidyverse) html=read_html(&#39;https://shop.tcgplayer.com/price-guide/pokemon/base-set&#39;) %&gt;% html_table(fill = TRUE) html ## [[1]] ## # A tibble: 101 x 6 ## PRODUCT Rarity Number `Market Price` `Listed Median` `` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abra Common 43 $0.40 $0.50 View ## 2 Alakazam Holo Rare 1 $32.16  View ## 3 Arcanine Uncommon 23 $1.61 $2.00 View ## 4 Beedrill Rare 17 $3.86 $3.55 View ## 5 Bill Common 91 $0.33 $0.39 View ## 6 Blastoise Holo Rare 2 $119.42  View ## 7 Bulbasaur Common 44 $1.87 $2.53 View ## 8 Caterpie Common 45 $0.43 $0.70 View ## 9 Chansey Holo Rare 3 $21.55  View ## 10 Charizard Holo Rare 4 $357.52  View ## # ... with 91 more rows # Saved as a list by default. Now extract your table from said list html=as_tibble(html[[1]] %&gt;% # find out which number it is in the list select(&#39;PRODUCT&#39;,&#39;Rarity&#39;,&#39;Number&#39;,&#39;Market Price&#39;)) # if needed, specify which columns you want too html ## # A tibble: 101 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Abra Common 43 $0.40 ## 2 Alakazam Holo Rare 1 $32.16 ## 3 Arcanine Uncommon 23 $1.61 ## 4 Beedrill Rare 17 $3.86 ## 5 Bill Common 91 $0.33 ## 6 Blastoise Holo Rare 2 $119.42 ## 7 Bulbasaur Common 44 $1.87 ## 8 Caterpie Common 45 $0.43 ## 9 Chansey Holo Rare 3 $21.55 ## 10 Charizard Holo Rare 4 $357.52 ## # ... with 91 more rows # remove $ symbol in Price column to make it easier to work with html$`Market Price`=str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) html=html %&gt;% mutate(`Market Price`=as.numeric(`Market Price`)) # convert from string to numeric # view finished table head(html) ## # A tibble: 6 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Abra Common 43 0.4 ## 2 Alakazam Holo Rare 1 32.2 ## 3 Arcanine Uncommon 23 1.61 ## 4 Beedrill Rare 17 3.86 ## 5 Bill Common 91 0.33 ## 6 Blastoise Holo Rare 2 119. Slightly more complicated example Reading a table into R takes a few steps. Step 1 is to copy and paste the URL into the read_html() verb like below: pacman::p_load(rvest, tidyverse) exonerations_table=read_html(&quot;https://www.law.umich.edu/special/exoneration/Pages/detaillist.aspx&quot;) %&gt;% html_nodes(&quot;table.ms-listviewtable&quot;) %&gt;% html_table(fill=TRUE, header = TRUE) Sometimes if the web page is extremely basic and pretty much the only thing on it is a table, you can stop there. Most of the time though, there will be tons of other stuff on the website and you need to get more specific so R can find the table. This is the html_nodes() part of the above command; in there you specify the exact part of the web page where the table is located/what object file it is. To find this you will need to use the Developer mode in your browser. See this screenshot for an example knitr::include_graphics(here::here(&quot;pics&quot;, &quot;scrape.png&quot;)) In Firefox you open this by going to Settings &gt; More Tools &gt; Web Developer Tools (or CNTRL + Shift + I). Begin by looking through the console in the center bottom for names that look like they would be related to your table. A good place to start might be  , which contains the main body of the web page. Click on a name to expand it and see all the elements on the page contained there. Ultimately what youre looking for is what you see above: an element that, when selected, highlights ONLY the area of the web page youre looking for. To get at this you will need to keep expanding, highlighting, and clicking repeatedly.it can take some digging. Keep drilling down through page elements until you find the one that highlights the table and just the table. When you find this, look for the .ms file in that name; you should also see this in the smaller console box on the right. That is the file youll need. Write that name in the html_node command and read it into R. Thats stage 1. From here you now need to clean up the table. exonerations_table=as.data.frame(exonerations_table) # convert into a df Your table might be different, but this ones names were messed up when read in, so lets fix those first and then fix the rows and columns. # save the names to a vector table_names=exonerations_table$Last.Name[1:20] # Trim out the garbage rows and columns exonerations_table=exonerations_table %&gt;% select(Last.Name:Tags.1) %&gt;% slice(22:n()) # over-write incorrect col names with the vector of correct ones we saved above colnames(exonerations_table)=table_names # clean up names exonerations_table=exonerations_table %&gt;% janitor::clean_names() # verify structure of columns is correct # glimpse(exonerations_table) Yikes, a lot of stuff is stored incorrectly, and as a result theres some missing values that need to be addressed and other data that needs to be corrected. exonerations_table=as_tibble(exonerations_table) %&gt;% # convert to tibble mutate(across(c(dna,mwid:ild), na_if,&quot;&quot;)) %&gt;% # turn missing values into NA&#39;s mutate(across(c(dna,mwid:ild), replace_na, &quot;derp&quot;)) %&gt;% # replace NA&#39;s with a string (required for the next lines to work) mutate(dna=ifelse(dna==&quot;DNA&quot;,1,0), # change these variables from text to numeric to better facilitate analysis mwid=ifelse(mwid==&quot;MWID&quot;,1,0), fc=ifelse(fc==&quot;FC&quot;,1,0), p_fa=ifelse(p_fa==&quot;P/FA&quot;,1,0), f_mfe=ifelse(f_mfe==&quot;F/MFE&quot;,1,0)) %&gt;% mutate(across(c(st, crime, dna:f_mfe),factor)) # correct form by converting to factors And thats it! Check out final result! head(exonerations_table) ## # A tibble: 6 x 20 ## last_name first_name age race st county_of_crime tags om_tags crime sentence convicted exonerated dna x mwid fc p_fa f_mfe om ild ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abbitt Joseph 31 Black NC Forsyth CV, IO, SA &quot;&quot; Child ~ Life 1995 2009 1 &quot;&quot; 1 0 0 0 derp derp ## 2 Abbott Cinque 19 Black IL Cook CIU, IO, NC, P &quot;OF, WH~ Drug P~ Probation 2008 2022 0 &quot;&quot; 0 0 1 0 OM derp ## 3 Abdal Warith Habib 43 Black NY Erie IO, SA &quot;OF, WH~ Sexual~ 20 to Li~ 1983 1999 1 &quot;&quot; 1 0 0 1 OM derp ## 4 Abernathy Christopher 17 White IL Cook CIU, CV, H, IO, SA &quot;OF, WH~ Murder Life wit~ 1987 2015 1 &quot;&quot; 0 1 1 0 OM derp ## 5 Abney Quentin 32 Black NY New York CV &quot;&quot; Robbery 20 to Li~ 2006 2012 0 &quot;&quot; 1 0 0 0 derp derp ## 6 Acero Longino 35 Hispanic CA Santa Clara NC, P &quot;&quot; Sex Of~ 2 years ~ 1994 2006 0 &quot;&quot; 0 0 0 0 derp ILD Check out this page for a quick overview. 9.2 Read SPSS files into R Use foreign::read.spss spss_version=foreign::read.spss(here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;, &quot;JLWOP_RYAN.sav&quot;), to.data.frame = TRUE) Might also want to add as_tibble() on the end. 9.3 Turn numbers into percentages Use scales::percent(), which converts normal numbers into percentages and includes the percent sign (%) afterwards simple_table=tribble(~n_people, ~votes_in_favor, 25, 14) simple_table=simple_table %&gt;% mutate(percent_voted_for=scales::percent(votes_in_favor/n_people, accuracy = 0.1, scale = 100)) simple_table ## # A tibble: 1 x 3 ## n_people votes_in_favor percent_voted_for ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 25 14 56.0% Scale is what to multiple the original number by (e.g., convert 0.05 to 5% by x100) Accuracy controls how many places out the decimal goes 9.4 Find all possible combindations of items in a vector y &lt;- c(2,4,6,8) combn(c(2,4,6,8),2) # find all possible combinations of these numbers, drawn two at a time ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 2 2 2 4 4 6 ## [2,] 4 6 8 6 8 8 9.5 Download files from the internet 9.6 Print multiple things in one statement Use cat() from base R cat(&quot;The p-value dropped below 0.05 for the first time as sample size&quot;, 100) ## The p-value dropped below 0.05 for the first time as sample size 100 "],["intermediate-r-functions-loops-and-iterative-programming.html", "Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions 10.2 For-loops 10.3 purrr and Iterative Functions 10.4 Other purrr commands 10.5 Using purrr to manage many models", " Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions A function is a command that performs a specified operation and returns an output in accordance with that operation. You can literally make a function to do anything you want. General structure of a basic function: # example structure Function_name=function(argument){ Expressions return(output) } Argument is your input. It is the thing you want to perform the operation on. Expressions is the actual operation (or operations) you want to perform on the supplied argument return tells R to return the result of the Expression to you when done. This example function takes an input of numbers in the form of a vector and subtracts two from each. numbers=c(2,10,12,80) sub_2=function(x){ result= x - 2 return(result) } sub_2(numbers) ## [1] 0 8 10 78 We can also supply the function with a single number and it still works sub_2(100) ## [1] 98 Well this looks useful. So whats the bigger picture? One of the primary advantages of functions are that they can reduce a long and complex process, or a process that involves many steps, into a single line of code; thus, creating your own functions is a fast way to make your life easier down the line either at some point in the far future or even in just a few minutes, if you know you will be writing the code for some process two or more times. Take this script for instance. You can see from the circled parts that I needed to transform three different data sets in a similar way: knitr::include_graphics(here::here(&quot;pics&quot;, &quot;repeat_process.jpg&quot;)) Yes, I could have just done a copy-paste of the original code and tweak it slightly each time. But that is time consuming, produces a sloppier and longer script, and introduces a lot more room for error because of the repeated code and extra steps. Better to write a single function that could be applied to all three. In short, use functions to reduce a multi-step process or a process that youre implementing &gt;=2 times in a single script into one command. This saves you space and makes the script shorter; it saves you the trouble and effort of re-writing or adapting code from earlier sections; and importantly, reduces the chances of you making a coding error by proxy of the former two. As a quick example, I was able to replace each of the circled paragraphs of code above with a custom function that ran everything in one simple line. Now instead of 3 whole (and redundant) paragraphs, I now have 3 short lines, like so. na_zero_helpreint=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;reintegrate_&quot;) na_blank=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) na_zero=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) Limitations to your average, everyday functions. While reducing a whole process or sequence of commands is extremely useful, it still leaves a limitation. For instance, while we avoided copying and pasting whole paragraphs or processes, I still had to copy-paste the same function three times. This still leaves chances for error on the table, and it still leaves us with wasted lines that make the script longer. In general, when you want to perform some function or process multiple times on multiple items (as above where the same command is used three times on three different data frames), you need to use a for-loop or iterating function. These can reduce further unwanted redundancies by applying the function or process iteratively. Read on for more info. 10.2 For-loops A for loop is essentially a function that applies a function or given set of operations to multiple things at once, and returns an output of many items. For example, this code finds the means of every vector/column in a dataset by repeatedly applying the same code over and over to element i in the given list: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) output &lt;- vector(&quot;double&quot;, ncol(df)) # 1.Output. Create the object you want the results of the loop stored in. for (i in seq_along(df)) { # 2.Sequence of operations. &quot;For each item &#39;i&#39; along data frame&quot; output[[i]] &lt;- median(df[[i]]) # 3.Body:&quot;every individual item in &#39;output&#39; = the median of each col in df } output ## [1] 0.3771802 -0.5176346 0.4171879 0.5704655 Check out this book chapter for a great and detailed explanation of for-loops and functional coding. Although for loops are nice, they are unwieldy. R programmers typically use iterating functions instead. Examples of iterating functions are the lapply, vapply, sapply, etc. family of base R commands. But these can also be confusing and the commands are not great. The purrr package offers a better way to do iterating functions over base R; its the tidyverse way to make efficient and understandable for loops! If you have a need for a for-loop for something, see the next section instead on how to use purrr to make an iterative function. Important to understand conceptually what a for-loop is, but using them is impractical when you have purrr 10.3 purrr and Iterative Functions All notes here come from Charlotte Wickhams lecture tutorial below Part 1: https://www.youtube.com/watch?v=7UlWJWfZO9M Part 2: https://www.youtube.com/watch?v=b0ozKTUho0A&amp;t=1210s purrrs map() series of functions offer a way to apply any existing function (even functions youve made) to multiple things at once, be it lists, data frame columns, individual items in vector, etc. In short, they are for doing the same type of task repeatedly in a very quick and efficient manner. They work in much the same way as for-loops, but are far simpler to write, and can be applied in the same way to solve the same problems. How to use purrr The structure of map() commands is the same as the others in the tidyverse: #option 1 map(data, function) # option 2 data %&gt;% map(function) As a quick example and to highlight why purrr is so much more efficient and easier to use than for-loops, look at the same example from before, now using map() instead of a for: df |&gt; map_dbl(median) ## a b c d ## 0.3771802 -0.5176346 0.4171879 0.5704655 A single line is all it took to get the same results! And, it follows tidyverse grammar structure. Now lets get into how it works. map() commands work like this: For each element of x, do f. So if you pass it object x and object x is. - A vector, it will perform function f on every item in the vector - A data frame, it will perform function f on every column in the data frame - A list, it will perform function f on every level in the list Etc., etc.; the point is it applies a function repeatedly to every element in the object you supply it with. So lets walk through a case example. 10.3.1 Reproducible example: Scraping web data This is an example walk through showing how we can use purrr to speed things up dramatically and/or reduce the use of unwanted, extra code in our scripts. In this guide Ill be building a table of LPGA Tour statistics from multiple webpages. The workflow for purrr goes like this: First, you want to figure out how to do each step of your process line-by-line, for a single item. The idea is to try and walk through each step of the process and see exactly what will need to be done each each step and what the code will like, before trying to code it all at once at a higher level. Once you have each step for the first item figured out, then you make functions for each step that condense that code down to one command. Lastly, apply each function from your individual steps to all items in your list by using purr::map(). Do for One library(rvest) # STEP 1 # Figure out a line-by-line process for one item/one single web page html1=read_html(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; html_table(fill = TRUE, header=TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() head(html1) ## rank name distance ## 1 1 Emily Pedersen 282.269 ## 2 2 Nanna Koerstz Madsen 276.758 ## 3 3 Maude-Aimee Leblanc 275.393 ## 4 4 Yuka Saso 274.671 ## 5 5 A Lim Kim 274.595 ## 6 6 Madelene Sagstrom 274.488 # STEP 2 # create a custom function of the above to shorten and generalize the process quick_read_html=function(url){ web_page=read_html(url) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; # fortunately this node works for all four pages so it can be baked into the function html_table(fill = TRUE, header = TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() return(web_page) } # test to verify it works test=quick_read_html(url= &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;) head(test) # nice ## rank name putt_average ## 1 1 Jeong Eun Lee 1.698 ## 2 2 Danielle Kang 1.720 ## 3 3 Minjee Lee 1.726 ## 4 4 Lydia Ko 1.727 ## 5 5 Patty Tavatanakit 1.727 ## 6 6 Su-Hyun Oh 1.736 DO FOR ALL. Now create the object that contains all the elements you want to iterate over, and then pass it to your generalized function with map. # Step 3a # create an object that contains ALL elements of interest URLs=c(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=06&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=12&quot;) # Step 4 # use the power of map and be amazed lpga_data= URLs |&gt; map(quick_read_html) head(lpga_data) ## [[1]] ## rank name distance ## 1 1 Emily Pedersen 282.269 ## 2 2 Nanna Koerstz Madsen 276.758 ## 3 3 Maude-Aimee Leblanc 275.393 ## 4 4 Yuka Saso 274.671 ## 5 5 A Lim Kim 274.595 ## 6 6 Madelene Sagstrom 274.488 ## 7 7 Bianca Pagdanganan 274.326 ## 8 8 Maria Fassi 273.775 ## 9 9 Alana Uriell 273.575 ## 10 10 Lexi Thompson 273.463 ## 11 11 Pauline Roussin-Bouchard 273.333 ## 12 12 Patty Tavatanakit 272.773 ## 13 13 Angel Yin 272.745 ## 14 14 Yu Liu 272.717 ## 15 15 Carlota Ciganda 272.394 ## 16 16 Brooke Matthews 272.200 ## 17 17 Brooke Henderson 271.825 ## 18 18 Atthaya Thitikul 271.756 ## 19 19 Charley Hull 271.519 ## 20 20 Minjee Lee 270.938 ## 21 21 Jessica Korda 270.675 ## 22 22 Rachel Rohanna 270.649 ## 23 23 Hannah Green 270.406 ## 24 24 Perrine Delacour 270.241 ## 25 25 Janie Jackson 270.217 ## 26 26 Amanda Doherty 269.827 ## 27 27 Nelly Korda 269.200 ## 28 28 Jennifer Kupcho 269.063 ## 29 29 Yealimi Noh 268.014 ## 30 30 Stephanie Meadow 267.883 ## 31 31 Frida Kinhult 267.712 ## 32 32 Gaby Lopez 266.984 ## 33 33 Hyejin Choi 266.943 ## 34 34 Sei Young Kim 266.591 ## 35 35 Albane Valenzuela 266.550 ## 36 36 Alison Lee 265.782 ## 37 37 Gerina Mendoza 265.591 ## 38 38 Ally Ewing 265.517 ## 39 39 Jeong Eun Lee 265.340 ## 40 40 Cydney Clanton 265.237 ## 41 41 Ryann O&#39;Toole 265.218 ## 42 42 Xiyu Lin 264.962 ## 43 43 Georgia Hall 264.855 ## 44 44 Giulia Molinaro 264.569 ## 45 45 Pajaree Anannarukarn 264.451 ## 46 46 Sanna Nuutinen 264.281 ## 47 47 Nasa Hataoka 264.000 ## 48 48 Amy Yang 263.953 ## 49 49 Lilia Vu 263.806 ## 50 50 Lauren Coughlin 263.778 ## 51 51 Fatima Fernandez Cano 263.667 ## 52 52 Ruixin Liu 263.643 ## 53 53 Sophia Schubert 263.150 ## 54 54 Sarah Schmelzel 263.122 ## 55 55 Hyo Joo Kim 262.750 ## 56 56 Brittany Lincicome 261.976 ## 57 57 Lauren Stephenson 261.537 ## 58 58 Sung Hyun Park 261.477 ## 59 59 Haylee Harford 261.292 ## 60 60 Agathe Laisne 260.750 ## 61 61 Mel Reid 260.603 ## 62 62 Katherine Kirk 260.355 ## 63 63 Katherine Perry-Hamski 260.326 ## 64 64 Hinako Shibuno 260.321 ## 65 65 Annie Park 260.177 ## 66 66 Dewi Weber 260.136 ## 67 67 Ariya Jutanugarn 260.108 ## 68 68 Jaye Marie Green 260.107 ## 69 69 Jodi Ewart Shadoff 259.982 ## 70 70 Min Lee 259.946 ## 71 71 Stephanie Kyriacou 259.929 ## 72 72 Gina Kim 259.900 ## 73 73 Eun-Hee Ji 259.808 ## 74 75 Peiyun Chien 259.353 ## 75 76 Na Rin An 258.877 ## 76 77 Ruoning Yin 258.857 ## 77 78 Isi Gabsa 258.760 ## 78 79 Sophia Popov 258.250 ## 79 80 Lauren Hartlage 258.071 ## 80 81 Savannah Vilaubi 257.958 ## 81 82 Wei-Ling Hsu 257.938 ## 82 83 Kaitlyn Papp 257.611 ## 83 84 Austin Ernst 257.472 ## 84 85 Matilda Castren 257.319 ## 85 86 Allisen Corpuz 257.239 ## 86 87 Esther Henseleit 257.115 ## 87 88 Sarah Kemp 256.980 ## 88 89 Lydia Ko 256.889 ## 89 90 Linnea Johansson 256.652 ## 90 91 Pernilla Lindberg 256.638 ## 91 92 Yu-Sang Hou 256.625 ## 92 93 Amy Olson 256.509 ## 93 94 Gemma Dryburgh 256.480 ## 94 95 Casey Danielson 256.250 ## 95 96 Jennifer Song 256.212 ## 96 97 Angela Stanford 255.844 ## 97 98 Ashleigh Buhai 255.589 ## 98 99 Wichanee Meechai 255.545 ## 99 100 Morgane Metraux 255.500 ## 100 101 Elizabeth Nagel 255.429 ## 101 102 Celine Boutier 255.214 ## 102 103 Ana Belac 255.182 ## 103 104 Su-Hyun Oh 254.840 ## 104 105 In Gee Chun 254.824 ## 105 106 Brittany Lang 254.607 ## 106 107 Megan Khang 254.459 ## 107 108 Jenny Shin 254.404 ## 108 109 Jin Young Ko 254.304 ## 109 110 Cristie Kerr 254.167 ## 110 111 Paula Reto 253.984 ## 111 112 Leona Maguire 253.797 ## 112 113 Kelly Tan 253.759 ## 113 114 Jenny Coleman 253.683 ## 114 115 Jennifer Chang 253.200 ## 115 116 Hee Young Park 252.423 ## 116 117 Emma Talley 252.125 ## 117 118 Jasmine Suwannapura 251.924 ## 118 119 Pornanong Phatlum 251.880 ## 119 120 Anna Nordqvist 251.871 ## 120 121 Bronte Law 251.712 ## 121 122 Danielle Kang 251.471 ## 122 123 So Yeon Ryu 251.229 ## 123 124 Brittany Altomare 251.081 ## 124 125 Chella Choi 251.063 ## 125 126 Lauren Kim 250.643 ## 126 127 Ayaka Furue 250.554 ## 127 128 Cheyenne Knight 250.409 ## 128 129 Marina Alex 250.338 ## 129 130 Muni He 250.333 ## 130 131 Haeji Kang 249.275 ## 131 132 Robynn Ree 249.100 ## 132 133t Youngin Chun 248.500 ## 133 133t Andrea Lee 248.500 ## 134 135 Moriya Jutanugarn 248.027 ## 135 136 Maddie Szeryk 247.900 ## 136 137 Lindsey Weaver 247.724 ## 137 138 Mirim Lee 247.111 ## 138 139 Mina Harigae 246.422 ## 139 140 Allison Emrey 246.364 ## 140 141 Christina Kim 246.056 ## 141 142 Caroline Masson 245.724 ## 142 143 Jiwon Jeon 245.714 ## 143 144 In-Kyung Kim 244.688 ## 144 145 Lizette Salas 244.291 ## 145 146 Inbee Park 243.262 ## 146 147 Yae Eun Hong 243.238 ## 147 148 Caroline Inglis 243.050 ## 148 149 Na Yeon Choi 242.190 ## 149 150 Marissa Steen 241.947 ## 150 151 Stacy Lewis 240.829 ## 151 152 Charlotte Thomas 240.386 ## 152 153 Olivia Cowan 239.375 ## 153 154 Vivian Hou 238.400 ## 154 155 Aditi Ashok 235.759 ## 155 156 Dana Finkelstein 234.857 ## ## [[2]] ## rank name putt_average ## 1 1 Jeong Eun Lee 1.698 ## 2 2 Danielle Kang 1.720 ## 3 3 Minjee Lee 1.726 ## 4 4 Lydia Ko 1.727 ## 5 5 Patty Tavatanakit 1.727 ## 6 6 Su-Hyun Oh 1.736 ## 7 7 Xiyu Lin 1.738 ## 8 8 Nasa Hataoka 1.739 ## 9 9 Yuka Saso 1.741 ## 10 10 Celine Boutier 1.741 ## 11 11 Leona Maguire 1.753 ## 12 12 Amy Yang 1.755 ## 13 13 Madelene Sagstrom 1.755 ## 14 14 Jessica Korda 1.757 ## 15 15 Nanna Koerstz Madsen 1.759 ## 16 16 Carlota Ciganda 1.762 ## 17 17 Amanda Doherty 1.764 ## 18 18 Hyo Joo Kim 1.765 ## 19 19 Gemma Dryburgh 1.765 ## 20 20 Atthaya Thitikul 1.766 ## 21 21 Haeji Kang 1.767 ## 22 22 Brooke Henderson 1.768 ## 23 23 Yae Eun Hong 1.769 ## 24 24 Ruoning Yin 1.772 ## 25 25 Jin Young Ko 1.773 ## 26 26 Georgia Hall 1.775 ## 27 27 Inbee Park 1.776 ## 28 28 Angel Yin 1.781 ## 29 29 Chella Choi 1.782 ## 30 30 Lexi Thompson 1.784 ## 31 31 Lauren Stephenson 1.785 ## 32 32 Nelly Korda 1.785 ## 33 33 Perrine Delacour 1.786 ## 34 34 Sung Hyun Park 1.786 ## 35 35 Aditi Ashok 1.787 ## 36 36 Charley Hull 1.789 ## 37 37 Hannah Green 1.789 ## 38 38 Cristie Kerr 1.789 ## 39 39 Alison Lee 1.790 ## 40 40 Ryann O&#39;Toole 1.791 ## 41 41 Brittany Altomare 1.791 ## 42 42 Megan Khang 1.791 ## 43 43 Hinako Shibuno 1.791 ## 44 44 Isi Gabsa 1.791 ## 45 45 In Gee Chun 1.792 ## 46 46 Allisen Corpuz 1.793 ## 47 47 Na Rin An 1.795 ## 48 48 Jasmine Suwannapura 1.796 ## 49 49 Sarah Schmelzel 1.797 ## 50 50 Pajaree Anannarukarn 1.798 ## 51 51 Paula Reto 1.799 ## 52 52 Pauline Roussin-Bouchard 1.799 ## 53 53 Annie Park 1.799 ## 54 54 Hyejin Choi 1.800 ## 55 55 Gaby Lopez 1.800 ## 56 56 Lizette Salas 1.804 ## 57 57 Ayaka Furue 1.804 ## 58 58 Stacy Lewis 1.805 ## 59 59 In-Kyung Kim 1.805 ## 60 60 Lilia Vu 1.807 ## 61 61 Marina Alex 1.808 ## 62 62 Jenny Shin 1.808 ## 63 63 Frida Kinhult 1.808 ## 64 64 Sophia Popov 1.809 ## 65 65 Yu Liu 1.809 ## 66 66 Sei Young Kim 1.810 ## 67 67 Eun-Hee Ji 1.812 ## 68 68 Mina Harigae 1.812 ## 69 69 A Lim Kim 1.813 ## 70 70 Janie Jackson 1.813 ## 71 71 Min Lee 1.815 ## 72 72 Emma Talley 1.815 ## 73 73 Lindsey Weaver 1.817 ## 74 74 Kelly Tan 1.818 ## 75 75 Moriya Jutanugarn 1.818 ## 76 76 Ashleigh Buhai 1.819 ## 77 77 Jennifer Kupcho 1.819 ## 78 78 Katherine Kirk 1.819 ## 79 79 Linnea Johansson 1.820 ## 80 80 Cheyenne Knight 1.821 ## 81 81 Alana Uriell 1.821 ## 82 82 So Yeon Ryu 1.822 ## 83 83 Ariya Jutanugarn 1.822 ## 84 84 Stephanie Kyriacou 1.824 ## 85 85 Brittany Lincicome 1.825 ## 86 86 Stephanie Meadow 1.825 ## 87 87 Maude-Aimee Leblanc 1.826 ## 88 88 Matilda Castren 1.828 ## 89 89 Wei-Ling Hsu 1.830 ## 90 90 Esther Henseleit 1.830 ## 91 91 Sanna Nuutinen 1.830 ## 92 92 Giulia Molinaro 1.830 ## 93 93 Kaitlyn Papp 1.831 ## 94 94 Ally Ewing 1.832 ## 95 95t Cydney Clanton 1.833 ## 96 95t Bronte Law 1.833 ## 97 95t Andrea Lee 1.833 ## 98 98 Pernilla Lindberg 1.834 ## 99 99 Angela Stanford 1.834 ## 100 100 Jennifer Chang 1.835 ## 101 101 Ana Belac 1.837 ## 102 102 Agathe Laisne 1.838 ## 103 104 Amy Olson 1.838 ## 104 105 Yealimi Noh 1.838 ## 105 106 Jodi Ewart Shadoff 1.839 ## 106 107 Morgane Metraux 1.841 ## 107 108 Anna Nordqvist 1.842 ## 108 109 Jiwon Jeon 1.843 ## 109 110 Charlotte Thomas 1.845 ## 110 111 Albane Valenzuela 1.847 ## 111 112 Sophia Schubert 1.847 ## 112 113 Hee Young Park 1.847 ## 113 114 Dana Finkelstein 1.849 ## 114 115 Allison Emrey 1.851 ## 115 116 Lauren Kim 1.851 ## 116 117 Caroline Masson 1.852 ## 117 118 Muni He 1.853 ## 118 119 Youngin Chun 1.855 ## 119 120 Yu-Sang Hou 1.857 ## 120 121 Maria Fassi 1.858 ## 121 122 Bianca Pagdanganan 1.861 ## 122 123 Emily Pedersen 1.862 ## 123 124 Gerina Mendoza 1.864 ## 124 125 Katherine Perry-Hamski 1.865 ## 125 126 Jaye Marie Green 1.865 ## 126 127 Austin Ernst 1.865 ## 127 128 Dewi Weber 1.866 ## 128 129 Wichanee Meechai 1.866 ## 129 130 Fatima Fernandez Cano 1.866 ## 130 131 Sarah Kemp 1.868 ## 131 132 Jenny Coleman 1.870 ## 132 133 Brittany Lang 1.872 ## 133 134 Jennifer Song 1.873 ## 134 135 Elizabeth Nagel 1.875 ## 135 136 Na Yeon Choi 1.877 ## 136 137 Lauren Coughlin 1.878 ## 137 138 Ruixin Liu 1.878 ## 138 139 Gina Kim 1.878 ## 139 140 Mirim Lee 1.880 ## 140 141 Christina Kim 1.882 ## 141 142 Rachel Rohanna 1.883 ## 142 143 Lauren Hartlage 1.883 ## 143 144 Peiyun Chien 1.886 ## 144 145 Mel Reid 1.890 ## 145 146 Olivia Cowan 1.895 ## 146 147 Brooke Matthews 1.896 ## 147 148 Haylee Harford 1.897 ## 148 149 Pornanong Phatlum 1.903 ## 149 150 Marissa Steen 1.910 ## 150 151 Maddie Szeryk 1.914 ## 151 152 Savannah Vilaubi 1.925 ## 152 153 Robynn Ree 1.933 ## 153 154 Casey Danielson 1.951 ## 154 155 Caroline Inglis 1.952 ## 155 156 Vivian Hou 1.972 ## ## [[3]] ## rank name greens_hit ## 1 1 Hyejin Choi 76.2 ## 2 2 Lexi Thompson 76.1 ## 3 3 Sanna Nuutinen 75.7 ## 4 4 Xiyu Lin 75.6 ## 5 5 Jennifer Kupcho 75.3 ## 6 6 Jodi Ewart Shadoff 75.1 ## 7 7t Brooke Henderson 74.8 ## 8 7t Minjee Lee 74.8 ## 9 9 Jin Young Ko 74.4 ## 10 10 Megan Khang 74.0 ## 11 11 Hannah Green 74.0 ## 12 12 Jeong Eun Lee 73.8 ## 13 13 A Lim Kim 73.7 ## 14 14 Celine Boutier 73.0 ## 15 15 Hyo Joo Kim 73.0 ## 16 16 Ally Ewing 73.0 ## 17 17 Emily Pedersen 72.9 ## 18 18 Matilda Castren 72.8 ## 19 19 Nanna Koerstz Madsen 72.7 ## 20 20 Yealimi Noh 72.7 ## 21 21 Sarah Schmelzel 72.5 ## 22 22 Nasa Hataoka 72.2 ## 23 23 Pajaree Anannarukarn 72.0 ## 24 24 Charley Hull 71.8 ## 25 25 Marina Alex 71.8 ## 26 26 Amy Yang 71.5 ## 27 27 In Gee Chun 71.5 ## 28 28 Perrine Delacour 71.5 ## 29 29 Atthaya Thitikul 71.2 ## 30 30 Chella Choi 71.0 ## 31 31 Hinako Shibuno 71.0 ## 32 32t Casey Danielson 70.8 ## 33 32t Jessica Korda 70.8 ## 34 32t Sophia Schubert 70.8 ## 35 35 Ryann O&#39;Toole 70.8 ## 36 36 Lauren Stephenson 70.8 ## 37 37 Nelly Korda 70.7 ## 38 38 Ariya Jutanugarn 70.7 ## 39 39 Lauren Coughlin 70.7 ## 40 40 Ayaka Furue 70.6 ## 41 41 Brittany Altomare 70.4 ## 42 42 Patty Tavatanakit 70.4 ## 43 43 Anna Nordqvist 70.3 ## 44 44 Danielle Kang 70.1 ## 45 45 Jenny Shin 69.7 ## 46 46 Madelene Sagstrom 69.6 ## 47 47t Maude-Aimee Leblanc 69.6 ## 48 47t Min Lee 69.6 ## 49 49 Gaby Lopez 69.6 ## 50 50t Caroline Inglis 69.4 ## 51 50t Kaitlyn Papp 69.4 ## 52 52 Ruixin Liu 69.3 ## 53 53 Jaye Marie Green 69.2 ## 54 54 Stacy Lewis 69.2 ## 55 55 Sei Young Kim 69.2 ## 56 56 Lydia Ko 69.0 ## 57 57 Peiyun Chien 69.0 ## 58 58 Austin Ernst 68.8 ## 59 59t Inbee Park 68.8 ## 60 59t Lilia Vu 68.8 ## 61 61t Andrea Lee 68.8 ## 62 61t So Yeon Ryu 68.8 ## 63 63 Carlota Ciganda 68.7 ## 64 64 Moriya Jutanugarn 68.6 ## 65 65 Cydney Clanton 68.4 ## 66 66 Kelly Tan 68.4 ## 67 67 Janie Jackson 68.4 ## 68 68 Gina Kim 68.3 ## 69 69 Jenny Coleman 68.1 ## 70 70 Katherine Perry-Hamski 68.1 ## 71 71 Wei-Ling Hsu 68.1 ## 72 72 Leona Maguire 68.0 ## 73 73 Bronte Law 67.9 ## 74 74 Ruoning Yin 67.9 ## 75 75 Yuka Saso 67.7 ## 76 76 Emma Talley 67.7 ## 77 77 Allisen Corpuz 67.6 ## 78 78 Albane Valenzuela 67.6 ## 79 79 Jennifer Song 67.5 ## 80 80 Jasmine Suwannapura 67.5 ## 81 81 Sarah Kemp 67.3 ## 82 82 Caroline Masson 67.2 ## 83 83 Haylee Harford 67.1 ## 84 84 Isi Gabsa 67.1 ## 85 85 Yu Liu 67.0 ## 86 86 Annie Park 67.0 ## 87 88 Eun-Hee Ji 66.9 ## 88 89 Paula Reto 66.8 ## 89 90t Stephanie Meadow 66.7 ## 90 90t Giulia Molinaro 66.7 ## 91 90t Elizabeth Nagel 66.7 ## 92 90t Charlotte Thomas 66.7 ## 93 90t Alana Uriell 66.7 ## 94 95 Alison Lee 66.5 ## 95 96 Brittany Lincicome 66.4 ## 96 97 Maria Fassi 66.4 ## 97 98 Mina Harigae 66.3 ## 98 99 Na Rin An 66.3 ## 99 100 Robynn Ree 66.1 ## 100 101 Olivia Cowan 66.0 ## 101 102 Bianca Pagdanganan 65.9 ## 102 103 Dewi Weber 65.9 ## 103 104 Pernilla Lindberg 65.9 ## 104 105 Lizette Salas 65.9 ## 105 106 Ashleigh Buhai 65.7 ## 106 107 Cheyenne Knight 65.7 ## 107 108 Su-Hyun Oh 65.6 ## 108 109 Esther Henseleit 65.4 ## 109 110 Wichanee Meechai 65.3 ## 110 111 Georgia Hall 65.2 ## 111 112 Rachel Rohanna 65.2 ## 112 113 Gerina Mendoza 65.2 ## 113 114 Marissa Steen 64.9 ## 114 115 Sung Hyun Park 64.9 ## 115 116 Lauren Hartlage 64.7 ## 116 117 Na Yeon Choi 64.6 ## 117 118 Frida Kinhult 64.5 ## 118 119t Haeji Kang 64.4 ## 119 119t Pornanong Phatlum 64.4 ## 120 121 In-Kyung Kim 64.2 ## 121 122 Fatima Fernandez Cano 63.7 ## 122 123t Amy Olson 63.6 ## 123 123t Pauline Roussin-Bouchard 63.6 ## 124 125t Agathe Laisne 63.5 ## 125 125t Sophia Popov 63.5 ## 126 127 Ana Belac 63.4 ## 127 128 Gemma Dryburgh 63.3 ## 128 129 Angel Yin 63.3 ## 129 130 Amanda Doherty 63.2 ## 130 131 Dana Finkelstein 63.2 ## 131 132 Stephanie Kyriacou 63.1 ## 132 133 Linnea Johansson 63.0 ## 133 134 Hee Young Park 63.0 ## 134 135t Jennifer Chang 63.0 ## 135 135t Muni He 63.0 ## 136 135t Christina Kim 63.0 ## 137 138 Mel Reid 62.8 ## 138 139 Lindsey Weaver 62.6 ## 139 140 Morgane Metraux 62.0 ## 140 141t Yae Eun Hong 61.9 ## 141 141t Brittany Lang 61.9 ## 142 143 Savannah Vilaubi 61.6 ## 143 144 Katherine Kirk 61.5 ## 144 145 Jiwon Jeon 60.7 ## 145 146 Vivian Hou 60.6 ## 146 147 Aditi Ashok 60.3 ## 147 148 Cristie Kerr 59.3 ## 148 149 Mirim Lee 59.0 ## 149 150 Brooke Matthews 58.9 ## 150 151 Lauren Kim 58.7 ## 151 152 Yu-Sang Hou 58.3 ## 152 153 Allison Emrey 55.8 ## 153 154 Youngin Chun 54.8 ## 154 155 Angela Stanford 54.5 ## 155 156 Maddie Szeryk 51.7 ## ## [[4]] ## rank name rounds score_average_actual ## 1 1 Minjee Lee 32 68.750 ## 2 2 Jin Young Ko 28 69.571 ## 3 3 Lexi Thompson 27 69.630 ## 4 4 Hyo Joo Kim 22 69.682 ## 5 5 Xiyu Lin 39 69.744 ## 6 6 Nanna Koerstz Madsen 33 69.848 ## 7 7 Hyejin Choi 35 69.857 ## 8 8 Atthaya Thitikul 43 69.884 ## 9 9 Brooke Henderson 32 69.938 ## 10 10 Celine Boutier 43 69.977 ## 11 11 Lydia Ko 36 70.083 ## 12 12 Hannah Green 32 70.125 ## 13 13 Nelly Korda 15 70.133 ## 14 14 Danielle Kang 34 70.206 ## 15 15 Amy Yang 32 70.250 ## 16 16 Nasa Hataoka 41 70.268 ## 17 17 Patty Tavatanakit 33 70.303 ## 18 18 Madelene Sagstrom 41 70.317 ## 19 19 In Gee Chun 37 70.486 ## 20 20 Megan Khang 37 70.541 ## 21 21 Inbee Park 31 70.581 ## 22 22 Chella Choi 32 70.625 ## 23 23 Leona Maguire 37 70.676 ## 24 24 Jeong Eun Lee 25 70.680 ## 25 25 Yuka Saso 38 70.684 ## 26 26 Sarah Schmelzel 37 70.730 ## 27 27 Pajaree Anannarukarn 41 70.756 ## 28 28 Ryann O&#39;Toole 39 70.795 ## 29 29 Jennifer Kupcho 40 70.825 ## 30 30 Marina Alex 37 70.838 ## 31 31 Charley Hull 26 70.923 ## 32 32 Sanna Nuutinen 16 70.938 ## 33 33 Brittany Altomare 37 71.054 ## 34 34 Andrea Lee 16 71.063 ## 35 35 Alison Lee 39 71.077 ## 36 36 A Lim Kim 37 71.081 ## 37 37 Hinako Shibuno 28 71.143 ## 38 38 Jessica Korda 20 71.200 ## 39 39 Gemma Dryburgh 25 71.240 ## 40 40 Allisen Corpuz 23 71.261 ## 41 41 Perrine Delacour 29 71.276 ## 42 42 Stacy Lewis 35 71.286 ## 43 43 Georgia Hall 31 71.290 ## 44 44 Ayaka Furue 37 71.297 ## 45 45 Jasmine Suwannapura 33 71.303 ## 46 46 Cheyenne Knight 33 71.333 ## 47 47t Mina Harigae 32 71.344 ## 48 47t Gaby Lopez 32 71.344 ## 49 49 Sei Young Kim 22 71.364 ## 50 50 Carlota Ciganda 33 71.394 ## 51 51 So Yeon Ryu 24 71.458 ## 52 52 Lizette Salas 28 71.464 ## 53 53 Jenny Shin 29 71.483 ## 54 54 Lilia Vu 31 71.484 ## 55 55 Kelly Tan 29 71.517 ## 56 56 Su-Hyun Oh 25 71.520 ## 57 57 Emma Talley 32 71.531 ## 58 58 Min Lee 28 71.536 ## 59 59 Paula Reto 31 71.548 ## 60 60 Jodi Ewart Shadoff 29 71.552 ## 61 61 Eun-Hee Ji 26 71.577 ## 62 62 Lauren Stephenson 27 71.593 ## 63 63 Albane Valenzuela 30 71.633 ## 64 64 Moriya Jutanugarn 37 71.649 ## 65 65 Wei-Ling Hsu 24 71.667 ## 66 66 Ariya Jutanugarn 37 71.730 ## 67 68 Stephanie Meadow 30 71.767 ## 68 69t Anna Nordqvist 31 71.774 ## 69 69t Annie Park 31 71.774 ## 70 71 Ally Ewing 30 71.800 ## 71 72 Bronte Law 26 71.808 ## 72 73 Aditi Ashok 29 71.828 ## 73 74 Yealimi Noh 35 71.829 ## 74 75 Emily Pedersen 26 71.846 ## 75 76 Na Rin An 29 71.862 ## 76 77t Matilda Castren 36 71.889 ## 77 77t Kaitlyn Papp 18 71.889 ## 78 79 Alana Uriell 20 71.900 ## 79 80t Yae Eun Hong 21 71.952 ## 80 80t Ruixin Liu 21 71.952 ## 81 82t Frida Kinhult 26 72.000 ## 82 82t Charlotte Thomas 22 72.000 ## 83 82t Ruoning Yin 14 72.000 ## 84 85 Yu Liu 30 72.033 ## 85 86 Isi Gabsa 25 72.040 ## 86 87 Lindsey Weaver 29 72.103 ## 87 88t Maude-Aimee Leblanc 28 72.107 ## 88 88t Angel Yin 28 72.107 ## 89 90 Ashleigh Buhai 28 72.143 ## 90 91 Jennifer Chang 15 72.200 ## 91 92 Pauline Roussin-Bouchard 27 72.222 ## 92 93 Sophia Schubert 20 72.300 ## 93 94t Caroline Masson 29 72.310 ## 94 94t Giulia Molinaro 29 72.310 ## 95 96 Brittany Lincicome 21 72.381 ## 96 97 Amanda Doherty 26 72.385 ## 97 98 Austin Ernst 18 72.444 ## 98 99 Dana Finkelstein 21 72.476 ## 99 100 Pernilla Lindberg 29 72.483 ## 100 101t Ana Belac 22 72.500 ## 101 101t Haeji Kang 20 72.500 ## 102 101t In-Kyung Kim 16 72.500 ## 103 104 Jaye Marie Green 28 72.571 ## 104 105 Sarah Kemp 25 72.600 ## 105 106 Wichanee Meechai 33 72.667 ## 106 107 Esther Henseleit 26 72.692 ## 107 108 Katherine Perry-Hamski 23 72.696 ## 108 109t Maria Fassi 20 72.700 ## 109 109t Robynn Ree 10 72.700 ## 110 111 Sophia Popov 28 72.714 ## 111 112 Lauren Coughlin 18 72.722 ## 112 113 Dewi Weber 22 72.727 ## 113 114 Janie Jackson 23 72.739 ## 114 115 Jennifer Song 26 72.769 ## 115 116 Amy Olson 27 72.778 ## 116 117 Bianca Pagdanganan 23 72.783 ## 117 118 Agathe Laisne 14 72.857 ## 118 119 Katherine Kirk 16 72.875 ## 119 120 Cydney Clanton 19 72.947 ## 120 121 Sung Hyun Park 22 73.045 ## 121 122 Stephanie Kyriacou 14 73.071 ## 122 123 Gerina Mendoza 22 73.091 ## 123 124 Caroline Inglis 10 73.100 ## 124 125 Peiyun Chien 17 73.118 ## 125 126 Pornanong Phatlum 25 73.120 ## 126 127 Jenny Coleman 30 73.133 ## 127 128 Rachel Rohanna 19 73.158 ## 128 129 Muni He 18 73.167 ## 129 130 Linnea Johansson 23 73.261 ## 130 131 Gina Kim 10 73.300 ## 131 132 Christina Kim 18 73.333 ## 132 133 Hee Young Park 26 73.346 ## 133 134 Mel Reid 29 73.379 ## 134 135 Haylee Harford 12 73.417 ## 135 136 Marissa Steen 19 73.421 ## 136 137 Casey Danielson 16 73.438 ## 137 138t Cristie Kerr 12 73.500 ## 138 138t Brittany Lang 14 73.500 ## 139 140 Brooke Matthews 10 73.600 ## 140 141 Fatima Fernandez Cano 15 73.733 ## 141 142 Jiwon Jeon 14 73.786 ## 142 143 Morgane Metraux 18 73.889 ## 143 144 Angela Stanford 16 74.063 ## 144 145 Elizabeth Nagel 14 74.071 ## 145 146 Olivia Cowan 8 74.125 ## 146 147 Na Yeon Choi 21 74.190 ## 147 148 Yu-Sang Hou 12 74.250 ## 148 149 Lauren Kim 14 74.429 ## 149 150 Allison Emrey 22 74.500 ## 150 151 Lauren Hartlage 14 74.571 ## 151 152 Savannah Vilaubi 12 75.000 ## 152 153 Maddie Szeryk 10 75.200 ## 153 154 Mirim Lee 18 75.278 ## 154 155 Youngin Chun 7 75.429 ## 155 156 Vivian Hou 10 76.200 All done!! And just like that, weve downloaded four different web pages, extracted the tabled info, and formatted them without copying and pasting any code. The same process for all four was only used one time to write the initial function. Just apply some final formatting to clean it up a bit and combine the separate data frames into a single, unified one. lpga_data= lpga_data %&gt;% reduce(left_join, by=&quot;name&quot;) %&gt;% # Combine all list levels into a single tibble, matching by the &quot;Name&quot; column select(-contains(&quot;rank.&quot;)) |&gt; rename(&quot;score_average&quot;=&quot;score_average_actual&quot;) # VOILA! head(lpga_data) ## name distance putt_average greens_hit rounds score_average ## 1 Emily Pedersen 282.269 1.862 72.9 26 71.846 ## 2 Nanna Koerstz Madsen 276.758 1.759 72.7 33 69.848 ## 3 Maude-Aimee Leblanc 275.393 1.826 69.6 28 72.107 ## 4 Yuka Saso 274.671 1.741 67.7 38 70.684 ## 5 A Lim Kim 274.595 1.813 73.7 37 71.081 ## 6 Madelene Sagstrom 274.488 1.755 69.6 41 70.317 10.3.2 Non-reproducible example (Juvenile Life Without Parole study) In the Juvenile Lifers study, there were a series of questions that participants rated on a scale of 0-100 in terms of difficulty. Part of our analysis involved taking the ratings on those variables and giving them relative rankings, so that each of the 6 variables in the series was rated from the least to most difficult, by participant. Now if we only needed to compute these rankings once this wouldnt have been any big deal; however, we needed to do it three times. Much of the same code and the same process would need to be copied and pasted, resulting in a very long, messy, harder to read script. With purrr however, we can reduce the redundancies to a minimum, saving time and reducing the chances of mistakes. Step 1. Just like before, the first step is to find a line-by-line solution for a single item, and then to generalize this into a shortcut function that can be applied to the any item i in a series of items. For the sake of brevity, Im going to skip most of that and just include the functions below. load(&quot;C:/Github Repos/Studies/JLWOP/Data and Models/jlwop_reentry_survey.RData&quot;) #### CREATE THE DATA SETS WE NEED#### na_blank=jlwop_reentry_survey # analysis 1 keeps the data as-is na_zero=jlwop_reentry_survey %&gt;% # supplementary analysis replaces the NA&#39;s with 0 mutate(across(c(barrier_housing:barrier_identification), replace_na,0)) rm(jlwop_reentry_survey) # remove old data set to avoid confusion #### Functions #### # transformation function to wrangle the data into proper formatting rotate_data=function(data, variable_prefix){ data=data %&gt;% pivot_longer( cols= starts_with(variable_prefix), # collect all the desired variables (i.e., columns).... names_to = &quot;variable&quot;, #...and put them into a new categorical variable called &quot;variable&quot; values_to = &quot;participant_score&quot;) %&gt;% # ...and store their values in a new variable called &quot;participant_score&quot; arrange(unique,participant_score) %&gt;% select(c(unique, participant_score, variable)) %&gt;% # keep only these 3 variables relocate(variable, .before = participant_score) # put the newly created variable up front return(data) } # creating the rankings for each variable; then transform data back to original structure rank_and_unpivot=function(data){ data=data %&gt;% group_by(unique) %&gt;% # group the scores so they can be ranked by participant mutate(rank1=dense_rank(participant_score), # create ranking variable rank=max(rank1,na.rm = TRUE) + 1 - rank1) %&gt;% # fix ranks by flipping to ascending order mutate(rank=factor(rank)) %&gt;% # convert rank to factor structure select(-rank1) # Pivot back to wide data=data %&gt;% pivot_wider(names_from = variable, values_from = rank:participant_score) %&gt;% ungroup() # un-group the data and delete the generated names return(data) } Step 2. Again, like before, we want to combine all elements of interest into some object. Once we have that, we then pass said object to map() and supply the map call with our custom function. dfs=list(na_blank=na_blank, na_zero=na_zero) %&gt;% # create lists map(.f=rotate_data, variable_prefix = &quot;barrier&quot;) %&gt;% # apply custom function along whole list map(rank_and_unpivot) # again!! DO IT AGAIN! With another function this time. # extract list elements to make them data frames again list2env(dfs, globalenv()) rm(dfs) #discard list. It has fulfilled its purpose. And just like that, were done! 10.3.3 Example 3: Read/Import several files at once with map() Multiple ways you can do this. ################### Option 1: read all into the global environment, keeping them as SEPERATE df&#39;s ############### legaldmlab::read_all(path=&quot;Data Repository/Stats Data Repository/JASP files&quot;, extension = &quot;.csv&quot;) # Option 1.A: Squish ALL OBJECTS in the working environment into a list # Again, note the &quot;ALL OBJECTS&quot; part; make sure there are no functions or other things in the environment when you run this. files=mget(ls()) ############# Option 2: Read in all files as a LIST of df&#39;s, then stitch in the names ##################################### files=paste0(here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;, &quot;/&quot;), list.files(path=here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;), pattern = &quot;.csv&quot;)) files_list=files |&gt; map(readr::read_csv) names(files_list)=file.path(here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;)) |&gt; #specify file path as a string list.files(pattern = &quot;.csv&quot;) |&gt; # pass the path string to list files; search in this location for files with this extension gsub(pattern=&quot;.csv&quot;, replacement = &quot;&quot;) # remove this pattern to save only the name # Option 2.A: Extract each data frame and put everything into the global environment list2env(cog_data, globalenv()) 10.4 Other purrr commands Note that map() always returns a list, and depending on the output that you want, you may need to use a variation of map(). These variations are as follows: Command Return map_lgl() logical vector map_int() integer vector map_dbl() double vector map_chr() character vector walk() only returns the side effects of a function 10.4.1 walk and walk2 Walk() is useful for when you just want to plot something or write a save file to your disk, etc. It does not give you any return to store something in the environment. You use it to write/read files, open graphics windows, and so on. Example: Writing multiple files at once Utilize purrr::walk2() to apply a function iteratively on TWO objects simultaneously. To save multiple .csv files with walk2, we need two distinct lists: 1. A list of data frames that we wish to export, 2. and the file paths, complete with the file names and extensions, for each file to be written. First create and define both list items. Then apply walk2() to pluck an element from list 1 and its corresponding element from list 2, and apply the write_csv function in for-loop fashion. # DEMO 1: Writing multiple plots at once # Create list one, the list of objects figs = list(scatter_plot=scatter_plot, multi_plot=multi_plot) # create list 2, the list of file names fig_names = figs |&gt; names() |&gt; map(paste0, &quot;.png&quot;) # pass both to purrr to use ggsave iteratively over both lists once and save all graphs with one command walk2(figs, fig_names, ~ggsave(plot = figs, filename = fig_names path=here::here(&quot;Figures and Tables&quot;), device = &quot;png&quot;, dpi = 300)) A second demo, this time using write.csv to save/export multiple CSV files at once # DEMO 2: Wrinting multiple .csv files at once ### Custom function #### # Create needed function that grabs file names and stitches them together with the correct path and extension # Included in legaldmlab package bundle_paths=function(df_list, output_location, file_type){ names=names(df_list) paths=rep(here::here(output_location), length(names)) extension=rep(c(file_type), length(names)) fixed_names=paste0(&quot;/&quot;,names) path_bundle=list(paths,fixed_names, extension) %&gt;% pmap(., paste0) return(path_bundle) } #### Exporting the .csv files for SPSS/JASP/etc. #### # Define list 1 dfs=list(na_blank=na_blank, na_zero=na_zero, na_zero_helpreint=na_zero_helpreint) # list 2 paths_csv=bundle_paths(df_list = dfs, folder_location = &quot;JLWOP/Data and Models&quot;, file_type = &quot;.sav&quot;) # Iterate over all elements in list 1 and corresponding element in list 2; # and apply the the write_csv function to each walk2(.x=dfs, .y= paths, .f=haven::write_sav) #### .RData file for R users #### # Combine multiple data frames into a single .RData file and export save(list = c(&quot;na_blank&quot;, &quot;na_zero&quot;, &quot;na_zero_helpreint&quot;), file = here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;,&quot;ranking_data.RData&quot;)) 10.4.2 map2 knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_a.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_b.png&quot;)) 10.4.3 pmap for when you have a bunch of shit This function is for iterating over three or more elements. As soon as you have &gt;2 items you have to iterate over, you need pmap(), which acts on a list object called .i instead of a list object. The list .i is a list of all the objects you want to iterate over. If you give it a list of 18 items, it iterates over all 18. If the list only has two things, it only acts on those two. She says its easiest to imagine the list as a data frame, and the columns of the data frame like the elements of that list. knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap_2.png&quot;)) 10.5 Using purrr to manage many models Below is the full script I copied from Hadley Wickhams lecture, which you can watch here pacman::p_load(dplyr,purrr,tidyverse,gapminder) #### Workflow for managing many models in R #### # 1. Nest data with {tidyr} # 2. Use {purrr} to map a modeling function # 3. Use {broom} to inspect your tidy data gapminder=gapminder %&gt;% mutate(year1950= year-1950) #the number of years it&#39;s been since 1950 #-------------------------------------------------------------------------------------------- #### Step 1. Nest the data. #### # A nested data frame has one column per country. You&#39;re essentially # creating a Russian doll; a data frame inside of a larger data frame. by_country=gapminder %&gt;% group_by(continent,country) %&gt;% # variables to keep at the top level nest() # smush everything else into a df, and store this mini-df in its own column # with this, you can have an entire table per row; a whole data frame for each country # Essentially condensing a list into a table by_country$data[[1]] #-------------------------------------------------------------------------------------------- #### Step 2. Use purrr to map stuff. #### # 12:50 country_model=function(df){ lm(lifeExp ~ year1950, data = df) } models= by_country %&gt;% mutate( mod=map(data,country_model) ) gapminder %&gt;% group_by(continent,country) %&gt;% nest() %&gt;% mutate( mod= data %&gt;% map(country_model) ) # 27:11 #-------------------------------------------------------------------------------------------- ##### Step 3. #### # This creates another nested df inside of your main data frame that has the summary stats of each model models=models %&gt;% mutate( tidy=map(mod, broom::tidy), # tidy() gives model estimates glance=map(mod,broom::glance), # glance() gives model summaries augment=map(mod,broom::augment) # model coefficients ) # What can you do with this nest of data frames? # The reverse of step 1; un-nest it to unpack everything! # 34:40 # Keeps a massive list of related information neatly organized! unnest(models,data) # back to where we started unnest(models,glance, .drop = TRUE) unnest(models,tidy) and here is a version I made of the above to manage many Latent Growth Curve models. # CONDENSED MASTER TABLE VERSION ----------------------------------------------------------------------------- # Models table that has all models condensed models_noCovs=tibble( #### Define model names #### model_name=c(&quot;Linear&quot;, &quot;Quadratic&quot;, &quot;Latent_Basis&quot;), ##### List model specifications for lavaan #### model_spec=list( linear_model= &#39; # intercept and slope with fixed coefficients i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 + 1*panss_total_3400 + 1*panss_total_5200 s =~ 0*panss_total_400 + 3*panss_total_1000 + 6*panss_total_1600 + 9*panss_total_2200 + 12*panss_total_2800 + 15*panss_total_3400 + 24*panss_total_5200 &#39;, quadratic_model= &#39; # intercept and slope with fixed coefficients i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 + 1*panss_total_3400 s =~ 0*panss_total_400 + 3*panss_total_1000 + 6*panss_total_1600 + 9*panss_total_2200 + 12*panss_total_2800 + 15*panss_total_3400 + 24*panss_total_5200 qs =~ 0*panss_total_400 + 9*panss_total_1000 + 36*panss_total_1600 + 81*panss_total_2200 + 144*panss_total_2800 + 225*panss_total_3400 + 576*panss_total_5200 &#39;, latentBasis_model= &#39; # intercept and slope with fixed coefficients i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 + 1*panss_total_3400 + 1*panss_total_5200 s =~ 0*panss_total_400 + NA*panss_total_1000 + NA*panss_total_1600 + NA*panss_total_2200 + NA*panss_total_2800 + NA*panss_total_3400 + 1*panss_total_5200 &#39; ), #### Fit all models at once with purrr #### fitted_model=model_spec |&gt; map(lavaan::growth, data=panss_sem_data, missing=&quot;FIML&quot;), ) #### Add parameter estimates and fit stats #### models_noCovs=models_noCovs |&gt; mutate(tidy_parameters=map(fitted_model, tidy, conf.int=.95), #parameter estimates global_fit=map(fitted_model, performance::model_performance)) #global fit of models #### Clean up stuff #### models_noCovs$tidy_parameters=models_noCovs$tidy_parameters |&gt; map(select,-c(std.lv:std.nox, op)) |&gt; # remove extra columns map(mutate, estimate=round(estimate, digits = 2)) |&gt; # round numbers map(mutate, (across(c(std.error:p.value, conf.low, conf.high), round, 3))) models_noCovs$global_fit=models_noCovs$global_fit |&gt; map(select, c(Chi2:p_Chi2, RMSEA:SRMR, CFI, AIC)) Note how the functions inside map take on a slightly different form, but work the same. Using this framework, you can easily drill down into any column and be sure that youre accessing the right thing. Everything is always kept together, and always acted upon in the same way. This minimizes mistakes. models_noCovs$tidy_parameters$latent_Basis_model "],["intro-to-r-markdown.html", "Chapter 11 Intro to R Markdown 11.1 Important code chunk options 11.2 Writing math equations and symbols 11.3 Including graphics/inserting pictures 11.4 Footnotes 11.5 Change the color of your text 11.6 Re-using code chunk options 11.7 Making better tables 11.8 Running in-line code", " Chapter 11 Intro to R Markdown R Markdown is a better and more organized way to write scripts. Seriously, once you learn it, theres no going back. New and dont know where to start? Read The R Markdown Cookbook. Amazing overview with tons of neat tricks and how-tos. This other source may also be of some help. Below are some quick tips for common tasks; but be sure to read the Cookbook above. 11.1 Important code chunk options cache: TRUE or FALSE. Do you want to save the output of the chunk so it doesnt have to run next time? Creates a cached folder in the directory. eval: Do you want to evaluate (i.e., run) the code in the chunk? echo: Do you want to print the code after its run? include: Do you want to include code output in the final output document? Setting to FALSE means the code does not appear in the output document, but it is still run. 11.2 Writing math equations and symbols 11.2.1 Greek symbols A few notes first: Math notation is done with dollar signs and forward slashes For Greek letters, just type the name of the letter: $\\mu$ for \\(\\mu\\) $\\sigma$ for \\(\\sigma\\) $\\alpha$ for \\(\\alpha\\) $\\pi$ for \\(\\pi\\) $\\rho$ for \\(\\rho\\) 11.2.2 Math notation $\\pm$ for ± $\\ge$ for  $\\le$ for  $\\neq$ for  11.2.3 Statistics notation 11.2.4 Writing in-line code Use the funny looking symbol on the tilde key that looks like this: ` To write in line, code, put one of those symbols on either side of the code, like you would with quotation marks. Helps you write lines like: I love dplyr 11.3 Including graphics/inserting pictures The default method doesnt work for me for some reason, but you can still insert images using a combination of the here package and knitr. Use the include_graphics() command and specify both the file location and its name: knitr::include_graphics(here::here(&quot;pics&quot;,&quot;snapchat.png&quot;)) NOTE. Use 300-600 DPI to get good looking pictures. The bookdown book notes that: The syntax for controlling the image attributes is the same as when images are generated from R code. Chunk options fig.cap, out.width, and fig.show still have the same meanings. and: You can easily scale these images proportionally using the same ratio. This can be done via the dpi argument (dots per inch), which takes the value from the chunk option dpi by default If it is a numeric value and the chunk option out.width is not set, the output width of an image will be its actual width (in pixels) divided by dpi , and the unit will be inches. For example, for an image with the size 672 x 480, its output width will be 7 inches ( 7in ) when dpi=96. This feature requires the package png and/or jpeg to be installed. You can always override the automatic calculation of width in inches by providing a non-NULL value to the chunk option out.width , or use include_graphics(dpi = NA) 11.4 Footnotes To add a footnote, use the ^ symbol and put the note in brackets: You can also write footnotes1 like this. 11.5 Change the color of your text YOUR TEXT HERE 11.6 Re-using code chunk options https://yihui.org/en/2021/05/knitr-reuse/ 11.7 Making better tables https://rfortherestofus.com/2019/11/how-to-make-beautiful-tables-in-r/ 11.8 Running in-line code To run code in the middle of a sentence, you create a mini code chunk inside the sentence. For example: &gt; There are 2x2 apples in the basket Could be typed as There are 4 apples in the basket "],["statistics-and-psych-specific-stuff.html", "Chapter 12 Statistics and Psych-specific Stuff 12.1 Create or sample from a distribution 12.2 Calculating Interrater reliability 12.3 Statistical tests and modeling with easystats", " Chapter 12 Statistics and Psych-specific Stuff 12.1 Create or sample from a distribution Creating a binomial distribution When you do this, you are setting the true population parameter; you are in control of the Data Generating Process and the true distribution In a binomial distribution, the parameter is normally distributed, and can take any value from 0.0 to 1.0 But the data that this process generates is not normal rbinom(n= 1000, size= 1, prob = 0.5) ## [1] 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 0 0 0 0 ## [86] 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 ## [171] 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 ## [256] 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 ## [341] 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 ## [426] 0 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 1 1 ## [511] 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0 ## [596] 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 ## [681] 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 ## [766] 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 ## [851] 1 1 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 0 0 1 1 0 1 1 1 0 1 1 0 ## [936] 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 rnorm(n=2500,mean=500, sd=100) ## [1] 690.7642 503.0016 446.5626 484.7566 514.2144 558.1073 372.0421 486.2422 445.8801 507.1753 582.8456 556.2646 505.3560 457.7520 552.2014 443.2911 516.2216 507.5774 ## [19] 496.2267 700.0870 486.4038 544.2314 610.0555 490.9881 564.8317 588.5202 438.1985 365.3048 467.5639 521.1525 617.3123 554.5895 489.1160 512.8746 711.0196 692.8460 ## [37] 519.8707 324.4048 441.7998 534.3156 490.2324 585.4221 430.4960 460.9629 703.4337 510.0744 519.9681 468.5821 527.3318 413.0651 460.6879 664.7801 354.3382 370.0553 ## [55] 489.8274 326.0638 343.5181 669.8929 410.4743 259.5956 581.9515 479.6523 434.0253 461.1315 584.3208 423.5085 556.7683 466.5056 433.8107 588.9122 358.9104 296.3319 ## [73] 490.2643 374.4722 508.8257 550.2508 413.3520 491.6750 450.5777 637.9321 418.8566 530.1711 558.6032 531.9450 422.4871 509.5001 436.4969 609.5397 390.5833 573.7023 ## [91] 332.9137 355.7153 459.6057 416.6499 435.9233 393.1971 450.7034 370.8988 325.2500 560.9520 538.7495 596.3238 456.8505 378.8018 389.1940 546.0309 464.9669 437.3562 ## [109] 595.2106 358.9524 488.1387 577.6982 451.5795 555.9834 455.0614 424.9773 512.1967 333.4369 349.7942 497.6879 461.4079 579.9255 570.4377 496.8774 468.6098 553.1056 ## [127] 693.8511 668.4453 418.9909 571.5872 626.5654 682.8749 381.1959 380.5799 529.1706 495.0200 602.4397 488.4812 481.2003 480.3037 517.4619 430.3098 410.7283 383.9239 ## [145] 507.2224 677.0574 551.9872 743.8753 447.8830 553.9661 660.0133 426.8833 378.0482 586.2658 608.6438 414.7819 467.3578 443.9643 505.8476 582.4859 515.8577 579.6892 ## [163] 452.8202 320.2918 429.6083 496.7879 444.7553 546.7475 461.6923 495.1302 305.6665 481.9163 570.0053 411.7289 659.6989 319.8401 577.8611 325.9026 545.4157 460.2025 ## [181] 534.0228 406.5574 532.8106 611.0717 470.5804 470.7250 399.0586 376.7103 432.2565 510.2988 501.0330 311.5054 613.5306 491.6324 592.4899 497.2093 253.9098 540.8763 ## [199] 524.1655 570.1845 568.8148 649.2078 357.9979 505.1117 560.0656 488.2058 512.4326 429.8364 369.7317 562.7482 380.4301 691.9237 552.0891 642.8095 381.2714 610.1536 ## [217] 393.5623 353.8832 557.3873 478.6679 677.2078 493.7744 381.9982 534.3512 519.5267 354.7870 427.7809 492.3991 527.7539 236.7285 634.5613 611.5118 573.7486 494.7876 ## [235] 447.2701 450.1239 447.2279 528.8756 495.6735 432.1601 480.7788 407.7178 519.6089 533.9601 478.7551 535.1155 502.5280 342.3943 446.9967 523.1670 562.8475 462.3585 ## [253] 525.2291 523.7450 532.5428 435.9035 458.1338 661.8657 463.5148 520.1895 497.6584 427.7127 590.0779 563.6812 527.4667 580.2521 450.2358 609.7054 687.3053 315.5045 ## [271] 381.4677 354.9862 621.2616 445.6282 493.3062 557.0388 535.6489 391.4441 517.1641 643.9891 462.9767 456.7823 594.6212 679.8447 469.5304 585.8058 495.7101 515.8654 ## [289] 585.2954 475.4714 245.2662 573.7207 571.2908 510.1705 489.8945 427.6127 381.9606 549.0644 731.1598 624.7910 351.4736 516.0527 502.4989 575.3473 508.8300 598.5068 ## [307] 626.5723 394.5084 551.1109 481.5339 602.6052 490.7534 524.8933 594.8019 511.1983 451.8964 493.0614 628.4617 477.1235 576.7203 469.4596 528.6711 536.4981 466.4876 ## [325] 529.7832 514.9179 594.0735 475.0174 379.3909 551.3659 416.0800 653.6031 349.7248 498.9691 603.6682 541.4988 497.6942 419.0618 461.9743 433.0824 291.6662 454.9965 ## [343] 429.0149 266.4133 580.8652 417.8723 625.2045 507.4710 391.1100 654.7852 627.8170 398.9321 384.7853 501.1054 675.6450 476.3180 440.5276 509.3056 599.2357 592.1269 ## [361] 651.7668 524.3816 421.1368 452.8244 529.8621 644.1803 478.4175 508.7750 595.8007 558.5413 571.2349 555.1023 559.2593 489.7140 493.0326 669.2377 469.0042 347.3663 ## [379] 475.9720 421.2802 645.9101 453.0382 541.1887 411.5055 666.8346 494.7872 397.1767 497.7115 486.7867 534.5902 439.6220 480.8484 554.9179 368.8868 460.8274 489.0922 ## [397] 711.1718 501.9165 418.5724 718.6710 423.1709 435.8882 532.8551 649.3885 451.9812 587.7714 516.2318 511.8405 380.5151 573.8564 462.3429 510.8129 275.7200 535.5461 ## [415] 623.0225 456.8365 490.8698 594.5922 653.8893 516.6514 332.8932 612.6280 524.1005 538.2043 487.1793 524.3440 497.0323 502.0975 587.9191 448.9578 476.7128 602.9146 ## [433] 469.7850 376.6824 499.8641 650.7851 615.8094 493.3007 570.9333 608.2727 559.6828 587.6076 477.4335 535.3579 459.4781 448.0090 531.2777 522.1706 543.7536 606.8745 ## [451] 463.3416 491.5422 684.6416 437.6934 476.8128 499.2137 527.9584 599.6644 396.7951 477.0358 532.5132 478.9701 610.1375 570.7250 458.6793 490.3939 492.9727 548.0784 ## [469] 491.5192 534.6258 592.4548 429.9015 622.2405 514.6087 482.5179 631.1590 548.9731 476.7655 550.6699 458.7896 500.6046 489.9678 521.7070 376.1594 455.4748 478.8103 ## [487] 500.6990 450.0641 705.1408 493.3444 340.2326 450.3086 565.5474 510.5845 607.4647 525.2464 601.9312 393.1189 581.0030 537.7657 453.3838 318.0283 508.4975 531.2463 ## [505] 373.1126 548.7601 623.4519 440.0869 652.5000 437.5633 506.5816 415.4071 516.0706 544.6454 487.4728 544.8934 566.9092 617.6771 548.2616 631.0511 473.4739 571.3268 ## [523] 368.3856 501.6164 522.8763 629.5952 457.4299 527.2467 326.8235 513.6585 420.4447 447.2285 441.3478 517.2391 292.0001 494.4801 527.8838 542.8061 566.1641 578.9908 ## [541] 357.2008 473.6306 677.2587 535.5803 301.3727 403.7398 414.6190 625.7101 433.4926 513.9790 290.1241 249.8849 590.1190 487.6803 571.0442 601.2068 469.5465 475.7392 ## [559] 432.6426 496.2470 418.4045 303.4358 726.0839 609.2849 500.5518 578.1233 408.7615 380.4873 594.1007 499.7439 531.1113 368.6508 455.1496 609.9466 458.9388 604.4078 ## [577] 402.9088 415.8239 448.0483 462.1295 522.6474 487.6938 456.0863 577.8719 340.5999 633.7612 443.4922 454.3063 610.4940 435.0683 599.6636 397.2973 374.6455 325.8588 ## [595] 321.3024 482.7620 374.4408 425.1393 510.4970 531.1596 645.9354 448.5209 531.6261 348.1992 476.0654 562.8976 379.9974 532.0484 445.4427 634.4011 368.7167 672.7004 ## [613] 565.7535 491.0597 731.4451 604.8716 421.3364 300.1127 440.5535 527.3207 420.5020 543.1092 499.6028 422.6449 742.7753 436.8232 530.8857 546.1302 647.6666 448.8163 ## [631] 366.9691 396.1402 562.1381 741.5654 426.0017 551.1103 478.2167 600.2266 532.1149 436.3339 660.6611 354.6999 488.2334 632.6080 390.4946 389.2055 641.5392 502.8989 ## [649] 586.2955 447.9865 349.9972 500.9418 283.4726 583.2403 523.0787 475.2691 539.1305 525.1959 503.8490 297.1299 485.4389 480.3741 525.1787 659.0466 519.6565 513.0671 ## [667] 360.0704 637.1515 586.5342 580.8653 435.4805 624.2765 464.9148 409.2928 565.2380 556.4711 392.7148 588.4800 616.1251 523.3135 495.8529 359.6408 597.8614 557.3185 ## [685] 547.4084 554.3561 469.9123 471.6370 552.7903 457.7323 509.1782 484.3411 310.7837 349.0889 469.9034 736.6610 482.3783 423.4637 607.1870 525.9129 456.5308 358.0275 ## [703] 605.7850 646.5580 477.9809 377.2324 403.8891 601.9674 613.8450 408.5394 510.2801 372.3809 693.2728 609.3073 704.5412 441.4024 623.5082 511.3888 539.7074 465.6184 ## [721] 663.6630 699.4078 444.3459 485.8474 463.9139 436.9269 583.1504 435.3909 584.0109 687.2442 626.6060 504.6432 497.2372 306.4394 430.5330 566.2029 441.3459 484.8510 ## [739] 524.0242 440.1531 554.9754 572.8674 558.5630 621.3363 635.4167 549.3009 548.0824 469.9470 728.0815 691.3732 650.7351 311.2510 528.0012 513.0207 443.2585 495.4862 ## [757] 494.3729 577.6868 511.6519 387.7507 500.9570 528.0799 392.6372 575.0573 649.4153 424.9899 445.2550 589.0665 456.6779 417.0215 494.1086 466.9487 507.0484 617.9516 ## [775] 408.3503 486.6273 519.3648 367.0132 603.6742 357.5127 459.2164 414.9186 477.9580 375.7157 227.5575 447.0915 552.5945 541.2205 477.3557 511.3888 366.3676 541.8846 ## [793] 641.1269 493.1544 444.6010 571.4874 507.8334 376.4408 729.4874 607.8687 453.6202 377.1565 284.5644 514.1725 582.8606 423.0604 485.5145 430.1117 616.0109 367.8656 ## [811] 393.5513 450.6351 475.9432 513.4465 620.5404 560.5519 450.3671 520.9705 493.8261 507.4381 665.0495 325.3849 508.1991 343.4316 380.3415 424.6257 606.1888 579.1084 ## [829] 462.3792 389.8099 490.8874 591.3056 465.1908 624.4178 423.0522 253.6473 462.7211 452.0061 365.4780 334.9249 430.0378 310.6141 587.8560 435.5059 352.7550 366.2667 ## [847] 496.9301 464.1797 426.3634 524.6137 575.3314 462.3060 354.1794 577.5041 288.6520 467.3059 478.7292 635.0938 467.7270 610.1464 442.0070 583.9132 363.9433 530.5671 ## [865] 549.5344 468.3479 466.9145 655.7451 512.8609 504.2036 538.2761 475.7039 458.7146 418.8669 606.1192 448.4299 387.2663 474.5578 576.6278 502.9695 509.4259 505.9550 ## [883] 557.8631 606.7899 546.4254 517.9671 456.8737 534.7139 532.2812 506.9922 437.5018 354.5535 547.5040 546.1719 496.7769 594.6566 566.8912 651.1099 569.2072 399.9619 ## [901] 499.5997 453.0542 503.6737 479.9079 603.6589 588.4268 480.5298 719.0702 476.4992 463.0849 515.4054 557.1812 480.2085 517.2196 429.5344 664.7642 565.2157 489.8956 ## [919] 502.2124 514.0178 534.4487 468.6748 477.4474 428.4724 342.0031 591.6231 361.7524 579.8610 503.2338 531.1024 378.8424 436.3192 708.5300 524.2979 451.3790 368.0187 ## [937] 343.0128 649.4367 593.2486 716.6372 566.0013 563.3512 323.0505 712.7242 283.4797 593.6304 368.4733 586.7725 402.3331 557.0522 511.9797 398.3025 623.5914 529.3349 ## [955] 515.1055 495.3027 594.1564 578.8444 386.4326 402.7863 526.0448 507.9440 334.2873 398.9222 523.8656 365.6481 488.9171 459.0526 286.3321 337.5755 613.3931 478.8902 ## [973] 514.2966 745.8776 475.3705 517.7410 544.9654 630.6918 565.4086 335.1642 531.3661 436.6194 427.9133 425.6029 557.2813 501.2163 547.3963 370.5879 490.5978 447.3090 ## [991] 512.0111 506.0593 444.2074 446.8414 401.0092 463.4028 441.7573 460.8381 494.1673 437.6966 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1500 entries ] 12.2 Calculating Interrater reliability Cohens Kappa is useful for IRR agreement on categorical variablesUse the psych package for this see here; and read this web page for an overview of what Cohens Kappa is if you need a recap/intro. For 3+ raters on a continuous variable, use Intraclass Correlation. See this page. 12.3 Statistical tests and modeling with easystats https://easystats.github.io/easystats/ 12.3.1 Getting parameter estimates from model objects Scenario: Youve run some statistical test (like the below regression), and want a summary of the model estimates. rm(iris) model &lt;- lm(Sepal.Length ~ Species, data = iris) You have a few options when it comes to getting a summary of a model and getting the coefficient estimates: - summary() - broom::tidy() - paramters::model_paramters(), or just paramters::paramters() for short Theres no reason to use summary, generally speaking, because it sucks. It doesnt give you tidy output thats easy to manipulate or extract, its hard to read, and it cant be turned into a useful table. Skip it unless you need something specific from its output (i.e., youre using lavaan) Options two and three are pretty similar and both give you most of the same information, though parameters() prints neater to the console window. Generally I find parameters preferable. Note though that neither command will round the numbers if you store it as a table in the environment. So. If you want to manipulate ANY info in the table and/or extract info, use tidy or parameters. Both make tidy tibbles. If youre using the command to export said info in a neat, presentable MS Word table or HTML table, and you do not care about extracting/modifying/manipulating anything in it, then use parameters and pipe it to format_table() Using format_table() rounds all columns to 2 decimal places, reformats p-values to APA format, and collapses CIs into a single column. Do note though that it makes every column into a Character column! So this is for exporting-use only. Heres a comparison of brooms output (first) vs. parameters (second) when you save each in the environment. As you can see, both produce tidy tibbles And heres what parameters(model) |&gt; format_table() does to the a parameters table: Much cleaner for making a table to export to Word. 12.3.2 Getting model information and performance metrics Again, two options here. You can use either glance from the broom package, or performance from the package of the same name. These each produce slightly different output, though unlike above, I dont think one is necessarily better than the other. Use whichever one you prefer. broom::glance(model) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.619 0.614 0.515 119. 1.67e-31 2 -112. 231. 243. 39.0 147 150 performance::performance(model) ## # Indices of model performance ## ## AIC | BIC | R2 | R2 (adj.) | RMSE | Sigma ## ----------------------------------------------------- ## 231.452 | 243.494 | 0.619 | 0.614 | 0.510 | 0.515 12.3.3 Effect size info with effectsize logreg_model=glm(smoke ~ age + sex, data= legaldmlab::survey, family = &quot;binomial&quot;) logreg_model_coeff=parameters::parameters(logreg_model) logreg_model_coeff=logreg_model_coeff |&gt; dplyr::mutate(odds_ratio=exp(Coefficient)) effectsize::interpret_oddsratio(logreg_model_coeff$odds_ratio, rules = &quot;chen2010&quot;) ## [1] &quot;small&quot; &quot;very small&quot; &quot;very small&quot; ## (Rules: chen2010) 12.3.4 Quick, detailed, and automated reporting with report Check out https://easystats.github.io/report/ 12.3.5 Running correlations with correlation https://easystats.github.io/correlation/ "],["advanced-coding-tips.html", "Chapter 13 Advanced Coding Tips 13.1 Grammar and Syntax 13.2 Creating a package 13.3 Creating a bookdown", " Chapter 13 Advanced Coding Tips 13.1 Grammar and Syntax 13.1.1 Regex expressions and symbols str_remove(html$`Market Price`, pattern = &quot;$&quot;) # doesn&#39;t remove the $ sign str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) # works 13.1.2 The colon-equals (:=) operator Sometimes when making a function you need to use the colon-equals operator, rather than just the normal &lt;- or = assignment operators Specifically, when you have multiple named arguments in your functionRead my question and someones answer on this blogpost: https://community.rstudio.com/t/help-creating-simple-function/109011/2 13.1.3 User-supplied expressions or named columns in functions This is when you have to put double braces around something 13.1.4 When a command requires a named column or data set, but youve already supplied it and its required a second time If youre writing a function with a pipe but the command youre using needs the data set defined in it, you specify it as .x , or simply just .; Here is an example: 13.2 Creating a package https://rstudio4edu.github.io/rstudio4edu-book/data-pkg.html 13.2.1 Documenting package meta-data https://r-pkgs.org/description.html 13.2.2 Connecting to other packages https://kbroman.org/pkg_primer/pages/depends.html 13.2.3 Linking Git and Github view this detailed guide by Jenny Bryan, and this YouTube video if you want the full guide; or just follow the TL;DR below. Quick summary of steps in YouTube video: Open project folder in Windows Explorer and click in the URL bar, then type cmd to open command prompt If there are any pre-existing git files or repository info there, remove it with the following: rd .git /S/Q Tell git to create a new repo by typing: git init Then tell it to include all files in the current place by typing: git add . Commit these files with: git commit -m \"Initial commit\" At this point youve created a git and GitHub repo each; now link them with: git remote add origin [https URL of GitHub repo] Push all these changes live with: git push -u origin master 13.3 Creating a bookdown https://www.youtube.com/watch?app=desktop&amp;v=m5D-yoH416Y&amp;feature=youtu.be 13.3.1 Rendering the book once its done Render locally with bookdown::render_book(index.Rmd) Use browseURL(\"docs/index.html\") to view your book locally (or just open index.html in a browser). If it looks good, commit and push all changed files to GitHub. "],["creating-a-simulated-data-set.html", "Chapter 14 Creating a simulated data set 14.1 Part 1: Independent samples from a normal distribution 14.2 Part 2: Creating data sets with quantitative and categorical variables 14.3 Part 3: Repeatedly simulate samples with replicate() 14.4 Part 4: repeatedly making whole data sets 14.5 Part 5: Using purrr", " Chapter 14 Creating a simulated data set From the tutorial on this page 14.1 Part 1: Independent samples from a normal distribution Consider the following first before you start doing stuff: - How many subjects are in each condition? - What are the means and standard deviations of each group? Set that shit below. # number of subjects per group A_sub_n &lt;- 50 B_sub_n &lt;- 50 # distribution parameters A_mean &lt;- 10 A_sd &lt;- 2.5 B_mean &lt;- 11 B_sd &lt;- 2.5 Now generate scores for each group A_scores &lt;- rnorm(A_sub_n, A_mean, A_sd) B_scores &lt;- rnorm(B_sub_n, B_mean, B_sd) Technically you could stop here and just analyze the data in this fashionbut its better to organize it into a table. One that looks like something you would import after real data collection. So do that next; make it look nice. dat &lt;- tibble( sub_condition = rep( c(&quot;A&quot;, &quot;B&quot;), c(A_sub_n, B_sub_n) ), score = c(A_scores, B_scores) ) head(dat) ## # A tibble: 6 x 2 ## sub_condition score ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 11.4 ## 2 A 12.0 ## 3 A 12.5 ## 4 A 8.84 ## 5 A 6.84 ## 6 A 11.0 Always perform a quality and consistency check on your data to verify that shits ok. dat %&gt;% group_by(sub_condition) %&gt;% summarise(n = n() , mean = mean(score), sd = sd(score)) ## # A tibble: 2 x 4 ## sub_condition n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 50 9.98 2.78 ## 2 B 50 10.9 3.02 14.2 Part 2: Creating data sets with quantitative and categorical variables From the web page at this link 14.2.1 2.a. DATA WITH NO DIFFERENCE AMONG GROUPS Critically important notes to know: When you use the rep() function, there are several different arguments you can specify inside it that control how stuff is repeated: using rep(x, each= ) repeats things element-wise; each element gets replicated n times, in order rep(c(&quot;A&quot;,&quot;B&quot;), each=3) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; using rep(x, times= ) repeats the sequence; the vector as a whole, as it appears, will be repeated with one sequence following the next rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), times=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; using rep(x, length.out) repeats only the number of elements you specify, in their original order rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), length.out=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; In this particular data, we want every combination of group and letter to be present ONCE. letters=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;) tibble(group = rep(letters[1:2], each = 3), factor = rep(LETTERS[3:5], times = 2), response = rnorm(n = 6, mean = 0, sd = 1) ) ## # A tibble: 6 x 3 ## group factor response ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A C -0.559 ## 2 A D 0.0378 ## 3 A E -1.14 ## 4 B C -1.10 ## 5 B D 0.509 ## 6 B E -0.304 14.2.2 2.b. Data WITH A DIFFERENCE among groups What if we want data where the means are different between groups? Lets make two groups of three observations where the mean of one group is 5 and the other is 10. The two groups have a shared variance (and so standard deviation) of 1. 14.2.2.1 Some notes first Creating a difference between the two groups average score means we have to tell R to sample itteratively from distributions with different means. We do this by specifying a vector of means within rnorm, like so: response = rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 4.066141 9.831952 4.307055 11.059641 4.808349 10.009015 You can see that: 1. draw 1 is from the distribution \\((\\mu=5,\\sigma=1)\\) 2. draw 2 is from the distribution \\((\\mu=5,\\sigma=1)\\) And this process repeats a total of six times. And if you happen to also specify a vector of standard deviations (purely to demonstrate what is happening, we wont actually do this), the first mean is paired with the first SD; the second mean is paired with the second SD; and so on. rnorm(n = 6, mean = c(5, 10), sd = c(2,0.1)) ## [1] 5.744356 10.043787 7.378609 9.952296 8.064520 10.119440 14.2.2.2 Ok, back to creating the data If you want there to be differences between the groups, we need to change the way the vector of factors is replicated, in addition to specifying the vector of means. We want to ensure that the sequence of A, B in the group column matches the sequence repeated in the response column. Here we are going to use length.out so that the whole sequence of A,B is repeated exactly in line with the alternating drawing from \\(\\mu=5\\), \\(\\mu=10\\). Its often best to do this by building each thing separately, and then combining it into a tibble when you have it figured out. group=rep(letters[1:2], length.out = 6) group ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; response=rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 4.948867 10.118024 3.109300 8.589811 4.047384 10.121841 tibble(group, response) ## # A tibble: 6 x 2 ## group response ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 4.95 ## 2 B 10.1 ## 3 A 3.11 ## 4 B 8.59 ## 5 A 4.05 ## 6 B 10.1 14.2.3 2.c. Data with MULTIPLE QUANTITATIVE VARIABLES with groups 14.3 Part 3: Repeatedly simulate samples with replicate() Instead of drawing values one at a time from a distribution, we want to do it many times. This is a job for replicate(). What replicate() does is run a function repeatedly. The replicate() function will perform a given operation as many times as you tell it to. Here we tell it to generate numbers from the distribution \\(N~(\\mu=0, \\sigma=1)\\), three times (as specified in the n=3 argument in line one) replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = FALSE ) ## [[1]] ## [1] 1.3203910 0.2040899 -0.7970636 -1.2431823 -0.4844866 ## ## [[2]] ## [1] -1.2322047 0.1063858 0.7922842 0.4413982 -0.8931153 ## ## [[3]] ## [1] 1.7023983 0.1654943 0.7787761 -0.1186586 -0.2111680 The argument simplify=FALSE tells it to return the output as a list. If you set this to TRUE it returns a matrix instead replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) ## [,1] [,2] [,3] ## [1,] 0.72115635 0.7402300 0.1134487 ## [2,] 0.92810115 -1.1035296 -1.0247406 ## [3,] 0.53107386 0.0286275 -0.3009290 ## [4,] -0.01083094 0.5085364 1.7663764 ## [5,] -0.51711197 0.3824463 -1.0787283 Specifying as.data.frame() with the matrix output can turn it into a data frame. replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) %&gt;% as.data.frame() %&gt;% rename(sample_a=V1, sample_b=V2, sample_c=V3) ## sample_a sample_b sample_c ## 1 0.56415036 -1.7060196 1.1146509 ## 2 -0.24011787 1.8521680 1.4152690 ## 3 -0.09948105 1.9130434 0.1568660 ## 4 -1.76657069 -0.2574811 -1.0662388 ## 5 0.67864934 0.5891597 -0.1166993 14.4 Part 4: repeatedly making whole data sets This is combining parts 2 and 3 to repeatedly create and sample data sets, resulting in a list of many data sets. simlist = replicate(n = 3, expr = data.frame(group = rep(letters[1:2], each = 3), response = rnorm(n = 6, mean = 0, sd = 1) ), simplify = FALSE) simlist ## [[1]] ## group response ## 1 A -1.1482417 ## 2 A -0.5806775 ## 3 A 0.3688638 ## 4 B 0.3304444 ## 5 B 0.3483946 ## 6 B -1.1257075 ## ## [[2]] ## group response ## 1 A 0.8829318 ## 2 A -1.3660498 ## 3 A -0.7142761 ## 4 B -0.2700098 ## 5 B -0.2419871 ## 6 B -0.4623374 ## ## [[3]] ## group response ## 1 A -0.6018124 ## 2 A 1.0289143 ## 3 A 0.1779939 ## 4 B 1.7056511 ## 5 B -0.3255277 ## 6 B -0.7897194 14.5 Part 5: Using purrr See this blog post Kruschke, J. (2015). Goals, power, and sample size. In J. K. Kruschke (Ed.), Doing bayesian data analysis: A tutorial with r, jags, and stan (2nd ed., pp. 359-398). Academic Press. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
