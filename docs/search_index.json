[["index.html", "Creating a simulated data set Chapter 1 A Monument to my Madness 1.1 What this book is, and what it is not", " Creating a simulated data set Ryan Schneider 2022-04-26 Chapter 1 A Monument to my Madness This book contains all my personal coding notes from the last two years. Why am I doing this? Probably because Im a glutton for punishment, and Id rather procrastinate than write my dissertation proposal. &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD 1.1 What this book is, and what it is not You know those absolutely amazing, comprehensive guides where you can learn everything you need to know about R? This is is not one of those guides. This book is designed as a quick reference guide for many of the most common things youll need to do in everyday data analysis and research. Think of it like a coding dictionary, as opposed to a manual or comprehensive text. If you want (or need) to learn R in-depth and/or from the ground up (i.e., youre a novice user), then you should go read Hadley Wickhams book and the tidyverse websites. Also, these slides might be a good high-level overview if youve never used the tidyverse before. That said, if youre already familiar with R and the tidyverse and just need a quick reference for what command do I need to accomplish XYZ, youve come to the right place. "],["introduction-r-basics.html", "Chapter 2 Introduction: R Basics 2.1 Importing Data 2.2 Exporting (i.e., saving) Data and Output", " Chapter 2 Introduction: R Basics For the love of God before you do anything, familiarize yourself with R Projects and the here package. These make R so much more user friendly and less of a nightmare. If you need an overview, go here: http://jenrichmond.rbind.io/post/how-to-use-the-here-package/ Now lets get stuck in. library(tidyverse) 2.1 Importing Data 2.1.1 Spreadsheets See https://nacnudus.github.io/spreadsheet-munging-strategies/index.html for more detailed and in-depth tutorials (if you need that kind of thing) 2.2 Exporting (i.e., saving) Data and Output 2.2.1 Exporting to .CSV Generally speaking, unless you have a specific reason to, dont. But if you must: write_csv() 2.2.2 Export to .RData (and load the data again later) save(obj_name, file=here::here(&quot;subfolder&quot;, &quot;save_file_name&quot;), compress = FALSE) load(here::here(&quot;folder&quot;, &quot;save_name.RData&quot;)) 2.2.3 Export to Excel library(openxlsx) #Method 1: If you only want to export 1 thing, and/or only need output document #write as object, with no formatting: write.xlsx(objectname,file = &quot;filenamehere.xlsx&quot;,colnames=TRUE, borders=&quot;columns&quot;) #write as table: write.xlsx(objectname,&quot;filename.xlsx&quot;,asTable = TRUE) #Method 2: If you want to do the above, but add multiple objects or tables to one workbook/file: ## first Create Workbook object wb &lt;- createWorkbook(&quot;AuthorName&quot;) #then add worksheets (as many as desired) addWorksheet(wb, &quot;worksheetnamehere&quot;) #then write the object to the worksheet writeData(wb, &quot;test&quot;, nameofobjectordataframe, startCol = 2, startRow = 3, rowNames = TRUE) #save excel file saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) #Method 3: exact same as method 2, but creating a more fancy tables wb &lt;- createWorkbook(&quot;Ryan&quot;) addWorksheet(wb, &quot;worksheetnamehere&quot;) writeDataTable(wb, sheetName, objectName, startCol = 1, startRow = 1, colNames = TRUE, rowNames = FALSE, tableStyle=&quot;TableStyleLight2&quot;,tableName=NULL, headerStyle = NULL,withFilter=FALSE,keepNA=TRUE,sep=&quot;, &quot;, stack = FALSE, firstColumn = FALSE, lastColumn = FALSE,bandedRows = TRUE,bandedCols = FALSE) saveWorkbook(wb, &quot;filenamehere.xlsx&quot;, overwrite =TRUE) 2.2.4 Access/edit specific cell number values rainbow=tibble::tribble(~Color, &quot;red&quot;, &quot;orange&quot;, &quot;black&quot;, &quot;green&quot;, &quot;blue&quot;, &quot;purple&quot;) rainbow$Color[3] # access, but can&#39;t overwrite this way ## [1] &quot;black&quot; rainbow[3,&quot;Color&quot;] # access and can overwrite ## # A tibble: 1 x 1 ## Color ## &lt;chr&gt; ## 1 black rainbow[3, &quot;Color&quot;]= &quot;yellow&quot; # save this value to row 3 in column &quot;Color&quot; rainbow ## # A tibble: 6 x 1 ## Color ## &lt;chr&gt; ## 1 red ## 2 orange ## 3 yellow ## 4 green ## 5 blue ## 6 purple "],["wrangle-data.html", "Chapter 3 Wrangle Data 3.1 Joining or Splitting 3.2 Selecting/extracting specific variables 3.3 If-then and Case-when 3.4 Conditional replacement of values 3.5 Merging variables 3.6 Apply a function to multiple variables at once 3.7 Pivoting (i.e., transposing) data 3.8 Managing Many Models 3.9 Turn row names into a column/variable 3.10 How to edit/change column names 3.11 Re-order columns in a data set 3.12 Date and time variables 3.13 Reverse-code a variable 3.14 Dummy coding (the very fast and easy way) 3.15 Create a relative ranking among several variables 3.16 Manipulating the working environment and many things at once", " Chapter 3 Wrangle Data This chapter contains useful tips on wrangling (i.e., manipulating) data. If you need to know to do to things like create new variables, split one variable into multiple variables, pivot a data set from wide to long, etc., look no further. If you want a pretty good intro tutorial to the dplyr package, click here 3.1 Joining or Splitting Joining and splitting data is pretty straightforward. 3.1.1 Whole Data Sets The code below is from this excellent tutorial set.seed(2018) df1=data.frame(customer_id=c(1:10), product=sample(c(&#39;toaster&#39;,&#39;TV&#39;,&#39;Dishwasher&#39;),10,replace = TRUE)) df2=data.frame(customer_id=c(sample(df1$customer_id, 5)),state=sample(c(&#39;New York&#39;,&#39;California&#39;),5,replace = TRUE)) df1=tibble::as_tibble(df1) df2=tibble::as_tibble(df2) # df1 =left table # df2= right table Inner join - retains only rows with values that appear in both tables, and matches by keys. If youre joining two Qualtrics surveys together, this is most likely the one you want to use (e.g.Â matching by participant name, and only keeping rows in the joined data set for participants that have responses logged in both survey 1 and survey 2 df1 %&gt;% inner_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York Left join - returns everything in the left, and rows with matching keys in the right df1 %&gt;% left_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Right join - returns everything in the right, and rows with matching keys in the left df1 %&gt;% right_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 3 Dishwasher New York ## 3 6 toaster New York ## 4 8 Dishwasher New York ## 5 9 Dishwasher New York # note: example if the customer id column was named something different in the second df #df1 %&gt;% left_join(df2,by=c(&#39;customer_id&#39;=&#39;name2&#39;)) Full join - retain all rows from both tables, and join matching keys in both right and left df1 %&gt;% full_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 10 x 3 ## customer_id product state ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Dishwasher New York ## 2 2 Dishwasher &lt;NA&gt; ## 3 3 Dishwasher New York ## 4 4 toaster &lt;NA&gt; ## 5 5 TV &lt;NA&gt; ## 6 6 toaster New York ## 7 7 toaster &lt;NA&gt; ## 8 8 Dishwasher New York ## 9 9 Dishwasher New York ## 10 10 TV &lt;NA&gt; Anti join - returns all rows in the left that do not have matching keys in the right df1 %&gt;% anti_join(df2,by=&#39;customer_id&#39;) ## # A tibble: 5 x 2 ## customer_id product ## &lt;int&gt; &lt;chr&gt; ## 1 2 Dishwasher ## 2 4 toaster ## 3 5 TV ## 4 7 toaster ## 5 10 TV 3.1.2 Individual Columns/Variables Splitting or joining columns is much easier than doing it to whole data sets. You can use dplyr::separate() to accomplish the former, and dplyr::unite() for the latter. print(&quot;hello&quot;) ## [1] &quot;hello&quot; 3.2 Selecting/extracting specific variables Sometimes when working with a data set, you want to work with a few specific variables. For instance, maybe you want to view a graph of only reverse-coded variables (which start with the prefix r); or maybe you want to create a subset of your data that has a few specific variables removed. For this you can use dplyr::select() and its associated helper commands select() can be thought of as extract; it tells R to identify and extract a specific variable (or variables) cars=mtcars # select one column cars %&gt;% select(mpg) # select multiple columns, if they are all next to one another cars %&gt;% select(mpg:hp) # select multiple columns by name (when not next to one another) by defining them in a vector cars %&gt;% select(c(mpg, hp, wt)) # select only variables that start with a certain prefix/character/pattern/etc. cars %&gt;% select(starts_with(&quot;d&quot;)) # ...or columns that end with a certain prefix/etc. cars %&gt;% select(ends_with(&quot;t&quot;)) # ...or contains a certain pattern or string cars %&gt;% select(contains(&quot;se&quot;)) # select ALL OF the variables in a data set that match those of a pre-defined vector # first define the names in a vector vars=c(&quot;hp&quot;, &quot;drat&quot;, &quot;gear&quot;, &quot;carb&quot;) #now use helper cars %&gt;% select(all_of(vars)) # select ANY OF the variables in a pre-defined vector vars_2=c(&quot;hp&quot;, &quot;drat&quot;, &quot;watermelon&quot;, &quot;grilled_cheese&quot;) # only the first two will be in the data cars %&gt;% select(any_of(vars_2)) # only (and all of) the variables actually PRESENT in the data are pulled # select only variables of a certain class or type cars %&gt;% select(where(is.numeric)) cars %&gt;% select(where(is.character)) Other examples can be seen on THIS LINK for a simple but detailed guide. 3.3 If-then and Case-when 3.3.1 If-then The premise of an if/then or if/else statement is simple: If condition 1 is satisfied, perform x operation; if not, then do y mtcars %&gt;% mutate(power_level=ifelse(mtcars$hp&lt;350, &quot;Low&quot;, &quot;High&quot;)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb power_level ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 Low ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 Low ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 Low ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 Low ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 Low ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Low This line of code effectively says: if the length in Sepal.Length is &gt;5, set new variable = to short; else, set it to long 3.3.2 Case-when When you have 3+ conditions, its easier to use case-when. This is a more simple and straightforward approach than nesting multiple if-else commands My_vector= case_when( Condition1 ~ value1, Condition2 ~ value2, Condition3 ~ value3 TRUE ~ valueForEverythingElse #catch all for things that don&#39;t meet the above conditions ) Example: mtcars %&gt;% mutate(size= case_when(cyl==4 ~ &quot;small&quot;, cyl==6 ~ &quot;medium&quot;, cyl==8 ~ &quot;large&quot;)) %&gt;% select(c(cyl,size)) %&gt;% head() ## cyl size ## Mazda RX4 6 medium ## Mazda RX4 Wag 6 medium ## Datsun 710 4 small ## Hornet 4 Drive 6 medium ## Hornet Sportabout 8 large ## Valiant 6 medium 3.4 Conditional replacement of values The following code is useful if you want to replace a value in one column, and the replacement is conditional upon the value in another column. mpg %&gt;% mutate(across(.cols = c(displ, cty, hwy), .fns = ~case_when(cyl == 4L ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) ## # A tibble: 234 x 11 ## manufacturer model displ year cyl trans drv cty hwy fl class ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 audi a4 NA 1999 4 auto(l5) f NA NA p compact ## 2 audi a4 NA 1999 4 manual(m5) f NA NA p compact ## 3 audi a4 NA 2008 4 manual(m6) f NA NA p compact ## 4 audi a4 NA 2008 4 auto(av) f NA NA p compact ## 5 audi a4 2.8 1999 6 auto(l5) f 16 26 p compact ## 6 audi a4 2.8 1999 6 manual(m5) f 18 26 p compact ## 7 audi a4 3.1 2008 6 auto(av) f 18 27 p compact ## 8 audi a4 quattro NA 1999 4 manual(m5) 4 NA NA p compact ## 9 audi a4 quattro NA 1999 4 auto(l5) 4 NA NA p compact ## 10 audi a4 quattro NA 2008 4 manual(m6) 4 NA NA p compact ## # ... with 224 more rows test %&gt;% mutate(across(.cols = c(rank), .fns = ~case_when(is.na(participant_score) ~ as.numeric(NA), TRUE ~ as.numeric(.x)))) 3.5 Merging variables Sometimes youll have multiple variables and you want to collapse them into a single variable. The pmin() command is useful for this. example_data=tribble(~A,~B,~C, 1,NA,NA, 2,NA,NA, 3,NA,NA, NA,4,NA, NA,5,NA, NA,6,NA, NA,NA,7, NA,NA,8, NA,NA,9) example_data %&gt;% mutate(accept_reject = pmin(A,B,C,na.rm = TRUE)) 3.6 Apply a function to multiple variables at once You can either specify each column individually, like above, or tell R to identify columns for you based on their type or their name. This requires adding in one additional verbeither contains() or where() depending on what you want to do. Two simple examples: # turn multiple variables into factors ex_data=dplyr::tribble(~color, ~car, &quot;red&quot;, &quot;corvette&quot;, &quot;blue&quot;, &quot;chevelle&quot;, &quot;green&quot;, &quot;camaro&quot;, &quot;red&quot;, &quot;corvette&quot;, &quot;green&quot;, &quot;chevelle&quot;, &quot;yellow&quot;, &quot;gto&quot;) dplyr::glimpse(ex_data) ## Rows: 6 ## Columns: 2 ## $ color &lt;chr&gt; &quot;red&quot;, &quot;blue&quot;, &quot;green&quot;, &quot;red&quot;, &quot;green&quot;, &quot;yellow&quot; ## $ car &lt;chr&gt; &quot;corvette&quot;, &quot;chevelle&quot;, &quot;camaro&quot;, &quot;corvette&quot;, &quot;chevelle&quot;, &quot;gto&quot; ex_data %&gt;% mutate(across(c(color, car),factor)) ## # A tibble: 6 x 2 ## color car ## &lt;fct&gt; &lt;fct&gt; ## 1 red corvette ## 2 blue chevelle ## 3 green camaro ## 4 red corvette ## 5 green chevelle ## 6 yellow gto # round multiple columns to 1 decimal place mtcars %&gt;% mutate(across(c(disp:qsec),round,1)) %&gt;% head() ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.9 2.6 16.5 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.9 2.9 17.0 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.9 2.3 18.6 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.1 3.2 19.4 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.1 3.4 17.0 0 0 3 2 ## Valiant 18.1 6 225 105 2.8 3.5 20.2 1 0 3 1 3.7 Pivoting (i.e., transposing) data 3.7.1 Condense multiple rows into a single column (pivot wide to long) Rearranging data like this can make it easier to work with and analyze. Example below from my gradebook for stats (exported from Canvas), with fake names. The command structure is as follows: pivot_longer( # Transpose LENGTHWISE by.... cols = everything(), # Taking ALL variable names... names_to=&quot;variable&quot;, # ...and dumping them into this new variable/column values_to=&quot;missing_count&quot;) #...and placing their values in this other new column NOTE!!! Pivoting data from wide to long like this expands the number of rows to make a matrix so that (for example, each student now has as a row for each assignment). Therefore, you can only pivot longways (or wide) ONCE, otherwise you will make duplicates. If you need to pivot multiple columns, just include all of the columns in one single pivot; do not use two separate, back to back pivot commands. Example: gradebook=tibble::tribble( ~Student, ~Homework.1, ~Homework.2, ~Homework.3, ~Homework.4, ~Homework.5, ~Quiz.1, ~Quiz.2, ~Quiz.3, ~Quiz.4, ~Final, &quot;Bob&quot;, 19L, 0L, 13, 16, 0L, 21, 7L, 15, 17.5, 33, &quot;Jane&quot;, 17L, 19L, 16, 16.5, 25L, 21.5, 19L, 14.75, 9.5, 39.5, &quot;John&quot;, 19L, 19L, 14.5, 19.5, 25L, 21, 21L, 18.5, 17, 46.5 ) head(gradebook) ## # A tibble: 3 x 11 ## Student Homework.1 Homework.2 Homework.3 Homework.4 Homework.5 Quiz.1 Quiz.2 Quiz.3 Quiz.4 Final ## &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Bob 19 0 13 16 0 21 7 15 17.5 33 ## 2 Jane 17 19 16 16.5 25 21.5 19 14.8 9.5 39.5 ## 3 John 19 19 14.5 19.5 25 21 21 18.5 17 46.5 gradebook=gradebook %&gt;% pivot_longer( # Transpose lengthwise by: cols = Homework.1:Final, # Taking these variables names_to=&quot;Assignment&quot;, # ...and dumping them into this new variable, storing them lengthwise values_to=&quot;Points&quot;) #...then place their values in this new column gradebook %&gt;% head() ## # A tibble: 6 x 3 ## Student Assignment Points ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Bob Homework.1 19 ## 2 Bob Homework.2 0 ## 3 Bob Homework.3 13 ## 4 Bob Homework.4 16 ## 5 Bob Homework.5 0 ## 6 Bob Quiz.1 21 3.8 Managing Many Models Imagine the concept of Russian Dolls, applied to data sets. You can manage data sets more effectively my collapsing them into a single tiny, mini data frame, and stuffing that inside of another one. This is done via nesting Effectively, you smush/collapse everything down so it fits inside one column. You can unnest to expand this data back out later when you need it, and keep it collapsed when you dont. Code works like this: by_country=gapminder::gapminder %&gt;% group_by(continent,country) %&gt;% # indicate the variables to keep at the top level nest() # smush the rest into a list-column country_model=function(df){ lm(lifExp~year1950,data = df) } # Transform a list of models into a df models=by_country %&gt;% mutate(mod=map(data,country_model)) You can store anything in a data frame. You can keep the df connected to the model, which makes it very easy to manage a whole slew of related models You can use functional programming (i.e., iterative functions) to map functions or combinations of functions in new ways. Converting data into tidy data sets gives you a whole new way (and easier way) to manage lots of information Below is the full script I copied from Hadley Wickhams lecture, which you can watch here pacman::p_load(dplyr,purrr,tidyverse,gapminder) #### Workflow for managing many models in R #### # 1. Nest data with {tidyr} # 2. Use {purrr} to map a modeling function # 3. Use {broom} to inspect your tidy data gapminder=gapminder %&gt;% mutate(year1950= year-1950) #the number of years it&#39;s been since 1950 #-------------------------------------------------------------------------------------------- #### Step 1. Nest the data. #### # A nested data frame has one column per country. You&#39;re essentially # creating a Russian doll; a data frame inside of a larger data frame. by_country=gapminder %&gt;% group_by(continent,country) %&gt;% # variables to keep at the top level nest() # smush everything else into a df, and store this mini-df in its own column # with this, you can have an entire table per row; a whole data frame for each country # Essentially condensing a list into a table by_country$data[[1]] #-------------------------------------------------------------------------------------------- #### Step 2. Use purrr to map stuff. #### # 12:50 country_model=function(df){ lm(lifeExp ~ year1950, data = df) } models= by_country %&gt;% mutate( mod=map(data,country_model) ) gapminder %&gt;% group_by(continent,country) %&gt;% nest() %&gt;% mutate( mod= data %&gt;% map(country_model) ) # 27:11 #-------------------------------------------------------------------------------------------- ##### Step 3. #### # This creates another nested df inside of your main data frame that has the summary stats of each model models=models %&gt;% mutate( tidy=map(mod, broom::tidy), # tidy() gives model estimates glance=map(mod,broom::glance), # glance() gives model summaries augment=map(mod,broom::augment) # model coefficients ) # What can you do with this nest of data frames? # The reverse of step 1; un-nest it to unpack everything! # 34:40 # Keeps a massive list of related information neatly organized! unnest(models,data) # back to where we started unnest(models,glance, .drop = TRUE) unnest(models,tidy) and here is a version I made of the above to manage many Bayesian models. Admittedly, Im not really sure how useful this is though. # CONDENSED MASTER TABLE VERSION ----------------------------------------------------------------------------- # Models table that has all models condensed models=tribble(~Model_name, ~model_descrip, ~model, &quot;Thesis_Model&quot;, &quot;Discount and PTS&quot;, Thesis_Model, &quot;discount_model&quot;, &quot;Discount variable only&quot;, discount_model, &quot;PTS_model&quot;, &quot;PTS variable only&quot;, PTS_model ) # Clean up work space #rm(DiscountPrior,Priors_MEmodel,Priors_Interactionmodel) # Grab and store all model info models=models %&gt;% mutate(prior_info=map(model,describe_prior), posterior_info=map(model, describe_posterior_fancy), model_performance=map(model,performance::performance) ) # DO NOT TRY AND VIEW THE TABLE IN A WINDOW!!!! RStan objects are so large they cause R to lock up # Call the model in the console instead #### summon individual model stats #### describe_prior(models$model[[1]]) #by specific model map(models$model,describe_prior) # do for all models at once # or all info for all models at once unnest(models,posterior_info) %&gt;% select(-c(model,prior_info,model_descrip)) 3.9 Turn row names into a column/variable Use the rownames() command to turn row names into a variable cars=rownames_to_column(mtcars, var = &quot;car&quot;) as_tibble(cars) %&gt;% slice(1:6) ## # A tibble: 6 x 12 ## car mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 Mazda RX4 Wag 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 Datsun 710 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 Hornet 4 Drive 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 Hornet Sportabout 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 Valiant 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 3.10 How to edit/change column names TWO WAYS TO DO THIS: Use colnames() (for base R) or rename() (for tidyverse) colnames() pulls up all the column/variable names as a vector. If you want to actually change them, youll need to combine this command with something like the sub() or gsub() commands (for base R). Im going to skip this becauseits base R. To access and change the names faster via tidyverse, run use rename() rm(list=ls()) # clear R&#39;s memory iris %&gt;% rename(&quot;hurr&quot;=&quot;Sepal.Length&quot;, &quot;durr&quot;=&quot;Sepal.Width&quot;, &quot;abcdefgh&quot;=&quot;Species&quot;) %&gt;% head() ## hurr durr Petal.Length Petal.Width abcdefgh ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa If you need to do some really fancy conditional renaming (e.g., changing all variables that start with r to start with rf instead, to make it more clear that the prefix actually stands for risk factor rather than reverse coded), youll need to use rename_with(). This command has two parts to it: the data set, and the function you wish to apply to it (which you put after the ~) rename_with(iris, ~ gsub(pattern = &quot;.&quot;, replacement = &quot;_&quot;, .x, fixed = TRUE)) %&gt;% head() ## Sepal_Length Sepal_Width Petal_Length Petal_Width Species ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa The gsub() function from Base R identifies matching patterns in the data and substitutes them with what you want instead. Think of it like Rs version of Find/Replace from Microsoft Word. The above line of code thus does the following: 1. First, it checks the column names of the supplied data set (iris) for a specific pattern (specified in pattern= ) 2. Then it replaces that pattern with your input in replacement= The great thing about rename_with() is that the .fn (or ~ for short) can take ANY function as input. For example, if you want to add an element to the column names rather than replace something, (e.g., a prefix or suffix), you can change the function to: rename_with( iris, ~ paste0(.x, &quot;_text&quot;)) %&gt;% head() ## Sepal.Length_text Sepal.Width_text Petal.Length_text Petal.Width_text Species_text ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3.0 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5.0 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa The above line adds a suffix. You can also add a prefix in the exact same way, just by switching the order of the string and the pattern in the paste0 command. Alternative method to the above This is a second way to do the above. It may appear more simple, but its also probably not as theoretically consistent with how the packages were made..it uses the stringr package to rename the column names, and stringr is typically used for editing vectors of strings in a data set. so it works, but its a little unconventional because you call and edit the column names like you would a variable in your data set. colnames(iris)=str_replace(colnames(iris), pattern = &quot;.&quot;, replacement = &quot;_&quot;) In short: rename() and rename_with() are for renaming variables, as their names imply. The str_ verbs from the stringr package are for editing string-based variabels in your data set. Either works though with a little ingenuity. 3.11 Re-order columns in a data set Use relocate() to change column positions. If you need to move multiple columns at once, this command uses the same syntax as select(). mtcars # notice the column order ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## Duster 360 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 ## Merc 230 22.8 4 140.8 95 3.92 3.150 22.90 1 0 4 2 ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## Merc 280C 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## Merc 450SE 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## Merc 450SLC 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## Cadillac Fleetwood 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## Lincoln Continental 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## Chrysler Imperial 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## Fiat 128 32.4 4 78.7 66 4.08 2.200 19.47 1 1 4 1 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 1 1 4 2 ## Toyota Corolla 33.9 4 71.1 65 4.22 1.835 19.90 1 1 4 1 ## Toyota Corona 21.5 4 120.1 97 3.70 2.465 20.01 1 0 3 1 ## Dodge Challenger 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## AMC Javelin 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## Camaro Z28 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## Pontiac Firebird 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## Fiat X1-9 27.3 4 79.0 66 4.08 1.935 18.90 1 1 4 1 ## Porsche 914-2 26.0 4 120.3 91 4.43 2.140 16.70 0 1 5 2 ## Lotus Europa 30.4 4 95.1 113 3.77 1.513 16.90 1 1 5 2 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 ## Maserati Bora 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 ## Volvo 142E 21.4 4 121.0 109 4.11 2.780 18.60 1 1 4 2 mtcars %&gt;% relocate(hp:wt, .after= am) %&gt;% head() ## mpg cyl disp qsec vs am hp drat wt gear carb ## Mazda RX4 21.0 6 160 16.46 0 1 110 3.90 2.620 4 4 ## Mazda RX4 Wag 21.0 6 160 17.02 0 1 110 3.90 2.875 4 4 ## Datsun 710 22.8 4 108 18.61 1 1 93 3.85 2.320 4 1 ## Hornet 4 Drive 21.4 6 258 19.44 1 0 110 3.08 3.215 3 1 ## Hornet Sportabout 18.7 8 360 17.02 0 0 175 3.15 3.440 3 2 ## Valiant 18.1 6 225 20.22 1 0 105 2.76 3.460 3 1 3.12 Date and time variables Formatting a column of dates can be extremely helpful if you need to work with time data, but also an extreme pain in the ass if its not stored correctly. This tutorial will be divided into two parts to cover both scenarios that you could encounter. It requires things to be done in two stages, and very precisely. 3.12.1 Date-time objects If youre lucky enough to have a vector of date-times, like what Qualtrics gives you, this will be brainless. Just do the following: example_datetime_data=tibble::tribble(~datetime, &quot;2010-08-03 00:50:50&quot;, &quot;2010-08-04 01:40:50&quot;, &quot;2010-08-07 21:50:50&quot;) head(example_datetime_data) # stored as character string ## # A tibble: 3 x 1 ## datetime ## &lt;chr&gt; ## 1 2010-08-03 00:50:50 ## 2 2010-08-04 01:40:50 ## 3 2010-08-07 21:50:50 # Tidyverse lubridate::as_date(example_datetime_data$datetime) ## [1] &quot;2010-08-03&quot; &quot;2010-08-04&quot; &quot;2010-08-07&quot; 3.12.2 Date-only objects If youre unlucky enough to have only dates, and said dates are written in the traditional x/x/xxxx format, this will be an annoyance that has to be done in two stages. First, assuming your data is already imported and is being stored as a vector of character strings, you have to tell R to adjust the formatting of dates. You cannot change it from a character-based object into a Date or DateTime one until it recognizes the correct formatting. example_date_data=tibble::tribble(~X1, ~X2, &quot;8/4/2021&quot;, -49.87, &quot;8/4/2021&quot;, -13.85, &quot;8/3/2021&quot;, -7.45, &quot;8/3/2021&quot;, -172.71) # Correct formatting example_date_data$X1=format(as.POSIXct(example_date_data$X1,format=&#39;%m/%d/%Y&#39;),format=&#39;%Y-%m-%d&#39;) head(as_tibble(example_date_data)) ## # A tibble: 4 x 2 ## X1 X2 ## &lt;chr&gt; &lt;dbl&gt; ## 1 2021-08-04 -49.9 ## 2 2021-08-04 -13.8 ## 3 2021-08-03 -7.45 ## 4 2021-08-03 -173. In the code above, note that there are two format commands: The first one tells R how the date data is currently being stored, while the second at the end tells it how you want it to be stored. In this case, we are changing it from the way we would usually hand write a date (e.g., 10/26/1993) to a format commonly recognized and used in Excel and stats software (1993-10-26). If your column also has times in it, you also need to include that too! Second, you can now correct the objects structure. You can do this with base Rs as.Date() or tidyverses date() verbs. # Tidyverse example_date_data$X1= lubridate::date(example_date_data$X1) # Base R version example_date_data$X1=as.Date(example_date_data$X1) Notice how the object is now stored as the correct type in the table above. NOTE! This entire process has been included in the tidy_date() command in my package, legaldmlab. 3.12.3 Find the difference between two dates/times difftime(part_1$end_date[1], part_2$end_date[1], units=&quot;days&quot;) 3.13 Reverse-code a variable To reverse-score a variable, you should use car::recode() Can be done a few different ways, depending on how many variables youre looking to recode: # Recode just one variable df$column=recode(df$column,&quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;) # Recode a select bunch of variables df=df %&gt;% mutate(across(c(family_close : family_feelings), recode, &quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;)) # Recode the whole damn thing. All columns. df=df %&gt;% map_df(recode, &quot;1 = 7 ; 2 = 6 ; 3 = 5 ; 5 = 3 ; 6 = 2 ; 7 = 1&quot;) 3.14 Dummy coding (the very fast and easy way) Use dplyrs pivot_wider in conjunction with mutate to very quickly and automatically dummy code a column with any number of unique values. The middle part of the code below is what you needjust copy and paste it, and tweak the specifics library(tidyverse) mtcars |&gt; mutate(car=rownames(mtcars)) |&gt; dplyr::mutate(n=1) |&gt; tidyr::pivot_wider(names_from = cyl, values_from = n, names_prefix = &quot;number_cyl&quot;, values_fill = list(n=0)) |&gt; select(car, starts_with(&quot;number_&quot;)) |&gt; head() #truncate output for easier reading ## # A tibble: 6 x 4 ## car number_cyl6 number_cyl4 number_cyl8 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Mazda RX4 1 0 0 ## 2 Mazda RX4 Wag 1 0 0 ## 3 Datsun 710 0 1 0 ## 4 Hornet 4 Drive 1 0 0 ## 5 Hornet Sportabout 0 0 1 ## 6 Valiant 1 0 0 3.15 Create a relative ranking among several variables If you want to create a variable that is an ordinal ranking of other variables, first you need to make sure your data is long-wise. Then, depending on the type of ranking system you want, youll might need a different ranking command. The min_rank command from dplyr works in a manner similar to base Rs rank command. It ranks things like you see in sporting events. For example, if there is a clear winner in a game but 3 people tie for second place, the ranks would look like this: 1,2,2,2,4,5. Notice that the positions are independent from the counts. Using the same example from above, if you want the ranks to have no gaps (i.e.Â 1,2,2,2,3,4), you need to use dplyrs dense_rank command. In either case, the ranks are generated from lowest to highest, so if you want to flip them around youll need to include desc() in the command. dat=tibble::tribble(~name, ~score, &quot;bob&quot;, 0, &quot;bob&quot;, 5, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, 50, &quot;bob&quot;, NA, &quot;alice&quot;, 70, &quot;alice&quot;, 80, &quot;alice&quot;, 90, &quot;alice&quot;, 20, &quot;alice&quot;, 20, &quot;alice&quot;, 1) dat %&gt;% mutate(ranked = dense_rank(desc(score))) ## # A tibble: 12 x 3 ## name score ranked ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; ## 1 bob 0 8 ## 2 bob 5 6 ## 3 bob 50 4 ## 4 bob 50 4 ## 5 bob 50 4 ## 6 bob NA NA ## 7 alice 70 3 ## 8 alice 80 2 ## 9 alice 90 1 ## 10 alice 20 5 ## 11 alice 20 5 ## 12 alice 1 7 3.16 Manipulating the working environment and many things at once 3.16.1 Stuff the WHOLE working environment into a list files=mget(ls()) 3.16.2 Extract everything from a list into the environment list2env(cog_data, globalenv()) 3.16.3 Delete everything in the entire environment, except for one item rm(list=setdiff(ls(), &quot;cog_data&quot;)) # delete everything in the local environment except the final data set "],["clean-data.html", "Chapter 4 Clean Data 4.1 Replace a value with NA 4.2 Replace NAs with a value 4.3 Identify columns or rows with Missing values 4.4 Find the percentage of a variable that is missing 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set", " Chapter 4 Clean Data 4.1 Replace a value with NA Use dplyr::na_if() if you have a value coded in your data (e.g., 999) that you want to convert to NA example_data=dplyr::tribble(~name, ~bday_month, &quot;Ryan&quot;, 10, &quot;Z&quot;, 3, &quot;Jen&quot;, 999, &quot;Tristin&quot;, 999, &quot;Cassidy&quot;, 6) example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen 999 ## 4 Tristin 999 ## 5 Cassidy 6 example_data$bday_month=na_if(example_data$bday_month, 999) #example doing one column at a time example_data ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 example_data %&gt;% # can also pass the data to mutate and do it the tidyverse way mutate(bday_month=na_if(bday_month, 999)) ## # A tibble: 5 x 2 ## name bday_month ## &lt;chr&gt; &lt;dbl&gt; ## 1 Ryan 10 ## 2 Z 3 ## 3 Jen NA ## 4 Tristin NA ## 5 Cassidy 6 4.2 Replace NAs with a value tidyr::replace_na() is very useful if you have some NAs in your data and you want to fill them in with some value. example_data=tibble::tribble(~name, ~fav_color, ~fav_food, &quot;Ryan&quot;, &quot;green&quot;, &quot;Mexican&quot;, &quot;Cassidy&quot;, &quot;blue&quot;, NA, &quot;Z&quot;, NA, NA, &quot;Tristin&quot;, &quot;purple&quot;, NA, &quot;Tarika&quot;, NA, NA, &quot;Jen&quot;, NA, &quot;Italian&quot;) example_data ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue &lt;NA&gt; ## 3 Z &lt;NA&gt; &lt;NA&gt; ## 4 Tristin purple &lt;NA&gt; ## 5 Tarika &lt;NA&gt; &lt;NA&gt; ## 6 Jen &lt;NA&gt; Italian # replace NA&#39;s in one col tidyr::replace_na(example_data$fav_food, &quot;MISSING&quot;) ## [1] &quot;Mexican&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;MISSING&quot; &quot;Italian&quot; # replace in multiple columns example_data %&gt;% mutate(across(c(fav_color, fav_food), replace_na, &quot;MISSING&quot;)) ## # A tibble: 6 x 3 ## name fav_color fav_food ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Ryan green Mexican ## 2 Cassidy blue MISSING ## 3 Z MISSING MISSING ## 4 Tristin purple MISSING ## 5 Tarika MISSING MISSING ## 6 Jen MISSING Italian 4.3 Identify columns or rows with Missing values is.na() is the base R way to identify, in a TRUE/FALSE manner, whether or not there are missing values in a vector y &lt;- c(1,2,3,NA) is.na(y) # returns a vector (F F F T) ## [1] FALSE FALSE FALSE TRUE 4.4 Find the percentage of a variable that is missing Sometimes necessary to check before conducting an analysis. This requires my package, legaldmlab ?legaldmlab::count_missing mtcars %&gt;% select(hp:drat) %&gt;% legaldmlab::count_missing() ## # A tibble: 2 x 3 ## variable missing_count percent_missing ## &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 hp 0 0.0% ## 2 drat 0 0.0% 4.5 Exclude Missing values from analysis 4.6 Dropping Missing values from the data set Use tidyr::drop_na() to remove rows with missing values. example_data=dplyr::tribble(~name, ~bday_month, ~car, &quot;Ryan&quot;, 10, &quot;kia&quot;, &quot;Z&quot;, NA, &quot;toyota&quot;, &quot;Jen&quot;, NA, NA, &quot;Tristin&quot;, 999, NA, &quot;Cassidy&quot;, 6, &quot;honda&quot;) knitr::kable(example_data) name bday_month car Ryan 10 kia Z NA toyota Jen NA NA Tristin 999 NA Cassidy 6 honda example_data %&gt;% drop_na() # with nothing specified, it drops ALL variables that have &gt;=1 missing value ## # A tibble: 2 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Cassidy 6 honda example_data %&gt;% drop_na(car) # drops only rows with values missing in the specified column ## # A tibble: 3 x 3 ## name bday_month car ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Ryan 10 kia ## 2 Z NA toyota ## 3 Cassidy 6 honda "],["working-with-factors.html", "Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels 5.2 Collapse factor levels 5.3 Add levels to a factor 5.4 Drop unused levels 5.5 Change the order of a factors levels", " Chapter 5 Working with Factors 5.1 Manually recode/change a factors levels Use forcats::fct_recode() diamonds=diamonds %&gt;% as_tibble() diamonds$cut=fct_recode(diamonds$cut, &quot;meh&quot;=&quot;Fair&quot;, &quot;Wow&quot;=&quot;Premium&quot;) summary(diamonds$cut) ## meh Good Very Good Wow Ideal ## 1610 4906 12082 13791 21551 5.2 Collapse factor levels Extremely useful command for when you have infrequent cases in one factor and need to combine it with another. Works by specifying a series of new level names, each of which contains the information from the old variables. Format is as follows: fct_collapse(dataset$variable, NewLevelA=c(&quot;OldLevel1&quot;,&quot;Oldlevel2&quot;), # NewLevelA is the new variable that contains both variables 1 and 2 NewLevelB=c(&quot;OldLevel3&quot;)) 5.3 Add levels to a factor use fct_expand() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.4 Drop unused levels Use fct_drop() print(&quot;temp&quot;) ## [1] &quot;temp&quot; 5.5 Change the order of a factors levels example_data=tribble(~person, ~condition, &quot;bob&quot;, &quot;25 years&quot;, &quot;jane&quot;, &quot;5 years&quot;, &quot;jim&quot;, &quot;5 years&quot;, &quot;john&quot;, &quot;25 years&quot;) example_data$condition=factor(example_data$condition) str(example_data$condition) ## Factor w/ 2 levels &quot;25 years&quot;,&quot;5 years&quot;: 1 2 2 1 Notice that R thinks these are nominal factors, and that 25 comes before 5. To fix this and correct the level order example_data$condition =fct_relevel(example_data$condition, c(&quot;5 years&quot;, &quot;25 years&quot;)) # specify level order str(example_data$condition) ## Factor w/ 2 levels &quot;5 years&quot;,&quot;25 years&quot;: 2 1 1 2 "],["working-with-strings.html", "Chapter 6 Working with Strings 6.1 Remove a pattern from a string 6.2 Replace one pattern in a string with another 6.3 Find (i.e., filter for) all instances of a string 6.4 Drop all rows from a data set that contain a certain string 6.5 Force all letters to lower case", " Chapter 6 Working with Strings 6.1 Remove a pattern from a string price_table=tribble(~car, ~price, &quot;Corvette&quot;, &quot;$65,000&quot;, &quot;Mustang GT&quot;, &quot;$40,000&quot;) # BASE R METHOD (sub by replacing something with nothing) gsub(&quot;\\\\$&quot;, &quot;&quot;,price_table$price) # (pattern, replace with, object$column) ## [1] &quot;65,000&quot; &quot;40,000&quot; # TIDYVERSE METHOD str_remove(price_table$price, pattern = &quot;\\\\$&quot;) ## [1] &quot;65,000&quot; &quot;40,000&quot; 6.2 Replace one pattern in a string with another Tidyverse command: str_replace() Base R command: gsub() # base R gsub(mtcars, replacement = ) #tidyverse 6.3 Find (i.e., filter for) all instances of a string Useful for finding very specific things inside a column (e.g., one particular persons name in a roster of names; everyone with a particular last name) Tidyverse command: str_detect() Base R command: grepl() Note both must be nested inside of filter() cars_df=rownames_to_column(mtcars, var = &quot;car&quot;) # base R cars_df |&gt; filter(grepl(&quot;Firebird&quot;, car)) # tidyverse cars_df %&gt;% filter(str_detect(car,&quot;Firebird&quot;)) You can also search for multiple strings simultaneously by including the or logical operator inside the quotes. cars_df |&gt; filter(str_detect(car, &quot;Firebird|Fiat&quot;)) You can also include the negation logical operator to filter for all instances except those with the specified string. # base R cars_df |&gt; filter(!(grepl(&quot;Pontiac&quot;, car))) # tidyverse cars_df |&gt; filter(!(str_detect(car, &quot;Pontiac&quot;))) 6.4 Drop all rows from a data set that contain a certain string # Tidyverse method cars_df |&gt; filter(str_detect(car, &quot;Merc&quot;, negate = TRUE)) #including negate=TRUE will negate all rows with the matched string # base R cars_df[!grepl(&quot;Merc&quot;, cars_df$car),] 6.5 Force all letters to lower case Use stringr::str_to_lower() blah=tribble(~A, ~B, &quot;A&quot;,&quot;X&quot;, &quot;A&quot;,&quot;X&quot;) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 A X ## 2 A X blah$A=str_to_lower(blah$A) blah ## # A tibble: 2 x 2 ## A B ## &lt;chr&gt; &lt;chr&gt; ## 1 a X ## 2 a X "],["figures-and-graphs-with-the-ggplot-and-see-packages.html", "Chapter 7 Figures and Graphs with the ggplot and see packages 7.1 Commands for ggplot graph types 7.2 Specific Commands for Specific Types of Analysis 7.3 Highlight specific points 7.4 Add labels to data points 7.5 Plotting multiple graphs at once 7.6 Change the colors (bars; columns; dots; etc.) 7.7 Other aesthetic mappings 7.8 Adding and Customizing Text 7.9 Remove gridlines 7.10 Faceting 7.11 Log transformations 7.12 Changing the scale of the axis 7.13 Add a regression line", " Chapter 7 Figures and Graphs with the ggplot and see packages There are three parts to a ggplot2 call: 1. data 2. aesthetic mapping 3. Layer There is no piping involved in ggplot. You simply invoke ggplot, and tell it what they dataset is. Then you specify the aesthetics, and then the mapping. Lastly, include other optional stuff (e.g.Â expanded y-axis scale; titles and legends; etc.) Every single plot has the exact same layout that ONLY USES the above three points: ggplot(dataframe, aes(graph dimensions and variables used)) + geom_GraphType(specific graph controls) ## OR ## ggplot(dataframe) + geom_GraphType(aes(graph dimensions and variables used), specific graph controls) # mapping= aes() can go in either spot Then if you have other stuff you want to add on top of this, like axis labels, annotations, highlights, etc., you keep adding those in separate lines 7.1 Commands for ggplot graph types Graph Type Geom command Scatter geom_point() Line geom_line() Box geom_boxplot() Bar geom_bar() Column geom_col() Histogram geom_histogram() Density curve geom_density() Note that bar and column graphs look identical at first glance, but they serve two different purposes. Bar graphs are for frequency counts, and thus only take an X-axis variable; Column graphs are for showing the relationship between two variables X and Y, and display the values in the data # BAR GRAPH # height of bars is a frequency count of each level of the X variable cut bar_plot=ggplot(diamonds, aes(x=cut)) + geom_bar()+ theme_classic() # COLUMN GRAPH # height of bars represents relationship between price and cut col_plot=ggplot(diamonds, aes(x=cut, y=price)) + geom_col()+ theme_classic() see::plots(bar_plot, col_plot, n_columns = 2, tags = c(&quot;Bar&quot;, &quot;Column&quot;)) 7.2 Specific Commands for Specific Types of Analysis 7.2.1 lavaan stuff 7.2.1.1 Plotting an SEM or CFA model First lets set up a model to use. library(lavaan) ## This is lavaan 0.6-9 ## lavaan is FREE software! Please report any bugs. HS.model &lt;- &#39; visual =~ x1 + x2 + x3 textual =~ x4 + x5 + x6 speed =~ x7 + x8 + x9&#39; fit1 &lt;- cfa(HS.model, data=HolzingerSwineford1939) Two options for graphing it. Option 1 is graph_sem() from the tidySEM package. tidySEM::graph_sem(fit1) Option 2 is from the easystats suite plot(parameters::parameters(fit1)) ## Using `sugiyama` as default layout 7.2.2 Bayes stuff Quick highlights here of my favorite functions from this package. See (ha) the full package overview at this link You can adjust the colors of the figures by setting them yourself (with scale_fill_manual), or by using the appropriate scale_fill command 7.2.2.1 Probability of Direction (Pd) figure Use plot(pd()) to visualize the Probability of Direction index. plot(bayestestR::pd(fit1))+ scale_fill_manual(values=c(&quot;#FFC107&quot;, &quot;#E91E63&quot;))+ theme_classic()+ theme(plot.title = element_text(hjust = 0.5, size = 14, face = &quot;italic&quot;)) 7.2.2.2 ROPE figure plot(fit1, rope_color = &quot;grey70&quot;)+ gameofthrones::scale_fill_got_d(option = &quot;white_walkers&quot;) # scale_fill_manual(values = c(&quot;gray75&quot;,&quot;red&quot;) ROPE tests are plots of distributions, and therefore use scale_fill_xyz_d commands. (the d stands for discrete). You can use any scale theme color set from any package, as long as it ends in _d values=c(#FFC107, #E91E63) is the default bayestestR theme colors from their website 7.2.2.3 Bayes factor models comparison figure plot(bayesfactor_models(Thesis_Model,discount_model))+ scale_fill_flat(palette = &quot;complement&quot; , reverse = TRUE)+ # scale color adjustment 7.2.3 Histograms and density curves Since I use these so often I figure they deserve their own special section. Basic histograms can be built with the following code: ggplot(data = mtcars, aes(x=cyl)) + geom_histogram(binwidth = .5, colour=&quot;Black&quot;, fill=&quot;green&quot;) + # histogram theme_classic() and your basic density curve with the following: ggplot(diamonds, aes(x=price)) + geom_density(alpha=.3)+ # density plot. Alpha sets the transparency level of the fill. theme_classic() You can also use the following code from bayestestR to build a really quick and nice density curve plot(bayestestR::point_estimate(diamonds, centrality=c(&quot;median&quot;,&quot;mean&quot;)))+ labs(title=&quot;Mean and Median&quot;) 7.3 Highlight specific points The gghighlight package is great for this # example 1 ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp))+ # add data labels (optional) gghighlight::gghighlight(hp &gt; 200) # add highlights, according to some criteria # example 2 diamonds_abr=diamonds %&gt;% slice(1:100) ggplot(diamonds_abr, aes(x= cut, y= price, colour=price))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = diamonds_abr, aes(label = price))+ # this line labels gghighlight::gghighlight(cut %in% c(&quot;Very Good&quot;, &quot;Ideal&quot;)) #this line highlights 7.4 Add labels to data points ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point()+ theme_classic()+ ggrepel::geom_text_repel(data = mtcars, aes(label = hp)) ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point() + geom_text(aes(label=hp, hjust=2.5, vjust=2.5)) #geom_label(aes(label = scales::comma(n)), size = 2.5, nudge_y = 6) 7.5 Plotting multiple graphs at once see::plots() is good for this. print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.6 Change the colors (bars; columns; dots; etc.) This can be done in at least two different ways, depending on your goal. To change the fill color by factor or group, add fill = ___ within the aes() command. If you want to add color and/or fill to a continuous variable, do that within the geom_density() command. If you want to add color and make all of the (bars; dots; lines; etc.) the same color, than that is a graph-wide control and needs to be put in geom_point(). This manually sets the color for the whole graph. # add a color scale to the dots ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(color=&quot;blue&quot;) If you want to add color that changes according to a variable (e.g., by factor level), then the color needs to be specified as a variable name, in the aes mapping with the other variables. ggplot(mtcars, aes(x= mpg, y=hp, color=cyl))+ geom_point() 7.6.1 Fine-tuning colors You can change the spectrum of colors to specific colors if you want. Useful for example, when making graphs for APLS presentations; you can change the colors to be Montclair State University themed. When changing the color scale of graphs, note that scale_fill commands are used for representing nominal data, while scale_color commands are for representing continuous data. As such, you use scale_fill to fill in area on a graph that shows a whole category or distinct things; and scale_color to use gradients of color to show changes in continuous data. For figures that have solid area (e.g., density; box; bar; violin plots; etc.), use scale_fill For figures that have continuous changes (e.g., line and scatter plots), use scale_color # Set colors manually ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_manual(values=c(&quot;green&quot;, &quot;yellow&quot;, &quot;orange&quot;, &quot;red&quot;, &quot;purple&quot;, &quot;blue&quot;)) ggplot(mtcars, aes(x = wt, y = mpg, color=as.factor(cyl)))+ geom_point() + scale_color_manual(values=c(&quot;#999999&quot;, &quot;#E69F00&quot;, &quot;#56B4E9&quot;)) # Use color scales from a package library(gameofthrones) # NOTICE THAT scale_fill AND scale_color STILL APPLY TO THEIR RESPECTIVE GRAPH TYPES # bar graphs ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Tully&quot;) ggplot(mtcars, aes(factor(cyl), fill=factor(vs)))+ geom_bar() + scale_fill_got(discrete = TRUE, option = &quot;Daenerys&quot;) # scatter plot ggplot(mtcars, aes(x = mpg, y = disp, colour = hp))+ geom_point(size = 2) + scale_colour_got(option = &quot;Lannister&quot;) Fill graphs also come with an extra option: Setting the outline color. You can change the outline of the bar/column/etc. by specifying the color inside geom_x() # change only the fill of the bars ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar() # Change the outline of the bars by adding color inside the geom_bar() command ggplot(mtcars, aes(factor(gear), fill=factor(carb)))+ geom_bar(color=&quot;black&quot;) 7.6.2 More options with the see package See this link for setting color gradients for continuous variables, or using other custom color palattes like the gameofthrones package. Check out the see package for some good color scales; the commands for which are here. Incidentally, see is great not only for regular ggplot graphs, but also Bayesian stats graphs link; effect size graphs link; correlation graphs link; and more. 7.7 Other aesthetic mappings shape() controls the shapes on the graph alpha() controls transparency size() controls size Note again that if you want it to change by variable, it goes INSIDE aes(); but if you want to set it manually for the whole graph, it goes in geom_x() # shape ggplot(mtcars, aes(x= mpg, y=hp, shape=as.factor(cyl)))+ geom_point() ggplot(mtcars, aes(x= mpg, y=hp))+ geom_point(shape=23) # transparency ggplot(mtcars, aes(x= mpg, y=hp, alpha=hp))+ geom_point() # size ggplot(mtcars, aes(x= mpg, y=hp, size=cyl))+ geom_point() 7.8 Adding and Customizing Text 7.8.1 Add a title, axis labels, and captions All three can be added with labs(). ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;) 7.8.2 Center graph title Add the line theme(plot.title = element_text(hjust = 0.5)) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5)) 7.8.3 Use different fonts See tutorial on this web page Or, use the extrafont package, and set everything using the theme() command. # Visualize new groups library(extrafont) loadfonts(device=&quot;win&quot;) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;)) 7.9 Remove gridlines Add theme(panel.grid = element_blank()) ggplot(mtcars, aes(x=cyl))+ geom_bar(colour=&quot;gray&quot;, fill=&quot;lightgreen&quot;)+ labs(title = &quot;Ages of Survey Respondants by Group&quot;, x=&quot;Age Group&quot;, caption=&quot;Note. Younger= ages 11-29; Older= ages 30-86.&quot;)+ theme(plot.title = element_text(hjust = 0.5))+ theme(axis.title = element_text(face = &quot;bold&quot;, family = &quot;Courier New&quot;, size = 12), axis.text = element_text(face = &quot;italic&quot;), plot.caption = element_text(face = &quot;italic&quot;, family = &quot;Calibri&quot;, size = 9), plot.title = element_text(face = &quot;bold&quot;,size = 14, family = &quot;Courier New&quot;))+ theme(panel.grid = element_blank()) 7.10 Faceting This is dividing one plot into subplots, in order to communicate relationships better. Again, this is just a single extra command, this time at the end of the code: facet_wrap(~columnhead) The tilde sign in R means by, as in divide (something) by this print(&quot;temp&quot;) This line produces a graph of population and life expectency, breaking it down to make a separate graph per each continent 7.11 Log transformations Sometimes when your data is really squished together on a graph it is hard to read. In this case, log transformations are really helpful, to change the scale of the data. For example, by multiplying all your points by 10x To create a log transformation of the same scatter plot above, add one extra bit: scale_x_log10() print(&quot;temp&quot;) You can also make both axis be logged by adding +scale again for y 7.12 Changing the scale of the axis Add coord_cartesian(xlim = c(lower,upper)) print(&quot;temp&quot;) ## [1] &quot;temp&quot; 7.13 Add a regression line Add the line geom_smooth(method = \"lm\", formula = y ~ x) ggplot(mtcars, aes(x= mpg, y=hp, color=mpg))+ geom_point()+ geom_smooth(method = &quot;lm&quot;, formula = y ~ x) "],["making-tables-with-flextable.html", "Chapter 8 Making Tables with flextable 8.1 APA Table Components 8.2 Indent values 8.3 Add a Horizontal border (AKA horizontal spanner) 8.4 Change font and font size 8.5 Grouped table 8.6 Complete Example", " Chapter 8 Making Tables with flextable NOTES: - j refers to the column - i refers to the row number 8.1 APA Table Components 8.2 Indent values https://davidgohel.github.io/flextable/reference/padding.html https://stackoverflow.com/questions/64134725/indentation-in-the-first-column-of-a-flextable-object Use the padding function: ft &lt;- padding(ft, i=2, j=1, padding.left=20) 8.3 Add a Horizontal border (AKA horizontal spanner) hline(., i=4, j=1:2, part = &quot;body&quot;) 8.4 Change font and font size glm_table&lt;-flextable::font(glm_table, part = &quot;all&quot;, fontname = &quot;Times&quot;) %&gt;% # Font fontsize(., size = 11, part = &quot;all&quot;) # Font size 8.5 Grouped table cars=rownames_to_column(mtcars, var = &quot;Model&quot;) test=flextable::as_grouped_data(x=cars, groups = c(&quot;cyl&quot;)) 8.6 Complete Example "],["misc.-stuff.html", "Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables 9.2 Read SPSS files into R 9.3 Turn numbers into percentages 9.4 Find all possible combindations of items in a vector 9.5 Download files from the internet 9.6 Print multiple things in one statement", " Chapter 9 Misc. Stuff 9.1 Scrape web pages for data tables Note. See Chapter 10s example purrr walk through for a guide on how to scrape multiple web tables simultaneously Simple example. library(rvest) ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding library(tidyverse) html=read_html(&#39;https://shop.tcgplayer.com/price-guide/pokemon/base-set&#39;) %&gt;% html_table(fill = TRUE) html ## [[1]] ## # A tibble: 101 x 6 ## PRODUCT Rarity Number `Market Price` `Listed Median` `` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Abra Common 43 $0.57 $0.55 View ## 2 Alakazam Holo Rare 1 $32.71  View ## 3 Arcanine Uncommon 23 $2.17 $2.09 View ## 4 Beedrill Rare 17 $3.41 $3.58 View ## 5 Bill Common 91 $0.35 $0.40 View ## 6 Blastoise Holo Rare 2 $105.78  View ## 7 Bulbasaur Common 44 $2.63 $2.97 View ## 8 Caterpie Common 45 $0.86 $0.85 View ## 9 Chansey Holo Rare 3 $24.10  View ## 10 Charizard Holo Rare 4 $349.99  View ## # ... with 91 more rows # Saved as a list by default. Now extract your table from said list html=as_tibble(html[[1]] %&gt;% # find out which number it is in the list select(&#39;PRODUCT&#39;,&#39;Rarity&#39;,&#39;Number&#39;,&#39;Market Price&#39;)) # if needed, specify which columns you want too html ## # A tibble: 101 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; ## 1 Abra Common 43 $0.57 ## 2 Alakazam Holo Rare 1 $32.71 ## 3 Arcanine Uncommon 23 $2.17 ## 4 Beedrill Rare 17 $3.41 ## 5 Bill Common 91 $0.35 ## 6 Blastoise Holo Rare 2 $105.78 ## 7 Bulbasaur Common 44 $2.63 ## 8 Caterpie Common 45 $0.86 ## 9 Chansey Holo Rare 3 $24.10 ## 10 Charizard Holo Rare 4 $349.99 ## # ... with 91 more rows # remove $ symbol in Price column to make it easier to work with html$`Market Price`=str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) html=html %&gt;% mutate(`Market Price`=as.numeric(`Market Price`)) # convert from string to numeric # view finished table head(html) ## # A tibble: 6 x 4 ## PRODUCT Rarity Number `Market Price` ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 Abra Common 43 0.57 ## 2 Alakazam Holo Rare 1 32.7 ## 3 Arcanine Uncommon 23 2.17 ## 4 Beedrill Rare 17 3.41 ## 5 Bill Common 91 0.35 ## 6 Blastoise Holo Rare 2 106. Slightly more complicated example Reading a table into R takes a few steps. Step 1 is to copy and paste the URL into the read_html() verb like below: pacman::p_load(rvest, tidyverse) exonerations_table=read_html(&quot;https://www.law.umich.edu/special/exoneration/Pages/detaillist.aspx&quot;) %&gt;% html_nodes(&quot;table.ms-listviewtable&quot;) %&gt;% html_table(fill=TRUE, header = TRUE) Sometimes if the web page is extremely basic and pretty much the only thing on it is a table, you can stop there. Most of the time though, there will be tons of other stuff on the website and you need to get more specific so R can find the table. This is the html_nodes() part of the above command; in there you specify the exact part of the web page where the table is located/what object file it is. To find this you will need to use the Developer mode in your browser. See this screenshot for an example knitr::include_graphics(here::here(&quot;pics&quot;, &quot;scrape.png&quot;)) In Firefox you open this by going to Settings &gt; More Tools &gt; Web Developer Tools (or CNTRL + Shift + I). Begin by looking through the console in the center bottom for names that look like they would be related to your table. A good place to start might be  , which contains the main body of the web page. Click on a name to expand it and see all the elements on the page contained there. Ultimately what youre looking for is what you see above: an element that, when selected, highlights ONLY the area of the web page youre looking for. To get at this you will need to keep expanding, highlighting, and clicking repeatedly.it can take some digging. Keep drilling down through page elements until you find the one that highlights the table and just the table. When you find this, look for the .ms file in that name; you should also see this in the smaller console box on the right. That is the file youll need. Write that name in the html_node command and read it into R. Thats stage 1. From here you now need to clean up the table. exonerations_table=as.data.frame(exonerations_table) # convert into a df Your table might be different, but this ones names were messed up when read in, so lets fix those first and then fix the rows and columns. # save the names to a vector table_names=exonerations_table$Last.Name[1:20] # Trim out the garbage rows and columns exonerations_table=exonerations_table %&gt;% select(Last.Name:Tags.1) %&gt;% slice(22:n()) # over-write incorrect col names with the vector of correct ones we saved above colnames(exonerations_table)=table_names # clean up names exonerations_table=exonerations_table %&gt;% janitor::clean_names() # verify structure of columns is correct # glimpse(exonerations_table) Yikes, a lot of stuff is stored incorrectly, and as a result theres some missing values that need to be addressed and other data that needs to be corrected. exonerations_table=as_tibble(exonerations_table) %&gt;% # convert to tibble mutate(across(c(dna,mwid:ild), na_if,&quot;&quot;)) %&gt;% # turn missing values into NA&#39;s mutate(across(c(dna,mwid:ild), replace_na, &quot;derp&quot;)) %&gt;% # replace NA&#39;s with a string (required for the next lines to work) mutate(dna=ifelse(dna==&quot;DNA&quot;,1,0), # change these variables from text to numeric to better facilitate analysis mwid=ifelse(mwid==&quot;MWID&quot;,1,0), fc=ifelse(fc==&quot;FC&quot;,1,0), p_fa=ifelse(p_fa==&quot;P/FA&quot;,1,0), f_mfe=ifelse(f_mfe==&quot;F/MFE&quot;,1,0)) %&gt;% mutate(across(c(st, crime, dna:f_mfe),factor)) # correct form by converting to factors And thats it! Check out final result! head(exonerations_table) ## # A tibble: 6 x 20 ## last_name first_name age race st county_of_crime tags om_tags crime sentence convicted exonerated dna x mwid fc p_fa f_mfe om ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; ## 1 Abbitt Joseph 31 Black NC Forsyth CV, ~ &quot;&quot; Chil~ Life 1995 2009 1 &quot;&quot; 1 0 0 0 derp ## 2 Abbott Cinque 19 Black IL Cook CIU,~ &quot;OF, W~ Drug~ Probati~ 2008 2022 0 &quot;&quot; 0 0 1 0 OM ## 3 Abdal Warith Hab~ 43 Black NY Erie IO, ~ &quot;OF, W~ Sexu~ 20 to L~ 1983 1999 1 &quot;&quot; 1 0 0 1 OM ## 4 Abernathy Christopher 17 White IL Cook CIU,~ &quot;OF, W~ Murd~ Life wi~ 1987 2015 1 &quot;&quot; 0 1 1 0 OM ## 5 Abney Quentin 32 Black NY New York CV &quot;&quot; Robb~ 20 to L~ 2006 2012 0 &quot;&quot; 1 0 0 0 derp ## 6 Acero Longino 35 Hisp~ CA Santa Clara NC, P &quot;&quot; Sex ~ 2 years~ 1994 2006 0 &quot;&quot; 0 0 0 0 derp ## # ... with 1 more variable: ild &lt;chr&gt; Check out this page for a quick overview. 9.2 Read SPSS files into R Use foreign::read.spss spss_version=foreign::read.spss(here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;, &quot;JLWOP_RYAN.sav&quot;), to.data.frame = TRUE) Might also want to add as_tibble() on the end. 9.3 Turn numbers into percentages Use scales::percent(), which converts normal numbers into percentages and includes the percent sign (%) afterwards simple_table=tribble(~n_people, ~votes_in_favor, 25, 14) simple_table=simple_table %&gt;% mutate(percent_voted_for=scales::percent(votes_in_favor/n_people, accuracy = 0.1, scale = 100)) simple_table ## # A tibble: 1 x 3 ## n_people votes_in_favor percent_voted_for ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 25 14 56.0% Scale is what to multiple the original number by (e.g., convert 0.05 to 5% by x100) Accuracy controls how many places out the decimal goes 9.4 Find all possible combindations of items in a vector y &lt;- c(2,4,6,8) combn(c(2,4,6,8),2) # find all possible combinations of these numbers, drawn two at a time ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 2 2 2 4 4 6 ## [2,] 4 6 8 6 8 8 9.5 Download files from the internet 9.6 Print multiple things in one statement Use cat() from base R cat(&quot;The p-value dropped below 0.05 for the first time as sample size&quot;, 100) ## The p-value dropped below 0.05 for the first time as sample size 100 "],["intermediate-r-functions-loops-and-iterative-programming.html", "Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions 10.2 For-loops 10.3 purrr and Iterative Functions 10.4 Other purrr commands", " Chapter 10 Intermediate R: Functions, Loops, and Iterative Programming 10.1 Functions A function is a command that performs a specified operation and returns an output in accordance with that operation. You can literally make a function to do anything you want. General structure of a basic function: # example structure Function_name=function(argument){ Expressions return(output) } Argument is your input. It is the thing you want to perform the operation on. Expressions is the actual operation (or operations) you want to perform on the supplied argument return tells R to return the result of the Expression to you when done. This example function takes an input of numbers in the form of a vector and subtracts two from each. numbers=c(2,10,12,80) sub_2=function(x){ result= x - 2 return(result) } sub_2(numbers) ## [1] 0 8 10 78 We can also supply the function with a single number and it still works sub_2(100) ## [1] 98 Well this looks useful. So whats the bigger picture? One of the primary advantages of functions are that they can reduce a long and complex process, or a process that involves many steps, into a single line of code; thus, creating your own functions is a fast way to make your life easier down the line either at some point in the far future or even in just a few minutes, if you know you will be writing the code for some process two or more times. Take this script for instance. You can see from the circled parts that I needed to transform three different data sets in a similar way: knitr::include_graphics(here::here(&quot;pics&quot;, &quot;repeat_process.jpg&quot;)) Yes, I could have just done a copy-paste of the original code and tweak it slightly each time. But that is time consuming, produces a sloppier and longer script, and introduces a lot more room for error because of the repeated code and extra steps. Better to write a single function that could be applied to all three. In short, use functions to reduce a multi-step process or a process that youre implementing &gt;=2 times in a single script into one command. This saves you space and makes the script shorter; it saves you the trouble and effort of re-writing or adapting code from earlier sections; and importantly, reduces the chances of you making a coding error by proxy of the former two. As a quick example, I was able to replace each of the circled paragraphs of code above with a custom function that ran everything in one simple line. Now instead of 3 whole (and redundant) paragraphs, I now have 3 short lines, like so. na_zero_helpreint=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;reintegrate_&quot;) na_blank=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) na_zero=rotate_data(data = na_zero_helpreint, variable_prefix = &quot;barrier_&quot;) Limitations to your average, everyday functions. While reducing a whole process or sequence of commands is extremely useful, it still leaves a limitation. For instance, while we avoided copying and pasting whole paragraphs or processes, I still had to copy-paste the same function three times. This still leaves chances for error on the table, and it still leaves us with wasted lines that make the script longer. In general, when you want to perform some function or process multiple times on multiple items (as above where the same command is used three times on three different data frames), you need to use a for-loop or iterating function. These can reduce further unwanted redundancies by applying the function or process iteratively. Read on for more info. 10.2 For-loops A for loop is essentially a function that applies a function or given set of operations to multiple things at once, and returns an output of many items. For example, this code finds the means of every vector/column in a dataset by repeatedly applying the same code over and over to element i in the given list: df &lt;- tibble( a = rnorm(10), b = rnorm(10), c = rnorm(10), d = rnorm(10) ) output &lt;- vector(&quot;double&quot;, ncol(df)) # 1.Output. Create the object you want the results of the loop stored in. for (i in seq_along(df)) { # 2.Sequence of operations. &quot;For each item &#39;i&#39; along data frame&quot; output[[i]] &lt;- median(df[[i]]) # 3.Body:&quot;every individual item in &#39;output&#39; = the median of each col in df } output ## [1] 0.3771802 -0.5176346 0.4171879 0.5704655 Check out this book chapter for a great and detailed explanation of for-loops and functional coding. Although for loops are nice, they are unwieldy. R programmers typically use iterating functions instead. Examples of iterating functions are the lapply, vapply, sapply, etc. family of base R commands. But these can also be confusing and the commands are not great. The purrr package offers a better way to do iterating functions over base R; its the tidyverse way to make efficient and understandable for loops! If you have a need for a for-loop for something, see the next section instead on how to use purrr to make an iterative function. Important to understand conceptually what a for-loop is, but using them is impractical when you have purrr 10.3 purrr and Iterative Functions All notes here come from Charlotte Wickhams lecture tutorial below Part 1: https://www.youtube.com/watch?v=7UlWJWfZO9M Part 2: https://www.youtube.com/watch?v=b0ozKTUho0A&amp;t=1210s purrrs map() series of functions offer a way to apply any existing function (even functions youve made) to multiple things at once, be it lists, data frame columns, individual items in vector, etc. In short, they are for doing the same type of task repeatedly in a very quick and efficient manner. They work in much the same way as for-loops, but are far simpler to write, and can be applied in the same way to solve the same problems. How to use purrr The structure of map() commands is the same as the others in the tidyverse: #option 1 map(data, function) # option 2 data %&gt;% map(function) As a quick example and to highlight why purrr is so much more efficient and easier to use than for-loops, look at the same example from before, now using map() instead of a for: df |&gt; map_dbl(median) ## a b c d ## 0.3771802 -0.5176346 0.4171879 0.5704655 A single line is all it took to get the same results! And, it follows tidyverse grammar structure. Now lets get into how it works. map() commands work like this: For each element of x, do f. So if you pass it object x and object x is. - A vector, it will perform function f on every item in the vector - A data frame, it will perform function f on every column in the data frame - A list, it will perform function f on every level in the list Etc., etc.; the point is it applies a function repeatedly to every element in the object you supply it with. So lets walk through a case example. 10.3.1 Reproducible example: Scraping web data This is an example walk through showing how we can use purrr to speed things up dramatically and/or reduce the use of unwanted, extra code in our scripts. In this guide Ill be building a table of LPGA Tour statistics from multiple webpages. The workflow for purrr goes like this: First, you want to figure out how to do each step of your process line-by-line, for a single item. The idea is to try and walk through each step of the process and see exactly what will need to be done each each step and what the code will like, before trying to code it all at once at a higher level. Once you have each step for the first item figured out, then you make functions for each step that condense that code down to one command. Lastly, apply each function from your individual steps to all items in your list by using purr::map(). Do for One library(rvest) # STEP 1 # Figure out a line-by-line process for one item/one single web page html1=read_html(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; html_table(fill = TRUE, header=TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() head(html1) ## rank name distance ## 1 1 Emily Pedersen 282.521 ## 2 2 Nanna Koerstz Madsen 280.160 ## 3 3 Lexi Thompson 278.100 ## 4 4 Yuka Saso 275.200 ## 5 5 Pauline Roussin-Bouchard 274.974 ## 6 6 Bianca Pagdanganan 274.923 # STEP 2 # create a custom function of the above to shorten and generalize the process quick_read_html=function(url){ web_page=read_html(url) |&gt; html_nodes(&quot;table.shsTable.shsBorderTable&quot;) |&gt; # fortunately this node works for all four pages so it can be baked into the function html_table(fill = TRUE, header = TRUE) |&gt; as.data.frame() |&gt; janitor::clean_names() return(web_page) } # test to verify it works test=quick_read_html(url= &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;) head(test) # nice ## rank name putt_average ## 1 1 Jeong Eun Lee 1.685 ## 2 2 Patty Tavatanakit 1.686 ## 3 3 Celine Boutier 1.709 ## 4 4 Danielle Kang 1.709 ## 5 5 Xiyu Lin 1.715 ## 6 6 Brooke Henderson 1.719 DO FOR ALL. Now create the object that contains all the elements you want to iterate over, and then pass it to your generalized function with map. # Step 3a # create an object that contains ALL elements of interest URLs=c(&quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=04&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=08&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=06&quot;, &quot;https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&amp;rank=12&quot;) # Step 4 # use the power of map and be amazed lpga_data= URLs |&gt; map(quick_read_html) head(lpga_data) ## [[1]] ## rank name distance ## 1 1 Emily Pedersen 282.521 ## 2 2 Nanna Koerstz Madsen 280.160 ## 3 3 Lexi Thompson 278.100 ## 4 4 Yuka Saso 275.200 ## 5 5 Pauline Roussin-Bouchard 274.974 ## 6 6 Bianca Pagdanganan 274.923 ## 7 7 Minjee Lee 274.750 ## 8 8 Charley Hull 274.222 ## 9 9 A Lim Kim 274.086 ## 10 10 Madelene Sagstrom 274.000 ## 11 11 Patty Tavatanakit 273.852 ## 12 12 Carlota Ciganda 273.543 ## 13 13 Atthaya Thitikul 273.065 ## 14 14 Maude-Aimee Leblanc 273.028 ## 15 15 Brooke Henderson 272.882 ## 16 16 Hannah Green 272.667 ## 17 17 Alana Uriell 272.643 ## 18 18 Brooke Matthews 272.313 ## 19 19 Janie Jackson 271.947 ## 20 20 Rachel Rohanna 271.897 ## 21 21 Angel Yin 271.771 ## 22 22 Yu Liu 271.417 ## 23 23 Perrine Delacour 271.315 ## 24 24 Jessica Korda 270.719 ## 25 25 Stephanie Meadow 270.275 ## 26 26 Yealimi Noh 269.630 ## 27 27 Maria Fassi 269.625 ## 28 28 Hyejin Choi 269.587 ## 29 29 Jennifer Kupcho 268.768 ## 30 30 Frida Kinhult 268.750 ## 31 31 Alison Lee 268.655 ## 32 32 Nelly Korda 268.636 ## 33 33 Amanda Doherty 267.750 ## 34 34 Gaby Lopez 267.600 ## 35 35 Sei Young Kim 267.571 ## 36 36 Amy Yang 267.540 ## 37 37 Gerina Mendoza 266.844 ## 38 38 Jeong Eun Lee 266.738 ## 39 39 Georgia Hall 266.196 ## 40 40 Ryann O&#39;Toole 266.148 ## 41 41 Cydney Clanton 266.133 ## 42 42 Fatima Fernandez Cano 265.955 ## 43 43 Sophia Schubert 265.036 ## 44 44 Giulia Molinaro 264.935 ## 45 45 Brittany Lincicome 264.676 ## 46 46 Xiyu Lin 264.667 ## 47 47 Albane Valenzuela 264.636 ## 48 48 Sung Hyun Park 264.583 ## 49 49 Lilia Vu 264.048 ## 50 50 Hyo Joo Kim 263.975 ## 51 51 Ally Ewing 263.611 ## 52 52 Pajaree Anannarukarn 263.455 ## 53 53 Sarah Schmelzel 263.190 ## 54 54 Lauren Coughlin 263.167 ## 55 55 Nasa Hataoka 263.121 ## 56 56 Jaye Marie Green 262.900 ## 57 57 Ruixin Liu 262.538 ## 58 58 Hinako Shibuno 262.229 ## 59 59 Annie Park 262.184 ## 60 60 Ariya Jutanugarn 261.712 ## 61 61 Katherine Perry-Hamski 261.600 ## 62 62 Jodi Ewart Shadoff 261.476 ## 63 63 Lauren Stephenson 261.405 ## 64 64 Mel Reid 261.370 ## 65 65 Na Rin An 261.098 ## 66 66 Sophia Popov 260.925 ## 67 67 Min Lee 260.900 ## 68 68 Eun-Hee Ji 260.857 ## 69 69 Stephanie Kyriacou 260.300 ## 70 70 Peiyun Chien 260.167 ## 71 71 Allisen Corpuz 260.038 ## 72 73 Haylee Harford 259.500 ## 73 74 Pernilla Lindberg 259.095 ## 74 75 Dewi Weber 259.028 ## 75 76 Linnea Johansson 259.000 ## 76 77 Lydia Ko 258.604 ## 77 78 Wei-Ling Hsu 258.528 ## 78 79 Isi Gabsa 258.433 ## 79 80 Elizabeth Nagel 258.333 ## 80 81 Jennifer Song 258.225 ## 81 82 Gemma Dryburgh 258.067 ## 82 83 Esther Henseleit 257.977 ## 83 84 Ruoning Yin 257.938 ## 84 85 Katherine Kirk 257.632 ## 85 86 Agathe Laisne 257.563 ## 86 87 Austin Ernst 257.472 ## 87 88 Casey Danielson 257.400 ## 88 89 Ana Belac 257.250 ## 89 90 Sarah Kemp 256.833 ## 90 91 Angela Stanford 256.786 ## 91 92 Ashleigh Buhai 256.500 ## 92 93 Celine Boutier 256.267 ## 93 94 Wichanee Meechai 256.019 ## 94 95 Amy Olson 255.879 ## 95 96 Paula Reto 255.857 ## 96 97 Yu-Sang Hou 255.563 ## 97 98 In Gee Chun 255.534 ## 98 99 Kaitlyn Papp 255.208 ## 99 100 Su-Hyun Oh 255.130 ## 100 101 Matilda Castren 255.071 ## 101 102 Jenny Shin 254.913 ## 102 103 Megan Khang 254.340 ## 103 104 Leona Maguire 254.138 ## 104 105 Jenny Coleman 253.712 ## 105 106 Cristie Kerr 253.450 ## 106 107 Jin Young Ko 253.375 ## 107 108 Kelly Tan 253.286 ## 108 109 Emma Talley 253.077 ## 109 110 Morgane Metraux 253.036 ## 110 111 So Yeon Ryu 253.031 ## 111 112 Brittany Altomare 253.000 ## 112 113 Savannah Vilaubi 252.917 ## 113 114 Pornanong Phatlum 252.816 ## 114 115 Cheyenne Knight 252.786 ## 115 116 Danielle Kang 252.448 ## 116 117 Jennifer Chang 252.278 ## 117 118 Marina Alex 252.148 ## 118 119 Hee Young Park 251.972 ## 119 120 Anna Nordqvist 251.957 ## 120 121 Muni He 251.583 ## 121 122 Jasmine Suwannapura 251.389 ## 122 123 Chella Choi 250.750 ## 123 124 Haeji Kang 250.607 ## 124 125 Bronte Law 250.471 ## 125 126 Ayaka Furue 250.093 ## 126 127 Brittany Lang 249.625 ## 127 128t Moriya Jutanugarn 248.500 ## 128 128t Lauren Kim 248.500 ## 129 130 Maddie Szeryk 247.583 ## 130 131 Mina Harigae 247.175 ## 131 132 Lindsey Weaver 247.043 ## 132 133 Lizette Salas 246.590 ## 133 134 In-Kyung Kim 246.063 ## 134 135 Mirim Lee 246.000 ## 135 136 Caroline Masson 245.929 ## 136 137 Allison Emrey 245.286 ## 137 138 Christina Kim 244.786 ## 138 139 Marissa Steen 243.773 ## 139 140 Yae Eun Hong 243.533 ## 140 141 Jiwon Jeon 243.250 ## 141 142 Inbee Park 242.907 ## 142 143 Na Yeon Choi 241.923 ## 143 144 Stacy Lewis 241.655 ## 144 145 Olivia Cowan 241.583 ## 145 146 Charlotte Thomas 241.536 ## 146 147 Aditi Ashok 236.413 ## 147 148 Vivian Hou 236.250 ## 148 149 Dana Finkelstein 234.133 ## ## [[2]] ## rank name putt_average ## 1 1 Jeong Eun Lee 1.685 ## 2 2 Patty Tavatanakit 1.686 ## 3 3 Celine Boutier 1.709 ## 4 4 Danielle Kang 1.709 ## 5 5 Xiyu Lin 1.715 ## 6 6 Brooke Henderson 1.719 ## 7 7 Su-Hyun Oh 1.728 ## 8 8 Jessica Korda 1.729 ## 9 9 Nanna Koerstz Madsen 1.735 ## 10 10 Minjee Lee 1.735 ## 11 11 Yuka Saso 1.737 ## 12 12 Amy Yang 1.741 ## 13 13 Haeji Kang 1.744 ## 14 14 Gemma Dryburgh 1.744 ## 15 15 Nasa Hataoka 1.746 ## 16 16 Leona Maguire 1.748 ## 17 17 Nelly Korda 1.750 ## 18 18 Lydia Ko 1.753 ## 19 19 Ruoning Yin 1.753 ## 20 20 Na Rin An 1.753 ## 21 21 Madelene Sagstrom 1.757 ## 22 22 Angel Yin 1.758 ## 23 23 Hyo Joo Kim 1.758 ## 24 24 Carlota Ciganda 1.759 ## 25 25 Jin Young Ko 1.761 ## 26 26 So Yeon Ryu 1.764 ## 27 27 Georgia Hall 1.766 ## 28 28 Atthaya Thitikul 1.768 ## 29 29 Eun-Hee Ji 1.772 ## 30 30 Brittany Altomare 1.775 ## 31 31 Lizette Salas 1.776 ## 32 32 Cristie Kerr 1.776 ## 33 33 Alison Lee 1.777 ## 34 34t Allisen Corpuz 1.778 ## 35 34t Lauren Stephenson 1.778 ## 36 36 Charley Hull 1.779 ## 37 37 Inbee Park 1.779 ## 38 38 Sei Young Kim 1.780 ## 39 39 Gaby Lopez 1.781 ## 40 40 Chella Choi 1.781 ## 41 41 Isi Gabsa 1.781 ## 42 42 Hinako Shibuno 1.782 ## 43 43 Lexi Thompson 1.782 ## 44 44 Pauline Roussin-Bouchard 1.782 ## 45 45 Ryann O&#39;Toole 1.783 ## 46 46 Perrine Delacour 1.785 ## 47 47 Stacy Lewis 1.786 ## 48 48 Aditi Ashok 1.787 ## 49 49 Megan Khang 1.790 ## 50 50 Janie Jackson 1.790 ## 51 51 Sarah Schmelzel 1.790 ## 52 52 Hyejin Choi 1.791 ## 53 53 Lauren Kim 1.793 ## 54 54 Hannah Green 1.793 ## 55 55 Paula Reto 1.794 ## 56 56 Pajaree Anannarukarn 1.795 ## 57 57 Ayaka Furue 1.795 ## 58 58 In-Kyung Kim 1.796 ## 59 59 Linnea Johansson 1.796 ## 60 60 Annie Park 1.797 ## 61 61 Min Lee 1.797 ## 62 62 Lilia Vu 1.797 ## 63 63 Stephanie Kyriacou 1.798 ## 64 64 Jennifer Kupcho 1.798 ## 65 65 In Gee Chun 1.799 ## 66 66 Yae Eun Hong 1.800 ## 67 67 Yu Liu 1.801 ## 68 68 Sung Hyun Park 1.801 ## 69 69 Jasmine Suwannapura 1.803 ## 70 70 Marina Alex 1.804 ## 71 71 Amanda Doherty 1.804 ## 72 72 Cheyenne Knight 1.804 ## 73 73 Maude-Aimee Leblanc 1.806 ## 74 74 Moriya Jutanugarn 1.806 ## 75 75 Stephanie Meadow 1.808 ## 76 76 Kaitlyn Papp 1.808 ## 77 77 Emma Talley 1.809 ## 78 78 A Lim Kim 1.809 ## 79 79 Ally Ewing 1.810 ## 80 80 Frida Kinhult 1.811 ## 81 81 Caroline Masson 1.816 ## 82 82 Lindsey Weaver 1.818 ## 83 83 Giulia Molinaro 1.818 ## 84 84 Ariya Jutanugarn 1.819 ## 85 85 Angela Stanford 1.819 ## 86 86 Cydney Clanton 1.820 ## 87 87 Jennifer Chang 1.820 ## 88 88 Hee Young Park 1.821 ## 89 89 Esther Henseleit 1.821 ## 90 90 Wei-Ling Hsu 1.821 ## 91 91 Charlotte Thomas 1.822 ## 92 92 Amy Olson 1.822 ## 93 93 Pernilla Lindberg 1.824 ## 94 94 Kelly Tan 1.824 ## 95 95 Alana Uriell 1.824 ## 96 97 Katherine Kirk 1.826 ## 97 98 Matilda Castren 1.827 ## 98 99 Yealimi Noh 1.827 ## 99 100 Jenny Shin 1.829 ## 100 101 Brittany Lincicome 1.831 ## 101 102 Sophia Popov 1.832 ## 102 103 Na Yeon Choi 1.833 ## 103 104 Ashleigh Buhai 1.834 ## 104 105 Mina Harigae 1.838 ## 105 106 Yu-Sang Hou 1.843 ## 106 107 Christina Kim 1.844 ## 107 108 Morgane Metraux 1.845 ## 108 109 Fatima Fernandez Cano 1.845 ## 109 110 Gerina Mendoza 1.847 ## 110 111 Agathe Laisne 1.847 ## 111 112 Dewi Weber 1.849 ## 112 113 Jodi Ewart Shadoff 1.850 ## 113 114 Bronte Law 1.851 ## 114 115 Albane Valenzuela 1.854 ## 115 116 Dana Finkelstein 1.855 ## 116 117 Allison Emrey 1.856 ## 117 118 Sophia Schubert 1.857 ## 118 119 Maria Fassi 1.859 ## 119 120 Ana Belac 1.861 ## 120 121 Mel Reid 1.861 ## 121 122 Jaye Marie Green 1.864 ## 122 123 Bianca Pagdanganan 1.865 ## 123 124 Austin Ernst 1.865 ## 124 125 Emily Pedersen 1.868 ## 125 126 Anna Nordqvist 1.869 ## 126 127 Wichanee Meechai 1.870 ## 127 128 Muni He 1.872 ## 128 129 Jennifer Song 1.873 ## 129 130 Jenny Coleman 1.873 ## 130 131 Rachel Rohanna 1.876 ## 131 132 Jiwon Jeon 1.876 ## 132 133 Brooke Matthews 1.878 ## 133 134 Mirim Lee 1.880 ## 134 135 Pornanong Phatlum 1.880 ## 135 136 Peiyun Chien 1.880 ## 136 137 Lauren Coughlin 1.884 ## 137 138 Katherine Perry-Hamski 1.885 ## 138 139 Haylee Harford 1.893 ## 139 140 Ruixin Liu 1.893 ## 140 141 Brittany Lang 1.898 ## 141 142 Sarah Kemp 1.911 ## 142 143 Marissa Steen 1.917 ## 143 144 Maddie Szeryk 1.923 ## 144 145 Olivia Cowan 1.932 ## 145 146 Savannah Vilaubi 1.953 ## 146 147 Casey Danielson 1.960 ## 147 148 Vivian Hou 1.984 ## 148 149 Elizabeth Nagel 1.985 ## ## [[3]] ## rank name greens_hit ## 1 1 Lexi Thompson 78.1 ## 2 2 Xiyu Lin 76.5 ## 3 3 Hannah Green 76.2 ## 4 4 Jodi Ewart Shadoff 75.7 ## 5 5 Minjee Lee 75.6 ## 6 6 Jennifer Kupcho 74.8 ## 7 7 Marina Alex 74.5 ## 8 8 Celine Boutier 74.4 ## 9 9 Hinako Shibuno 74.2 ## 10 10 A Lim Kim 74.1 ## 11 11 Hyejin Choi 73.9 ## 12 12 Yealimi Noh 73.9 ## 13 13t Nanna Koerstz Madsen 73.8 ## 14 13t Amy Yang 73.8 ## 15 15 Brooke Henderson 73.7 ## 16 16t Hyo Joo Kim 73.6 ## 17 16t Emily Pedersen 73.6 ## 18 18 Jeong Eun Lee 73.5 ## 19 19 Matilda Castren 73.2 ## 20 20 Maude-Aimee Leblanc 73.1 ## 21 21 Jessica Korda 72.9 ## 22 22 Megan Khang 72.9 ## 23 23 Jin Young Ko 72.6 ## 24 24 Charley Hull 72.5 ## 25 25t In Gee Chun 72.4 ## 26 25t Danielle Kang 72.4 ## 27 27t Pajaree Anannarukarn 72.2 ## 28 27t Sophia Schubert 72.2 ## 29 29 Sarah Schmelzel 72.0 ## 30 30 Chella Choi 72.0 ## 31 31 Perrine Delacour 71.8 ## 32 32 Atthaya Thitikul 71.7 ## 33 33t Nasa Hataoka 71.5 ## 34 33t Ariya Jutanugarn 71.5 ## 35 35 Haylee Harford 71.5 ## 36 36 Brittany Altomare 71.5 ## 37 37 Ally Ewing 71.3 ## 38 38 Inbee Park 70.8 ## 39 39 Stacy Lewis 70.7 ## 40 40 Na Rin An 70.6 ## 41 41 So Yeon Ryu 70.5 ## 42 42 Carlota Ciganda 70.0 ## 43 43 Anna Nordqvist 69.8 ## 44 44 Ayaka Furue 69.3 ## 45 45 Jenny Shin 69.3 ## 46 46 Gaby Lopez 69.3 ## 47 47 Allisen Corpuz 69.2 ## 48 48 Lydia Ko 69.2 ## 49 49t Leona Maguire 69.2 ## 50 49t Yuka Saso 69.2 ## 51 51t Wei-Ling Hsu 69.1 ## 52 51t Ryann O&#39;Toole 69.1 ## 53 53 Lauren Stephenson 69.0 ## 54 54 Jenny Coleman 69.0 ## 55 55 Casey Danielson 68.9 ## 56 56 Austin Ernst 68.8 ## 57 58 Patty Tavatanakit 68.7 ## 58 59 Nelly Korda 68.7 ## 59 60 Sei Young Kim 68.7 ## 60 61 Olivia Cowan 68.5 ## 61 62 Madelene Sagstrom 68.5 ## 62 63 Pauline Roussin-Bouchard 68.4 ## 63 64 Min Lee 68.3 ## 64 65 Moriya Jutanugarn 68.2 ## 65 66 Emma Talley 68.2 ## 66 67 Janie Jackson 68.1 ## 67 68t In-Kyung Kim 68.1 ## 68 68t Lizette Salas 68.1 ## 69 70 Jasmine Suwannapura 67.9 ## 70 71t Cydney Clanton 67.8 ## 71 71t Isi Gabsa 67.8 ## 72 71t Katherine Perry-Hamski 67.8 ## 73 74 Lilia Vu 67.7 ## 74 75 Esther Henseleit 67.7 ## 75 76 Giulia Molinaro 67.6 ## 76 77t Lauren Coughlin 67.6 ## 77 77t Yu Liu 67.6 ## 78 77t Kaitlyn Papp 67.6 ## 79 80 Annie Park 67.5 ## 80 81t Pernilla Lindberg 67.5 ## 81 81t Caroline Masson 67.5 ## 82 83 Albane Valenzuela 67.4 ## 83 84 Ruoning Yin 67.4 ## 84 85 Jaye Marie Green 67.2 ## 85 86 Paula Reto 66.9 ## 86 87 Ashleigh Buhai 66.9 ## 87 88t Sarah Kemp 66.7 ## 88 88t Wichanee Meechai 66.7 ## 89 88t Su-Hyun Oh 66.7 ## 90 88t Marissa Steen 66.7 ## 91 92 Stephanie Meadow 66.4 ## 92 93 Eun-Hee Ji 66.3 ## 93 94 Bianca Pagdanganan 66.2 ## 94 95 Kelly Tan 66.1 ## 95 96 Georgia Hall 65.9 ## 96 97 Peiyun Chien 65.7 ## 97 98t Bronte Law 65.7 ## 98 98t Brittany Lincicome 65.7 ## 99 100t Rachel Rohanna 65.6 ## 100 100t Jennifer Song 65.6 ## 101 102 Alana Uriell 65.5 ## 102 103 Dewi Weber 65.4 ## 103 104 Alison Lee 65.3 ## 104 105 Muni He 65.3 ## 105 106 Fatima Fernandez Cano 65.2 ## 106 107 Frida Kinhult 65.1 ## 107 108 Yae Eun Hong 64.8 ## 108 109 Charlotte Thomas 64.7 ## 109 110 Sophia Popov 64.4 ## 110 111 Amy Olson 64.4 ## 111 112 Mel Reid 64.3 ## 112 113 Ruixin Liu 64.1 ## 113 114 Morgane Metraux 63.9 ## 114 115 Gerina Mendoza 63.5 ## 115 116t Haeji Kang 63.5 ## 116 116t Cheyenne Knight 63.5 ## 117 118 Pornanong Phatlum 63.5 ## 118 119t Elizabeth Nagel 63.0 ## 119 119t Sung Hyun Park 63.0 ## 120 121 Ana Belac 62.7 ## 121 122 Maria Fassi 62.5 ## 122 123 Lindsey Weaver 62.3 ## 123 124 Linnea Johansson 61.9 ## 124 125 Jiwon Jeon 61.8 ## 125 126 Jennifer Chang 61.7 ## 126 127 Mina Harigae 61.7 ## 127 128t Dana Finkelstein 61.1 ## 128 128t Christina Kim 61.1 ## 129 128t Brittany Lang 61.1 ## 130 128t Angel Yin 61.1 ## 131 132 Gemma Dryburgh 60.7 ## 132 133 Amanda Doherty 60.7 ## 133 134t Katherine Kirk 60.6 ## 134 134t Stephanie Kyriacou 60.6 ## 135 136 Hee Young Park 60.2 ## 136 137 Aditi Ashok 60.1 ## 137 138 Cristie Kerr 59.4 ## 138 139 Savannah Vilaubi 59.3 ## 139 140 Agathe Laisne 59.0 ## 140 141 Na Yeon Choi 59.0 ## 141 142 Vivian Hou 58.3 ## 142 143 Allison Emrey 57.9 ## 143 144t Yu-Sang Hou 57.6 ## 144 144t Mirim Lee 57.6 ## 145 146 Angela Stanford 57.1 ## 146 147t Lauren Kim 56.9 ## 147 147t Brooke Matthews 56.9 ## 148 149 Maddie Szeryk 48.1 ## ## [[4]] ## rank name rounds score_average_actual ## 1 1 Minjee Lee 20 69.050 ## 2 2 Hyo Joo Kim 20 69.300 ## 3 3 Nanna Koerstz Madsen 25 69.320 ## 4 4 Brooke Henderson 26 69.423 ## 5 5 Celine Boutier 31 69.452 ## 6 6 Xiyu Lin 27 69.519 ## 7 7 Lexi Thompson 15 69.533 ## 8 8 Danielle Kang 29 69.655 ## 9 9 Jin Young Ko 16 69.688 ## 10 10 Amy Yang 25 69.720 ## 11 11 Atthaya Thitikul 31 69.774 ## 12 12 Hannah Green 24 69.917 ## 13 13 Patty Tavatanakit 27 69.963 ## 14 14 Nelly Korda 11 70.000 ## 15 15 Hyejin Choi 23 70.087 ## 16 16 Nasa Hataoka 33 70.242 ## 17 17 Yuka Saso 30 70.333 ## 18 18 Leona Maguire 29 70.345 ## 19 19 Lydia Ko 24 70.375 ## 20 20 In Gee Chun 29 70.379 ## 21 21 Jeong Eun Lee 21 70.381 ## 22 22 Jessica Korda 16 70.438 ## 23 23 Charley Hull 18 70.500 ## 24 24 Hinako Shibuno 24 70.542 ## 25 25 Madelene Sagstrom 31 70.548 ## 26 26 Lizette Salas 20 70.550 ## 27 27 Marina Alex 27 70.630 ## 28 28 Inbee Park 27 70.667 ## 29 29 Pajaree Anannarukarn 33 70.697 ## 30 30 Chella Choi 24 70.708 ## 31 31 Alison Lee 29 70.724 ## 32 32 Brittany Altomare 29 70.759 ## 33 33 Jennifer Kupcho 28 70.786 ## 34 34t Stacy Lewis 29 70.862 ## 35 34t Sarah Schmelzel 29 70.862 ## 36 36 Georgia Hall 23 70.870 ## 37 37 Ryann O&#39;Toole 27 70.889 ## 38 38 Na Rin An 21 70.905 ## 39 39 A Lim Kim 29 70.931 ## 40 40 Allisen Corpuz 13 71.000 ## 41 41 So Yeon Ryu 16 71.063 ## 42 42 Megan Khang 25 71.120 ## 43 43 Gaby Lopez 30 71.167 ## 44 44 Perrine Delacour 27 71.185 ## 45 45 Sei Young Kim 14 71.214 ## 46 46 Ayaka Furue 27 71.259 ## 47 47 Gemma Dryburgh 15 71.267 ## 48 48 Su-Hyun Oh 23 71.304 ## 49 49t Yealimi Noh 27 71.333 ## 50 49t Lilia Vu 21 71.333 ## 51 51 Carlota Ciganda 23 71.348 ## 52 52 Ariya Jutanugarn 33 71.424 ## 53 53 Emma Talley 26 71.462 ## 54 54 Pauline Roussin-Bouchard 19 71.474 ## 55 55 Maude-Aimee Leblanc 18 71.500 ## 56 56 Jasmine Suwannapura 27 71.519 ## 57 58 Isi Gabsa 15 71.600 ## 58 59 Aditi Ashok 23 71.609 ## 59 60t Cheyenne Knight 21 71.619 ## 60 60t Paula Reto 21 71.619 ## 61 62t Jodi Ewart Shadoff 21 71.667 ## 62 62t Caroline Masson 21 71.667 ## 63 64 Moriya Jutanugarn 29 71.690 ## 64 65 Emily Pedersen 24 71.708 ## 65 66t Matilda Castren 28 71.714 ## 66 66t Eun-Hee Ji 14 71.714 ## 67 68 Min Lee 20 71.750 ## 68 69 Wei-Ling Hsu 18 71.778 ## 69 70 Jenny Shin 23 71.783 ## 70 71 Stephanie Meadow 20 71.800 ## 71 72t Lauren Stephenson 21 71.810 ## 72 72t Kelly Tan 21 71.810 ## 73 74 Annie Park 19 71.842 ## 74 75 Kaitlyn Papp 12 71.917 ## 75 76t Giulia Molinaro 23 72.000 ## 76 76t Charlotte Thomas 14 72.000 ## 77 78 Yu Liu 24 72.083 ## 78 79 In-Kyung Kim 8 72.125 ## 79 80 Yae Eun Hong 15 72.133 ## 80 81 Lindsey Weaver 23 72.174 ## 81 82 Bronte Law 17 72.235 ## 82 83 Janie Jackson 19 72.263 ## 83 84 Albane Valenzuela 22 72.273 ## 84 85 Sophia Schubert 14 72.286 ## 85 86 Mina Harigae 20 72.300 ## 86 87 Anna Nordqvist 23 72.304 ## 87 88 Ally Ewing 18 72.333 ## 88 89 Alana Uriell 14 72.357 ## 89 90 Pernilla Lindberg 21 72.381 ## 90 91 Frida Kinhult 14 72.429 ## 91 92 Austin Ernst 18 72.444 ## 92 93 Esther Henseleit 22 72.455 ## 93 94 Brittany Lincicome 17 72.471 ## 94 95 Ruoning Yin 8 72.500 ## 95 96 Dewi Weber 18 72.556 ## 96 97 Ashleigh Buhai 22 72.591 ## 97 98 Jaye Marie Green 20 72.600 ## 98 99 Ruixin Liu 13 72.615 ## 99 100 Wichanee Meechai 27 72.630 ## 100 101 Ana Belac 14 72.643 ## 101 102t Jennifer Chang 9 72.667 ## 102 102t Angel Yin 18 72.667 ## 103 104 Mel Reid 23 72.696 ## 104 105 Dana Finkelstein 15 72.733 ## 105 106 Pornanong Phatlum 19 72.737 ## 106 107 Sophia Popov 20 72.850 ## 107 108 Haeji Kang 14 72.857 ## 108 109 Rachel Rohanna 15 72.867 ## 109 110 Amy Olson 17 72.882 ## 110 111t Jenny Coleman 26 72.923 ## 111 111t Bianca Pagdanganan 13 72.923 ## 112 113 Muni He 12 73.000 ## 113 114t Cydney Clanton 15 73.067 ## 114 114t Katherine Perry-Hamski 15 73.067 ## 115 116 Jennifer Song 20 73.100 ## 116 117 Haylee Harford 8 73.125 ## 117 118 Amanda Doherty 14 73.143 ## 118 119t Linnea Johansson 15 73.200 ## 119 119t Katherine Kirk 10 73.200 ## 120 121t Lauren Coughlin 12 73.250 ## 121 121t Gerina Mendoza 16 73.250 ## 122 123 Christina Kim 14 73.286 ## 123 124 Stephanie Kyriacou 10 73.300 ## 124 125 Sarah Kemp 15 73.333 ## 125 126 Marissa Steen 11 73.364 ## 126 127t Casey Danielson 10 73.500 ## 127 127t Cristie Kerr 10 73.500 ## 128 129 Brooke Matthews 8 73.625 ## 129 130 Morgane Metraux 14 73.643 ## 130 131t Peiyun Chien 12 73.667 ## 131 131t Hee Young Park 18 73.667 ## 132 133 Sung Hyun Park 12 73.750 ## 133 134 Maria Fassi 12 73.833 ## 134 135 Angela Stanford 14 73.857 ## 135 136 Agathe Laisne 8 73.875 ## 136 137 Fatima Fernandez Cano 11 73.909 ## 137 138 Jiwon Jeon 8 74.000 ## 138 139 Yu-Sang Hou 8 74.375 ## 139 140t Allison Emrey 14 74.500 ## 140 140t Brittany Lang 8 74.500 ## 141 142 Lauren Kim 8 74.625 ## 142 143 Olivia Cowan 6 74.667 ## 143 144 Na Yeon Choi 13 75.154 ## 144 145 Mirim Lee 16 75.250 ## 145 146 Savannah Vilaubi 6 76.000 ## 146 147 Maddie Szeryk 6 76.333 ## 147 148 Elizabeth Nagel 6 76.833 ## 148 149 Vivian Hou 6 77.000 All done!! And just like that, weve downloaded four different web pages, extracted the tabled info, and formatted them without copying and pasting any code. The same process for all four was only used one time to write the initial function. Just apply some final formatting to clean it up a bit and combine the separate data frames into a single, unified one. lpga_data= lpga_data %&gt;% reduce(left_join, by=&quot;name&quot;) %&gt;% # Combine all list levels into a single tibble, matching by the &quot;Name&quot; column select(-contains(&quot;rank.&quot;)) |&gt; rename(&quot;score_average&quot;=&quot;score_average_actual&quot;) # VOILA! head(lpga_data) ## name distance putt_average greens_hit rounds score_average ## 1 Emily Pedersen 282.521 1.868 73.6 24 71.708 ## 2 Nanna Koerstz Madsen 280.160 1.735 73.8 25 69.320 ## 3 Lexi Thompson 278.100 1.782 78.1 15 69.533 ## 4 Yuka Saso 275.200 1.737 69.2 30 70.333 ## 5 Pauline Roussin-Bouchard 274.974 1.782 68.4 19 71.474 ## 6 Bianca Pagdanganan 274.923 1.865 66.2 13 72.923 10.3.2 Non-reproducible example (Juvenile Life Without Parole study) In the Juvenile Lifers study, there were a series of questions that participants rated on a scale of 0-100 in terms of difficulty. Part of our analysis involved taking the ratings on those variables and giving them relative rankings, so that each of the 6 variables in the series was rated from the least to most difficult, by participant. Now if we only needed to compute these rankings once this wouldnt have been any big deal; however, we needed to do it three times. Much of the same code and the same process would need to be copied and pasted, resulting in a very long, messy, harder to read script. With purrr however, we can reduce the redundancies to a minimum, saving time and reducing the chances of mistakes. Step 1. Just like before, the first step is to find a line-by-line solution for a single item, and then to generalize this into a shortcut function that can be applied to the any item i in a series of items. For the sake of brevity, Im going to skip most of that and just include the functions below. load(&quot;C:/Github Repos/Studies/JLWOP/Data and Models/jlwop_reentry_survey.RData&quot;) #### CREATE THE DATA SETS WE NEED#### na_blank=jlwop_reentry_survey # analysis 1 keeps the data as-is na_zero=jlwop_reentry_survey %&gt;% # supplementary analysis replaces the NA&#39;s with 0 mutate(across(c(barrier_housing:barrier_identification), replace_na,0)) rm(jlwop_reentry_survey) # remove old data set to avoid confusion #### Functions #### # transformation function to wrangle the data into proper formatting rotate_data=function(data, variable_prefix){ data=data %&gt;% pivot_longer( cols= starts_with(variable_prefix), # collect all the desired variables (i.e., columns).... names_to = &quot;variable&quot;, #...and put them into a new categorical variable called &quot;variable&quot; values_to = &quot;participant_score&quot;) %&gt;% # ...and store their values in a new variable called &quot;participant_score&quot; arrange(unique,participant_score) %&gt;% select(c(unique, participant_score, variable)) %&gt;% # keep only these 3 variables relocate(variable, .before = participant_score) # put the newly created variable up front return(data) } # creating the rankings for each variable; then transform data back to original structure rank_and_unpivot=function(data){ data=data %&gt;% group_by(unique) %&gt;% # group the scores so they can be ranked by participant mutate(rank1=dense_rank(participant_score), # create ranking variable rank=max(rank1,na.rm = TRUE) + 1 - rank1) %&gt;% # fix ranks by flipping to ascending order mutate(rank=factor(rank)) %&gt;% # convert rank to factor structure select(-rank1) # Pivot back to wide data=data %&gt;% pivot_wider(names_from = variable, values_from = rank:participant_score) %&gt;% ungroup() # un-group the data and delete the generated names return(data) } Step 2. Again, like before, we want to combine all elements of interest into some object. Once we have that, we then pass said object to map() and supply the map call with our custom function. dfs=list(na_blank=na_blank, na_zero=na_zero) %&gt;% # create lists map(.f=rotate_data, variable_prefix = &quot;barrier&quot;) %&gt;% # apply custom function along whole list map(rank_and_unpivot) # again!! DO IT AGAIN! With another function this time. # extract list elements to make them data frames again list2env(dfs, globalenv()) rm(dfs) #discard list. It has fulfilled its purpose. And just like that, were done! 10.3.3 Example 3: Read/Import several files at once with map() Multiple ways you can do this. ################### Option 1: read all into the global environment, keeping them as SEPERATE df&#39;s ############### legaldmlab::read_all(path=&quot;Data Repository/Stats Data Repository/JASP files&quot;, extension = &quot;.csv&quot;) # Option 1.A: Squish ALL OBJECTS in the working environment into a list # Again, note the &quot;ALL OBJECTS&quot; part; make sure there are no functions or other things in the environment when you run this. files=mget(ls()) ############# Option 2: Read in all files as a LIST of df&#39;s, then stitch in the names ##################################### files=paste0(here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;, &quot;/&quot;), list.files(path=here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;), pattern = &quot;.csv&quot;)) files_list=files |&gt; map(readr::read_csv) names(files_list)=file.path(here::here(&quot;Data Repository&quot;, &quot;Stats Data Repository&quot;, &quot;JASP files&quot;)) |&gt; #specify file path as a string list.files(pattern = &quot;.csv&quot;) |&gt; # pass the path string to list files; search in this location for files with this extension gsub(pattern=&quot;.csv&quot;, replacement = &quot;&quot;) # remove this pattern to save only the name # Option 2.A: Extract each data frame and put everything into the global environment list2env(cog_data, globalenv()) 10.4 Other purrr commands Note that map() always returns a list, and depending on the output that you want, you may need to use a variation of map(). These variations are as follows: Command Return map_lgl() logical vector map_int() integer vector map_dbl() double vector map_chr() character vector walk() only returns the side effects of a function 10.4.1 The walk and walk2 commands Walk() is useful for when you just want to plot something or write a save file to your disk, etc. It does not give you any return to store something in the environment. You use it to write/read files, open graphics windows, and so on. Example: Writing multiple files at once Utilize purrr::walk2() to apply a function iteratively on TWO objects simultaneously. To save multiple .csv files with walk2, we need two distinct lists: 1. A list of data frames that we wish to export, 2. and the file paths, complete with the file names and extensions, for each file to be written First create and define both list items. Then apply walk2() to pluck an element from list 1 and its corresponding element from list 2, and apply the write_csv function in for-loop fashion. ### Custom function #### # Create needed function that grabs file names and stitches them together with the correct path and extension # Included in legaldmlab package bundle_paths=function(df_list, output_location, file_type){ names=names(df_list) paths=rep(here::here(output_location), length(names)) extension=rep(c(file_type), length(names)) fixed_names=paste0(&quot;/&quot;,names) path_bundle=list(paths,fixed_names, extension) %&gt;% pmap(., paste0) return(path_bundle) } #### Exporting the .csv files for SPSS/JASP/etc. #### # Define list 1 dfs=list(na_blank=na_blank, na_zero=na_zero, na_zero_helpreint=na_zero_helpreint) # list 2 paths_csv=bundle_paths(df_list = dfs, folder_location = &quot;JLWOP/Data and Models&quot;, file_type = &quot;.sav&quot;) # Iterate over all elements in list 1 and corresponding element in list 2; # and apply the the write_csv function to each walk2(.x=dfs, .y= paths, .f=haven::write_sav) #### .RData file for R users #### # Combine multiple data frames into a single .RData file and export save(list = c(&quot;na_blank&quot;, &quot;na_zero&quot;, &quot;na_zero_helpreint&quot;), file = here::here(&quot;JLWOP&quot;, &quot;Data and Models&quot;,&quot;ranking_data.RData&quot;)) 10.4.2 map2 (and walk2) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_a.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;map2_b.png&quot;)) 10.4.3 pmap for when you have a bunch of shit This function is for iterating over three or more elements. As soon as you have &gt;2 items you have to iterate over, you need pmap().It works similar to to map and map2, but instead of iterating over a single object x or two objects x and y, it acts on a list object called .l The list is a list of all the objects you want to iterate over. If you give it a list of 18 items, it iterates over all 18. If the list only has two things, it only acts on those two. She says its easiest to imagine the list as a data frame, and the columns of the data frame like the elements of that list. knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap.png&quot;)) knitr::include_graphics(here::here(&quot;pics&quot;, &quot;pmap_2.png&quot;)) "],["intro-to-r-markdown.html", "Chapter 11 Intro to R Markdown 11.1 Important code chunk options 11.2 Writing math equations and symbols 11.3 Including graphics/inserting pictures 11.4 Footnotes 11.5 Change the color of your text 11.6 Re-using code chunk options 11.7 Making better tables 11.8 Running in-line code", " Chapter 11 Intro to R Markdown R Markdown is a better and more organized way to write scripts. Seriously, once you learn it, theres no going back. New and dont know where to start? Read The R Markdown Cookbook. Amazing overview with tons of neat tricks and how-tos. This other source may also be of some help. Below are some quick tips for common tasks; but be sure to read the Cookbook above. 11.1 Important code chunk options cache: TRUE or FALSE. Do you want to save the output of the chunk so it doesnt have to run next time? Creates a cached folder in the directory. eval: Do you want to evaluate (i.e., run) the code in the chunk? echo: Do you want to print the code after its run? include: Do you want to include code output in the final output document? Setting to FALSE means the code does not appear in the output document, but it is still run. 11.2 Writing math equations and symbols 11.2.1 Greek symbols A few notes first: Math notation is done with dollar signs and forward slashes For Greek letters, just type the name of the letter: $\\mu$ for \\(\\mu\\) $\\sigma$ for \\(\\sigma\\) $\\alpha$ for \\(\\alpha\\) $\\pi$ for \\(\\pi\\) $\\rho$ for \\(\\rho\\) 11.2.2 Math notation $\\pm$ for Â± $\\ge$ for  $\\le$ for  $\\neq$ for  11.2.3 Statistics notation 11.2.4 Writing in-line code Use the funny looking symbol on the tilde key that looks like this: ` To write in line, code, put one of those symbols on either side of the code, like you would with quotation marks. Helps you write lines like: I love dplyr 11.3 Including graphics/inserting pictures The default method doesnt work for me for some reason, but you can still insert images using a combination of the here package and knitr. Use the include_graphics() command and specify both the file location and its name: knitr::include_graphics(here::here(&quot;pics&quot;,&quot;snapchat.png&quot;)) NOTE. Use 3000-600 DPI to get good looking pictures. The bookdown book notes that: The syntax for controlling the image attributes is the same as when images are generated from R code. Chunk options fig.cap, out.width, and fig.show still have the same meanings. and: You can easily scale these images proportionally using the same ratio. This can be done via the dpi argument (dots per inch), which takes the value from the chunk option dpi by default If it is a numeric value and the chunk option out.width is not set, the output width of an image will be its actual width (in pixels) divided by dpi , and the unit will be inches. For example, for an image with the size 672 x 480, its output width will be 7 inches ( 7in ) when dpi=96. This feature requires the package png and/or jpeg to be installed. You can always override the automatic calculation of width in inches by providing a non-NULL value to the chunk option out.width , or use include_graphics(dpi = NA) 11.4 Footnotes To add a footnote, use the ^ symbol and put the note in brackets: You can also write footnotes1 like this. 11.5 Change the color of your text YOUR TEXT HERE 11.6 Re-using code chunk options https://yihui.org/en/2021/05/knitr-reuse/ 11.7 Making better tables https://rfortherestofus.com/2019/11/how-to-make-beautiful-tables-in-r/ 11.8 Running in-line code To run code in the middle of a sentence, you create a mini code chunk inside the sentence. For example: &gt; There are 2x2 apples in the basket Could be typed as There are 4 apples in the basket Kruschke, J. (2015). Goals, power, and sample size. In J. K. Kruschke (Ed.), Doing bayesian data analysis: A tutorial with r, jags, and stan (2nd ed., pp.Â 359-398). Academic Press. "],["statistics-and-psych-specific-stuff.html", "Chapter 12 Statistics and Psych-specific Stuff 12.1 Create or sample from a distribution 12.2 Find Cohens Kappa (Interrater reliability) 12.3 Statistical tests and modeling with easystats", " Chapter 12 Statistics and Psych-specific Stuff 12.1 Create or sample from a distribution Creating a binomial distribution When you do this, you are setting the true population parameter; you are in control of the Data Generating Process and the true distribution In a binomial distribution, the parameter is normally distributed, and can take any value from 0.0 to 1.0 But the data that this process generates is not normal rbinom(n= 1000, size= 1, prob = 0.5) ## [1] 0 1 0 1 1 0 1 0 0 1 0 0 0 0 1 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 ## [72] 1 0 0 1 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 1 0 ## [143] 1 1 1 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 0 0 0 1 0 1 0 0 0 0 0 1 1 0 0 1 1 1 1 1 0 0 0 0 0 1 1 1 0 0 1 1 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 1 1 1 0 1 0 ## [214] 1 0 0 1 1 0 1 0 1 0 1 0 1 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 0 1 ## [285] 1 0 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 0 1 1 1 0 0 0 1 0 0 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 1 1 1 1 1 0 1 0 1 1 1 1 1 0 1 0 1 ## [356] 0 0 0 0 1 0 0 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 1 1 1 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 0 0 1 1 1 1 1 0 0 0 1 0 1 0 0 1 0 0 0 ## [427] 1 1 1 0 0 0 0 0 0 1 1 0 1 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 1 0 1 1 1 1 1 1 1 0 1 1 0 0 ## [498] 0 0 1 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 1 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 1 1 0 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 0 ## [569] 1 0 1 1 0 0 0 0 0 0 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0 0 0 1 0 1 1 1 1 0 1 1 1 0 0 0 0 0 0 1 0 1 0 1 1 1 0 1 1 1 0 0 0 0 1 0 0 0 1 0 1 0 1 0 0 0 0 ## [640] 1 1 1 1 0 0 1 0 1 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 0 0 1 1 1 0 1 1 1 0 0 1 0 0 0 0 0 1 0 1 1 ## [711] 0 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 1 1 0 0 1 1 0 1 0 ## [782] 0 0 0 0 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 1 0 1 0 0 0 0 1 1 0 1 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 1 0 1 0 1 1 0 1 1 ## [853] 0 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 0 1 1 0 0 1 0 0 1 1 1 1 1 1 0 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 0 1 1 0 1 1 1 1 1 0 0 0 1 1 1 0 0 0 0 0 1 1 0 0 ## [924] 0 0 1 1 0 1 1 1 0 1 1 0 1 1 0 1 0 1 1 1 0 1 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 0 ## [995] 0 0 0 0 1 1 rnorm(n=2500,mean=500, sd=100) ## [1] 690.7642 503.0016 446.5626 484.7566 514.2144 558.1073 372.0421 486.2422 445.8801 507.1753 582.8456 556.2646 505.3560 457.7520 552.2014 ## [16] 443.2911 516.2216 507.5774 496.2267 700.0870 486.4038 544.2314 610.0555 490.9881 564.8317 588.5202 438.1985 365.3048 467.5639 521.1525 ## [31] 617.3123 554.5895 489.1160 512.8746 711.0196 692.8460 519.8707 324.4048 441.7998 534.3156 490.2324 585.4221 430.4960 460.9629 703.4337 ## [46] 510.0744 519.9681 468.5821 527.3318 413.0651 460.6879 664.7801 354.3382 370.0553 489.8274 326.0638 343.5181 669.8929 410.4743 259.5956 ## [61] 581.9515 479.6523 434.0253 461.1315 584.3208 423.5085 556.7683 466.5056 433.8107 588.9122 358.9104 296.3319 490.2643 374.4722 508.8257 ## [76] 550.2508 413.3520 491.6750 450.5777 637.9321 418.8566 530.1711 558.6032 531.9450 422.4871 509.5001 436.4969 609.5397 390.5833 573.7023 ## [91] 332.9137 355.7153 459.6057 416.6499 435.9233 393.1971 450.7034 370.8988 325.2500 560.9520 538.7495 596.3238 456.8505 378.8018 389.1940 ## [106] 546.0309 464.9669 437.3562 595.2106 358.9524 488.1387 577.6982 451.5795 555.9834 455.0614 424.9773 512.1967 333.4369 349.7942 497.6879 ## [121] 461.4079 579.9255 570.4377 496.8774 468.6098 553.1056 693.8511 668.4453 418.9909 571.5872 626.5654 682.8749 381.1959 380.5799 529.1706 ## [136] 495.0200 602.4397 488.4812 481.2003 480.3037 517.4619 430.3098 410.7283 383.9239 507.2224 677.0574 551.9872 743.8753 447.8830 553.9661 ## [151] 660.0133 426.8833 378.0482 586.2658 608.6438 414.7819 467.3578 443.9643 505.8476 582.4859 515.8577 579.6892 452.8202 320.2918 429.6083 ## [166] 496.7879 444.7553 546.7475 461.6923 495.1302 305.6665 481.9163 570.0053 411.7289 659.6989 319.8401 577.8611 325.9026 545.4157 460.2025 ## [181] 534.0228 406.5574 532.8106 611.0717 470.5804 470.7250 399.0586 376.7103 432.2565 510.2988 501.0330 311.5054 613.5306 491.6324 592.4899 ## [196] 497.2093 253.9098 540.8763 524.1655 570.1845 568.8148 649.2078 357.9979 505.1117 560.0656 488.2058 512.4326 429.8364 369.7317 562.7482 ## [211] 380.4301 691.9237 552.0891 642.8095 381.2714 610.1536 393.5623 353.8832 557.3873 478.6679 677.2078 493.7744 381.9982 534.3512 519.5267 ## [226] 354.7870 427.7809 492.3991 527.7539 236.7285 634.5613 611.5118 573.7486 494.7876 447.2701 450.1239 447.2279 528.8756 495.6735 432.1601 ## [241] 480.7788 407.7178 519.6089 533.9601 478.7551 535.1155 502.5280 342.3943 446.9967 523.1670 562.8475 462.3585 525.2291 523.7450 532.5428 ## [256] 435.9035 458.1338 661.8657 463.5148 520.1895 497.6584 427.7127 590.0779 563.6812 527.4667 580.2521 450.2358 609.7054 687.3053 315.5045 ## [271] 381.4677 354.9862 621.2616 445.6282 493.3062 557.0388 535.6489 391.4441 517.1641 643.9891 462.9767 456.7823 594.6212 679.8447 469.5304 ## [286] 585.8058 495.7101 515.8654 585.2954 475.4714 245.2662 573.7207 571.2908 510.1705 489.8945 427.6127 381.9606 549.0644 731.1598 624.7910 ## [301] 351.4736 516.0527 502.4989 575.3473 508.8300 598.5068 626.5723 394.5084 551.1109 481.5339 602.6052 490.7534 524.8933 594.8019 511.1983 ## [316] 451.8964 493.0614 628.4617 477.1235 576.7203 469.4596 528.6711 536.4981 466.4876 529.7832 514.9179 594.0735 475.0174 379.3909 551.3659 ## [331] 416.0800 653.6031 349.7248 498.9691 603.6682 541.4988 497.6942 419.0618 461.9743 433.0824 291.6662 454.9965 429.0149 266.4133 580.8652 ## [346] 417.8723 625.2045 507.4710 391.1100 654.7852 627.8170 398.9321 384.7853 501.1054 675.6450 476.3180 440.5276 509.3056 599.2357 592.1269 ## [361] 651.7668 524.3816 421.1368 452.8244 529.8621 644.1803 478.4175 508.7750 595.8007 558.5413 571.2349 555.1023 559.2593 489.7140 493.0326 ## [376] 669.2377 469.0042 347.3663 475.9720 421.2802 645.9101 453.0382 541.1887 411.5055 666.8346 494.7872 397.1767 497.7115 486.7867 534.5902 ## [391] 439.6220 480.8484 554.9179 368.8868 460.8274 489.0922 711.1718 501.9165 418.5724 718.6710 423.1709 435.8882 532.8551 649.3885 451.9812 ## [406] 587.7714 516.2318 511.8405 380.5151 573.8564 462.3429 510.8129 275.7200 535.5461 623.0225 456.8365 490.8698 594.5922 653.8893 516.6514 ## [421] 332.8932 612.6280 524.1005 538.2043 487.1793 524.3440 497.0323 502.0975 587.9191 448.9578 476.7128 602.9146 469.7850 376.6824 499.8641 ## [436] 650.7851 615.8094 493.3007 570.9333 608.2727 559.6828 587.6076 477.4335 535.3579 459.4781 448.0090 531.2777 522.1706 543.7536 606.8745 ## [451] 463.3416 491.5422 684.6416 437.6934 476.8128 499.2137 527.9584 599.6644 396.7951 477.0358 532.5132 478.9701 610.1375 570.7250 458.6793 ## [466] 490.3939 492.9727 548.0784 491.5192 534.6258 592.4548 429.9015 622.2405 514.6087 482.5179 631.1590 548.9731 476.7655 550.6699 458.7896 ## [481] 500.6046 489.9678 521.7070 376.1594 455.4748 478.8103 500.6990 450.0641 705.1408 493.3444 340.2326 450.3086 565.5474 510.5845 607.4647 ## [496] 525.2464 601.9312 393.1189 581.0030 537.7657 453.3838 318.0283 508.4975 531.2463 373.1126 548.7601 623.4519 440.0869 652.5000 437.5633 ## [511] 506.5816 415.4071 516.0706 544.6454 487.4728 544.8934 566.9092 617.6771 548.2616 631.0511 473.4739 571.3268 368.3856 501.6164 522.8763 ## [526] 629.5952 457.4299 527.2467 326.8235 513.6585 420.4447 447.2285 441.3478 517.2391 292.0001 494.4801 527.8838 542.8061 566.1641 578.9908 ## [541] 357.2008 473.6306 677.2587 535.5803 301.3727 403.7398 414.6190 625.7101 433.4926 513.9790 290.1241 249.8849 590.1190 487.6803 571.0442 ## [556] 601.2068 469.5465 475.7392 432.6426 496.2470 418.4045 303.4358 726.0839 609.2849 500.5518 578.1233 408.7615 380.4873 594.1007 499.7439 ## [571] 531.1113 368.6508 455.1496 609.9466 458.9388 604.4078 402.9088 415.8239 448.0483 462.1295 522.6474 487.6938 456.0863 577.8719 340.5999 ## [586] 633.7612 443.4922 454.3063 610.4940 435.0683 599.6636 397.2973 374.6455 325.8588 321.3024 482.7620 374.4408 425.1393 510.4970 531.1596 ## [601] 645.9354 448.5209 531.6261 348.1992 476.0654 562.8976 379.9974 532.0484 445.4427 634.4011 368.7167 672.7004 565.7535 491.0597 731.4451 ## [616] 604.8716 421.3364 300.1127 440.5535 527.3207 420.5020 543.1092 499.6028 422.6449 742.7753 436.8232 530.8857 546.1302 647.6666 448.8163 ## [631] 366.9691 396.1402 562.1381 741.5654 426.0017 551.1103 478.2167 600.2266 532.1149 436.3339 660.6611 354.6999 488.2334 632.6080 390.4946 ## [646] 389.2055 641.5392 502.8989 586.2955 447.9865 349.9972 500.9418 283.4726 583.2403 523.0787 475.2691 539.1305 525.1959 503.8490 297.1299 ## [661] 485.4389 480.3741 525.1787 659.0466 519.6565 513.0671 360.0704 637.1515 586.5342 580.8653 435.4805 624.2765 464.9148 409.2928 565.2380 ## [676] 556.4711 392.7148 588.4800 616.1251 523.3135 495.8529 359.6408 597.8614 557.3185 547.4084 554.3561 469.9123 471.6370 552.7903 457.7323 ## [691] 509.1782 484.3411 310.7837 349.0889 469.9034 736.6610 482.3783 423.4637 607.1870 525.9129 456.5308 358.0275 605.7850 646.5580 477.9809 ## [706] 377.2324 403.8891 601.9674 613.8450 408.5394 510.2801 372.3809 693.2728 609.3073 704.5412 441.4024 623.5082 511.3888 539.7074 465.6184 ## [721] 663.6630 699.4078 444.3459 485.8474 463.9139 436.9269 583.1504 435.3909 584.0109 687.2442 626.6060 504.6432 497.2372 306.4394 430.5330 ## [736] 566.2029 441.3459 484.8510 524.0242 440.1531 554.9754 572.8674 558.5630 621.3363 635.4167 549.3009 548.0824 469.9470 728.0815 691.3732 ## [751] 650.7351 311.2510 528.0012 513.0207 443.2585 495.4862 494.3729 577.6868 511.6519 387.7507 500.9570 528.0799 392.6372 575.0573 649.4153 ## [766] 424.9899 445.2550 589.0665 456.6779 417.0215 494.1086 466.9487 507.0484 617.9516 408.3503 486.6273 519.3648 367.0132 603.6742 357.5127 ## [781] 459.2164 414.9186 477.9580 375.7157 227.5575 447.0915 552.5945 541.2205 477.3557 511.3888 366.3676 541.8846 641.1269 493.1544 444.6010 ## [796] 571.4874 507.8334 376.4408 729.4874 607.8687 453.6202 377.1565 284.5644 514.1725 582.8606 423.0604 485.5145 430.1117 616.0109 367.8656 ## [811] 393.5513 450.6351 475.9432 513.4465 620.5404 560.5519 450.3671 520.9705 493.8261 507.4381 665.0495 325.3849 508.1991 343.4316 380.3415 ## [826] 424.6257 606.1888 579.1084 462.3792 389.8099 490.8874 591.3056 465.1908 624.4178 423.0522 253.6473 462.7211 452.0061 365.4780 334.9249 ## [841] 430.0378 310.6141 587.8560 435.5059 352.7550 366.2667 496.9301 464.1797 426.3634 524.6137 575.3314 462.3060 354.1794 577.5041 288.6520 ## [856] 467.3059 478.7292 635.0938 467.7270 610.1464 442.0070 583.9132 363.9433 530.5671 549.5344 468.3479 466.9145 655.7451 512.8609 504.2036 ## [871] 538.2761 475.7039 458.7146 418.8669 606.1192 448.4299 387.2663 474.5578 576.6278 502.9695 509.4259 505.9550 557.8631 606.7899 546.4254 ## [886] 517.9671 456.8737 534.7139 532.2812 506.9922 437.5018 354.5535 547.5040 546.1719 496.7769 594.6566 566.8912 651.1099 569.2072 399.9619 ## [901] 499.5997 453.0542 503.6737 479.9079 603.6589 588.4268 480.5298 719.0702 476.4992 463.0849 515.4054 557.1812 480.2085 517.2196 429.5344 ## [916] 664.7642 565.2157 489.8956 502.2124 514.0178 534.4487 468.6748 477.4474 428.4724 342.0031 591.6231 361.7524 579.8610 503.2338 531.1024 ## [931] 378.8424 436.3192 708.5300 524.2979 451.3790 368.0187 343.0128 649.4367 593.2486 716.6372 566.0013 563.3512 323.0505 712.7242 283.4797 ## [946] 593.6304 368.4733 586.7725 402.3331 557.0522 511.9797 398.3025 623.5914 529.3349 515.1055 495.3027 594.1564 578.8444 386.4326 402.7863 ## [961] 526.0448 507.9440 334.2873 398.9222 523.8656 365.6481 488.9171 459.0526 286.3321 337.5755 613.3931 478.8902 514.2966 745.8776 475.3705 ## [976] 517.7410 544.9654 630.6918 565.4086 335.1642 531.3661 436.6194 427.9133 425.6029 557.2813 501.2163 547.3963 370.5879 490.5978 447.3090 ## [991] 512.0111 506.0593 444.2074 446.8414 401.0092 463.4028 441.7573 460.8381 494.1673 437.6966 ## [ reached getOption(&quot;max.print&quot;) -- omitted 1500 entries ] 12.2 Find Cohens Kappa (Interrater reliability) Useful for IRR agreement on categorical variables Going to use the psych package for this: https://www.rdocumentation.org/packages/psych/versions/2.1.6/topics/cohen.kappa See here for an overview of what Cohens Kappa is if you need a recap/intro. 12.3 Statistical tests and modeling with easystats https://easystats.github.io/easystats/ 12.3.1 Getting parameter estimates from model objects Scenario: Youve run some statistical test (like the below regression), and want a summary of the model estimates. rm(iris) model &lt;- lm(Sepal.Length ~ Species, data = iris) You have a few options when it comes to getting a summary of a model and getting the coefficient estimates: - summary() - broom::tidy() - paramters::model_paramters(), or just paramters::paramters() for short Theres no reason to use summary, generally speaking, because it sucks. It doesnt give you tidy output thats easy to manipulate or extract, its hard to read, and it cant be turned into a useful table. Skip it unless you need something specific from its output (i.e., youre using lavaan) Options two and three are pretty similar and both give you most of the same information, though parameters() prints neater to the console window. Generally I find parameters preferable. Note though that neither command will round the numbers if you store it as a table in the environment. So. If you want to manipulate ANY info in the table and/or extract info, just use tidy or parameters. If youre using the command to export said info in a neat table, or you want to view it in a more readable fashion and do not care about extracting/modifying/manipulating anything in it, then use parameters and pipe it to format_table() Using format_table() rounds all columns to 2 decimal places, reformats p-values to APA format, and collapses CIs into a single column. Do note though that it makes every column into a Character column! So this is for exporting-use only. Heres a comparison of brooms output (first) vs.Â parameters (second) when you save each in the environment. As you can see, both produce tidy tibbles And heres what parameters(model) |&gt; format_table() does to the a parameters table: Much cleaner for making a table to export to Word. 12.3.2 Getting model information and performance metrics Again, two options here. You can use either glance from the broom package, or performance from the package of the same name. These each produce slightly different output, though unlike above, I dont think one is necessarily better than the other. Use whichever one you prefer. broom::glance(model) ## # A tibble: 1 x 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual nobs ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 0.619 0.614 0.515 119. 1.67e-31 2 -112. 231. 243. 39.0 147 150 performance::performance(model) ## # Indices of model performance ## ## AIC | BIC | R2 | R2 (adj.) | RMSE | Sigma ## ----------------------------------------------------- ## 231.452 | 243.494 | 0.619 | 0.614 | 0.510 | 0.515 12.3.3 Effect size info with effectsize logreg_model=glm(smoke ~ age + sex, data= legaldmlab::survey, family = &quot;binomial&quot;) logreg_model_coeff=parameters::parameters(logreg_model) logreg_model_coeff=logreg_model_coeff |&gt; dplyr::mutate(odds_ratio=exp(Coefficient)) effectsize::interpret_oddsratio(logreg_model_coeff$odds_ratio, rules = &quot;chen2010&quot;) ## [1] &quot;small&quot; &quot;very small&quot; &quot;very small&quot; ## (Rules: chen2010) 12.3.4 Quick, detailed, and automated reporting with report Check out https://easystats.github.io/report/ 12.3.5 Running correlations with correlation https://easystats.github.io/correlation/ "],["coding-tips-and-tricks.html", "Chapter 13 Coding Tips and Tricks 13.1 Grammar stuff in base R 13.2 Tidyverse stuff 13.3 Function-related stuff 13.4 Creating a package 13.5 Creating a bookdown", " Chapter 13 Coding Tips and Tricks 13.1 Grammar stuff in base R 13.1.1 Regex expressions and symbols str_remove(html$`Market Price`, pattern = &quot;$&quot;) # doesn&#39;t remove the $ sign str_remove(html$`Market Price`, pattern = &quot;\\\\$&quot;) # works 13.1.2 The new pipe (Base R) Good reading material/stuff to know: https://www.r-bloggers.com/2021/05/the-new-r-pipe/?__twitter_impression=true&amp; Not really any functional differences from the tidyverse pipe, but using this instead puts one less dependency in your code. So its probably worth changing. You can set this in the global options in RStudio. 13.2 Tidyverse stuff 13.2.1 Sometimes when making a function you need to use the colon-equals operator, rather than just the normal &lt;- or = assignment operators Specifically, when you have multiple named arguments in your function Read my question and someones answer on this blogpost: https://community.rstudio.com/t/help-creating-simple-function/109011/2 13.3 Function-related stuff 13.3.1 User-supplied expressions or named columns in functions 13.3.2 When a command requires a named column or data set, but youve already supplied it and its required a second time If youre writing a function with a pipe but the command youre using needs the data set defined in it, you specify it as .x Here is an example: 13.3.3 Formulas within functions Generally when you see .fn inside a function (e.g., map(x, .fn)), that means function. You put whatever formula or function you want in there. You may also see the tilde used instead, which does the same thing. 13.4 Creating a package https://rstudio4edu.github.io/rstudio4edu-book/data-pkg.html 13.4.1 Documenting pacakge meta-data https://r-pkgs.org/description.html 13.4.2 Connecting to other packages https://kbroman.org/pkg_primer/pages/depends.html 13.4.3 Linking Git and Github view this detailed guide by Jenny Bryan, and this YouTube video. Quick summary of steps in YouTube video: Open project folder in Windows Explorer and click in the URL bar, then type cmd to open command prompt If there are any pre-existing git files or repository info there, remove it with the following: rd .git /S/Q Tell git to create a new repo by typing: git init Then tell it to include all files in the current place by typing: git add . Commit these files with: git commit -m \"Initial commit\" At this point youve created a git and GitHub repo each; now link them with: git remote add origin [https URL of GitHub repo] Push all these changes live with: git push -u origin master 13.5 Creating a bookdown https://www.youtube.com/watch?app=desktop&amp;v=m5D-yoH416Y&amp;feature=youtu.be 13.5.1 Rendering the book once its done Render locally with bookdown::render_book(index.Rmd) Use browseURL(\"docs/index.html\") to view your book locally (or just open index.html in a browser). If it looks good, commit and push all changed files to GitHub. "],["creating-a-simulated-data-set.html", "Chapter 14 Creating a simulated data set 14.1 Part 1: Independent samples from a normal distribution 14.2 Part 2: Creating data sets with quantitative and categorical variables 14.3 Part 3: Repeatedly simulate samples with replicate() 14.4 Part 4: repeatedly making whole data sets 14.5 Part 5: Using purrr", " Chapter 14 Creating a simulated data set From the tutorial on this page 14.1 Part 1: Independent samples from a normal distribution Consider the following first before you start doing stuff: - How many subjects are in each condition? - What are the means and standard deviations of each group? Set that shit below. # number of subjects per group A_sub_n &lt;- 50 B_sub_n &lt;- 50 # distribution parameters A_mean &lt;- 10 A_sd &lt;- 2.5 B_mean &lt;- 11 B_sd &lt;- 2.5 Now generate scores for each group A_scores &lt;- rnorm(A_sub_n, A_mean, A_sd) B_scores &lt;- rnorm(B_sub_n, B_mean, B_sd) Technically you could stop here and just analyze the data in this fashionbut its better to organize it into a table. One that looks like something you would import after real data collection. So do that next; make it look nice. dat &lt;- tibble( sub_condition = rep( c(&quot;A&quot;, &quot;B&quot;), c(A_sub_n, B_sub_n) ), score = c(A_scores, B_scores) ) head(dat) ## # A tibble: 6 x 2 ## sub_condition score ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 11.4 ## 2 A 12.0 ## 3 A 12.5 ## 4 A 8.84 ## 5 A 6.84 ## 6 A 11.0 Always perform a quality and consistency check on your data to verify that shits ok. dat %&gt;% group_by(sub_condition) %&gt;% summarise(n = n() , mean = mean(score), sd = sd(score)) ## # A tibble: 2 x 4 ## sub_condition n mean sd ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 50 9.98 2.78 ## 2 B 50 10.9 3.02 14.2 Part 2: Creating data sets with quantitative and categorical variables From the web page at this link 14.2.1 2.a. DATA WITH NO DIFFERENCE AMONG GROUPS Critically important notes to know: When you use the rep() function, there are several different arguments you can specify inside it that control how stuff is repeated: using rep(x, each= ) repeats things element-wise; each element gets replicated n times, in order rep(c(&quot;A&quot;,&quot;B&quot;), each=3) ## [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; &quot;B&quot; &quot;B&quot; using rep(x, times= ) repeats the sequence; the vector as a whole, as it appears, will be repeated with one sequence following the next rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), times=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; using rep(x, length.out) repeats only the number of elements you specify, in their original order rep(c(&quot;A&quot;,&quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), length.out=3) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; In this particular data, we want every combination of group and letter to be present ONCE. letters=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,&quot;E&quot;) tibble(group = rep(letters[1:2], each = 3), factor = rep(LETTERS[3:5], times = 2), response = rnorm(n = 6, mean = 0, sd = 1) ) ## # A tibble: 6 x 3 ## group factor response ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 A C -0.559 ## 2 A D 0.0378 ## 3 A E -1.14 ## 4 B C -1.10 ## 5 B D 0.509 ## 6 B E -0.304 14.2.2 2.b. Data WITH A DIFFERENCE among groups What if we want data where the means are different between groups? Lets make two groups of three observations where the mean of one group is 5 and the other is 10. The two groups have a shared variance (and so standard deviation) of 1. 14.2.2.1 Some notes first Creating a difference between the two groups average score means we have to tell R to sample itteratively from distributions with different means. We do this by specifying a vector of means within rnorm, like so: response = rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 4.066141 9.831952 4.307055 11.059641 4.808349 10.009015 You can see that: 1. draw 1 is from the distribution \\((\\mu=5,\\sigma=1)\\) 2. draw 2 is from the distribution \\((\\mu=5,\\sigma=1)\\) And this process repeats a total of six times. And if you happen to also specify a vector of standard deviations (purely to demonstrate what is happening, we wont actually do this), the first mean is paired with the first SD; the second mean is paired with the second SD; and so on. rnorm(n = 6, mean = c(5, 10), sd = c(2,0.1)) ## [1] 5.744356 10.043787 7.378609 9.952296 8.064520 10.119440 14.2.2.2 Ok, back to creating the data If you want there to be differences between the groups, we need to change the way the vector of factors is replicated, in addition to specifying the vector of means. We want to ensure that the sequence of A, B in the group column matches the sequence repeated in the response column. Here we are going to use length.out so that the whole sequence of A,B is repeated exactly in line with the alternating drawing from \\(\\mu=5\\), \\(\\mu=10\\). Its often best to do this by building each thing separately, and then combining it into a tibble when you have it figured out. group=rep(letters[1:2], length.out = 6) group ## [1] &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; &quot;A&quot; &quot;B&quot; response=rnorm(n = 6, mean = c(5, 10), sd = 1) response ## [1] 4.948867 10.118024 3.109300 8.589811 4.047384 10.121841 tibble(group, response) ## # A tibble: 6 x 2 ## group response ## &lt;chr&gt; &lt;dbl&gt; ## 1 A 4.95 ## 2 B 10.1 ## 3 A 3.11 ## 4 B 8.59 ## 5 A 4.05 ## 6 B 10.1 14.2.3 2.c. Data with MULTIPLE QUANTITATIVE VARIABLES with groups 14.3 Part 3: Repeatedly simulate samples with replicate() Instead of drawing values one at a time from a distribution, we want to do it many times. This is a job for replicate(). What replicate() does is run a function repeatedly. The replicate() function will perform a given operation as many times as you tell it to. Here we tell it to generate numbers from the distribution \\(N~(\\mu=0, \\sigma=1)\\), three times (as specified in the n=3 argument in line one) replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = FALSE ) ## [[1]] ## [1] 1.3203910 0.2040899 -0.7970636 -1.2431823 -0.4844866 ## ## [[2]] ## [1] -1.2322047 0.1063858 0.7922842 0.4413982 -0.8931153 ## ## [[3]] ## [1] 1.7023983 0.1654943 0.7787761 -0.1186586 -0.2111680 The argument simplify=FALSE tells it to return the output as a list. If you set this to TRUE it returns a matrix instead replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) ## [,1] [,2] [,3] ## [1,] 0.72115635 0.7402300 0.1134487 ## [2,] 0.92810115 -1.1035296 -1.0247406 ## [3,] 0.53107386 0.0286275 -0.3009290 ## [4,] -0.01083094 0.5085364 1.7663764 ## [5,] -0.51711197 0.3824463 -1.0787283 Specifying as.data.frame() with the matrix output can turn it into a data frame. replicate(n = 3, expr = rnorm(n = 5, mean = 0, sd = 1), simplify = TRUE ) %&gt;% as.data.frame() %&gt;% rename(sample_a=V1, sample_b=V2, sample_c=V3) ## sample_a sample_b sample_c ## 1 0.56415036 -1.7060196 1.1146509 ## 2 -0.24011787 1.8521680 1.4152690 ## 3 -0.09948105 1.9130434 0.1568660 ## 4 -1.76657069 -0.2574811 -1.0662388 ## 5 0.67864934 0.5891597 -0.1166993 14.4 Part 4: repeatedly making whole data sets This is combining parts 2 and 3 to repeatedly create and sample data sets, resulting in a list of many data sets. simlist = replicate(n = 3, expr = data.frame(group = rep(letters[1:2], each = 3), response = rnorm(n = 6, mean = 0, sd = 1) ), simplify = FALSE) simlist ## [[1]] ## group response ## 1 A -1.1482417 ## 2 A -0.5806775 ## 3 A 0.3688638 ## 4 B 0.3304444 ## 5 B 0.3483946 ## 6 B -1.1257075 ## ## [[2]] ## group response ## 1 A 0.8829318 ## 2 A -1.3660498 ## 3 A -0.7142761 ## 4 B -0.2700098 ## 5 B -0.2419871 ## 6 B -0.4623374 ## ## [[3]] ## group response ## 1 A -0.6018124 ## 2 A 1.0289143 ## 3 A 0.1779939 ## 4 B 1.7056511 ## 5 B -0.3255277 ## 6 B -0.7897194 14.5 Part 5: Using purrr See this blog post "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
