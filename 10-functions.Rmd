# Intermediate R: Functions, Loops, and Iterative Programming

## Functions

A function is a command that performs a specified operation and returns
an output in accordance with that operation. You can literally make a
function to do anything you want.

*General structure of a basic function:*

```{r chapter_10, eval=FALSE}
# example structure

Function_name=function(argument){
  Expressions
  return(output)
}
```

-   *Argument* is your input. It is the thing you want to perform the
    operation on.
-   *Expressions* is the actual operation (or operations) you want to
    perform on the supplied argument
-   *return* tells R to return the result of the Expression to you when
    done.

This example function takes an input of numbers in the form of a vector
and subtracts two from each.

```{r 10.2}
numbers=c(2,10,12,80)

sub_2=function(x){
  result= x - 2
  return(result)
}

sub_2(numbers)
```

We can also supply the function with a single number and it still works...
```{r}
sub_2(100)
```

***Well this looks useful. So what's the bigger picture?***

One of the primary advantages of functions are that they can reduce a long and
complex process, or a process that involves many steps, into a *single line of code*; thus, creating your own functions is a fast way to make
your life easier down the line either at some point in the far future or
even in just a few minutes, if you know you will be writing the code for
some process two or more times.

Take this script for instance. You can see from the circled parts that I
needed to transform three different data sets in a similar way:

```{r}
knitr::include_graphics(here::here("pics", "repeat_process.jpg"))
```

Yes, I could have just done a copy-paste of the original code and tweak it slightly each time....
But that is time consuming, produces a sloppier and longer script, and introduces a lot more room for error because of the repeated code and extra steps.

Better to write a single function that could be applied to all three....

***In short, use functions to reduce a multi-step process or a process
that you're implementing \>=2 times in a single script into one command.
This saves you space and makes the script shorter; it saves you the
trouble and effort of re-writing or adapting code from earlier sections;
and importantly, reduces the chances of you making a coding error by
proxy of the former two.*** 

As a quick example, I was able to replace each of the circled paragraphs of code above with a custom function that ran
everything in one simple line. Now instead of 3 whole (and redundant) paragraphs, I now have 3 short lines, like so....

```{r eval=FALSE}
na_zero_helpreint=rotate_data(data = na_zero_helpreint,
                              variable_prefix = "reintegrate_")

na_blank=rotate_data(data = na_zero_helpreint, variable_prefix = "barrier_")

na_zero=rotate_data(data = na_zero_helpreint, variable_prefix = "barrier_")
```

**Limitations to your average, everyday functions.** While reducing a
whole *process or sequence* of commands is extremely useful, it still
leaves a limitation. For instance, while we avoided copying and pasting
whole paragraphs or processes, I still had to copy-paste the same
function three times. *This still leaves chances for error on the table, and it still leaves us with wasted lines that make the script longer.*

In general, when you want to perform some function or process **multiple times on multiple items (as above where the same command is used three times on three different data frames)**, you need to use a for-loop or
iterating function. These can reduce further unwanted redundancies by applying the function or process iteratively. Read on for more info.

## For-loops

A for loop is essentially a function that applies a function or given
set of operations to multiple things at once, and returns an output of
many items.

For example, this code finds the means of every vector/column in a
dataset by repeatedly applying the same code over and over to element
"i" in the given list:

```{r 10.3}
df <- tibble(
  a = rnorm(10),
  b = rnorm(10),
  c = rnorm(10),
  d = rnorm(10)
)

output <- vector("double", ncol(df))  # 1.Output. Create the object you want the results of the loop stored in.

for (i in seq_along(df)) {            # 2.Sequence of operations. "For each item 'i' along data frameâ€¦"
  output[[i]] <- median(df[[i]])      # 3.Body:"every individual item in 'output' = the median of each col in df
}

output
```

Check out [this book
chapter](https://r4ds.had.co.nz/iteration.html#mapping-over-multiple-arguments)
for a great and detailed explanation of for-loops and functional coding.

Although for loops are nice, they are unwieldy. R programmers typically
use iterating functions instead. Examples of iterating functions are the
`lapply`, `vapply`, `sapply`, etc. family of base R commands. But these can
also be confusing and the commands are not great.

The purrr package offers a better way to do iterating functions over
base R; it's the tidyverse way to make efficient and understandable for
loops! **If you have a need for a for-loop for something, see the next
section instead on how to use purrr to make an iterative function.
Important to understand conceptually what a for-loop is, but using them
is impractical when you have purrr**

## purrr and Iterative Functions

*All notes here come from Charlotte Wickham's lecture tutorial below*

- Part 1: <https://www.youtube.com/watch?v=7UlWJWfZO9M> 
- Part 2: <https://www.youtube.com/watch?v=b0ozKTUho0A&t=1210s>

purrr's `map()` series of functions offer a way to apply any existing
function (even functions you've made) to multiple things at once, be it
lists, data frame columns, individual items in vector, etc. In short,
they are for doing the same type of task repeatedly in a very quick and
efficient manner. They work in much the same way as for-loops, but are
far simpler to write, and can be applied in the same way to solve the
same problems.

```{r purrr_pic, include=FALSE}
knitr::include_graphics(here::here("pics", "purrr.png"))
```

***How to use purrr***

The structure of `map()` commands is the same as the others in the
tidyverse:

```{r eval=FALSE}
#option 1
map(data, function)

# option 2
data %>% map(function)
```

As a quick example and to highlight why purrr is so much more efficient
and easier to use than for-loops, look at the same example from before,
now using `map()` instead of a `for`:

```{r}
df |> map_dbl(median)
```

A single line is all it took to get the same results! And, it follows
tidyverse grammar structure.

Now lets get into how it works....

```{r, include=FALSE}
knitr::include_graphics(here::here("pics", "map_structure.png"))
```

*map() commands work like this:* For each element of x, do f.

So if you pass it object `x` and object `x` is....
- A vector, it will perform function f on every item in the vector
- A data frame, it will perform function f on every *column* in the data frame
- A list, it will perform function f on every *level in the list*

Etc., etc.; the point is it applies a function repeatedly to every
element in the object you supply it with.

```{r, include=FALSE}
knitr::include_graphics(here::here("pics", "map_structure_2.png"))
```

So lets walk through a case example.

### Reproducible example: Scraping web data

This is an example walk through showing how we can use `purrr` to speed
things up dramatically and/or reduce the use of unwanted, extra code in
our scripts. In this guide I'll be building a table of LPGA Tour
statistics from multiple webpages.

The workflow for purrr goes like this:

First, you want to figure out how to do each step of your process
line-by-line, for a single item. The idea is to try and walk through
each step of the process and see exactly what will need to be done each
each step and what the code will like, before trying to code it all at
once at a higher level.

Once you have each step for the first item figured out, then you make functions
for each step that condense that code down to one command.

Lastly, apply each function from your individual steps to all items in
your list by using `purr::map()`.

***Do for One***

```{r 10.9}

library(rvest)

# STEP 1
# Figure out a line-by-line process for one item/one single web page
html1=read_html("https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&rank=04") |> 
  html_nodes("table.shsTable.shsBorderTable") |> 
  html_table(fill = TRUE, header=TRUE) |> 
  as.data.frame() |> 
  janitor::clean_names()

head(html1)


# STEP 2
# create a custom function of the above to shorten and generalize the process
quick_read_html=function(url){
  web_page=read_html(url) |> 
  html_nodes("table.shsTable.shsBorderTable") |> # fortunately this node works for all four pages so it can be baked into the function
    html_table(fill = TRUE, header = TRUE) |> 
    as.data.frame() |> 
    janitor::clean_names()
  
  return(web_page)
}



# test to verify it works
test=quick_read_html(url= "https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&rank=08") 

head(test) # nice 
```

***DO FOR ALL.***
Now create the object that contains all the elements you want to iterate over, and then pass it to your generalized function with map.

```{r 10.13}
# Step 3a
# create an object that contains ALL elements of interest
URLs=c("https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&rank=04",
       "https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&rank=08",
       "https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&rank=06",
       "https://scores.nbcsports.com/golf/averages.asp?tour=LPGA&rank=12")

# Step 4
# use the power of map and be amazed
lpga_data= URLs |> map(quick_read_html)

head(lpga_data)
```

All done!! And just like that, we've downloaded four different web
pages, extracted the tabled info, and formatted them without copying and
pasting any code. The same process for all four was only used one time
to write the initial function. Just apply some final formatting to clean
it up a bit and combine the separate data frames into a single, unified
one.

```{r}
lpga_data= lpga_data %>% 
  reduce(left_join, by="name") %>% # Combine all list levels into a single tibble, matching by the "Name" column
  select(-contains("rank.")) |> 
  rename("score_average"="score_average_actual")
  
# VOILA! 
head(lpga_data)
```



### Non-reproducible example (Juvenile Life Without Parole study)

In the Juvenile Lifers study, there were a series of questions that
participants rated on a scale of 0-100 in terms of difficulty. Part of
our analysis involved taking the ratings on those variables and giving
them relative rankings, so that each of the 6 variables in the series
was rated from the least to most difficult, by participant.

Now if we only needed to compute these rankings once this wouldn't have
been any big deal; however, we needed to do it **three times**. 

Much of the same code and the same process would need to be copied and pasted,
resulting in a very long, messy, harder to read script. With purrr
however, we can reduce the redundancies to a minimum, saving time and reducing the chances of mistakes.

***Step 1.***
Just like before, the first step is to find a line-by-line solution for a single item, and then to generalize this into a shortcut function that can be applied to the any item "i" in a series of items.

For the sake of brevity, I'm going to skip most of that and just include the functions below.

```{r 10_nonrepro_ex1, eval=FALSE}
load("C:/Github Repos/Studies/JLWOP/Data and Models/jlwop_reentry_survey.RData")

#### CREATE THE DATA SETS WE NEED####
na_blank=jlwop_reentry_survey # analysis 1 keeps the data as-is

na_zero=jlwop_reentry_survey %>% # supplementary analysis replaces the NA's with 0
  mutate(across(c(barrier_housing:barrier_identification), replace_na,0))

rm(jlwop_reentry_survey) # remove old data set to avoid confusion


#### Functions ####

# transformation function to wrangle the data into proper formatting
rotate_data=function(data, variable_prefix){
  
data=data %>% 
  pivot_longer(
    cols= starts_with(variable_prefix), # collect all the desired variables (i.e., columns)....
    names_to = "variable", #...and put them into a new categorical variable called "variable"
    values_to = "participant_score") %>% # ...and store their values in a new variable called "participant_score"
  arrange(unique,participant_score) %>% 
  
  select(c(unique, participant_score, variable)) %>% # keep only these 3 variables
  relocate(variable, .before = participant_score) # put the newly created variable up front

return(data)
}


# creating the rankings for each variable; then transform data back to original structure
rank_and_unpivot=function(data){
  data=data %>% 
  group_by(unique) %>% # group the scores so they can be ranked by participant
  mutate(rank1=dense_rank(participant_score), # create ranking variable
               rank=max(rank1,na.rm = TRUE) + 1 - rank1) %>% # fix ranks by flipping to ascending order
  mutate(rank=factor(rank)) %>% # convert rank to factor structure
  select(-rank1)


# Pivot back to wide
data=data %>% 
  pivot_wider(names_from = variable, values_from = rank:participant_score) %>% 
  ungroup() # un-group the data and delete the generated names

return(data)
}

```


***Step 2.***
Again, like before, we want to combine all elements of interest into some object. Once we have that, we then pass said object to `map()` and supply the map call with our custom function.
```{r 10_nonrepro_ex2, eval=FALSE}

dfs=list(na_blank=na_blank, na_zero=na_zero) %>% # create lists
  map(.f=rotate_data, variable_prefix = "barrier") %>% # apply custom function along whole list
  map(rank_and_unpivot) # again!!  DO IT AGAIN! With another function this time.

# extract list elements to make them data frames again
list2env(dfs, globalenv())

rm(dfs) #discard list. It has fulfilled its purpose.
```

And just like that, we're done!


### Example 3: Read/Import several files at once with `map()`

Multiple ways you can do this....

```{r 10.6a, eval=FALSE}

################### Option 1: read all into the global environment, keeping them as SEPERATE df's ###############
legaldmlab::read_all(path="Data Repository/Stats Data Repository/JASP files", extension = ".csv")

# Option 1.A: Squish ALL OBJECTS in the working environment into a list
# Again, note the "ALL OBJECTS" part; make sure there are no functions or other things in the environment when you run this.
files=mget(ls())


############# Option 2: Read in all files as a LIST of df's, then stitch in the names #####################################

files=paste0(here::here("Data Repository", "Stats Data Repository", "JASP files", "/"), 
             list.files(path=here::here("Data Repository", "Stats Data Repository", "JASP files"), pattern = ".csv"))

files_list=files |> 
  map(readr::read_csv)

names(files_list)=file.path(here::here("Data Repository", "Stats Data Repository", "JASP files")) |> #specify file path as a string
  list.files(pattern = ".csv") |> # pass the path string to list files; search in this location for files with this extension
  gsub(pattern=".csv", replacement = "") # remove this pattern to save only the name


# Option 2.A: Extract each data frame and put everything into the global environment
list2env(cog_data, globalenv())
```


## Other purrr commands

Note that `map()` always returns a list, and depending on the output
that you want, you may need to use a variation of `map()`. These
variations are as follows:

| Command     | Return                                      |
|-------------|---------------------------------------------|
| `map_lgl()` | logical vector                              |
| `map_int()` | integer vector                              |
| `map_dbl()` | double vector                               |
| `map_chr()` | character vector                            |
| `walk()`    | only returns the side effects of a function |



### The `walk` and `walk2` commands

Walk() is useful for when you just want to plot something or write a
save file to your disk, etc. It does not give you any return to store
something in the environment. You use it to write/read files, open
graphics windows, and so on.

```{r, include=FALSE}
knitr::include_graphics(here::here("pics", "walk.png"))
```

***Example: Writing multiple files at once***

```{r, include=FALSE}
knitr::include_graphics(here::here("pics", "export_purrr.png"))
```

Utilize `purrr::walk2()` to apply a function iteratively on TWO objects
simultaneously. To save multiple .csv files with `walk2`, we need two
distinct lists: 1. A list of data frames that we wish to export, 2. and
the file paths, complete with the file names and extensions, for each
file to be written

First create and define both list items. Then apply `walk2()` to pluck
an element from list 1 and its corresponding element from list 2, and
apply the `write_csv` function in for-loop fashion.

```{r walk_demo, eval=FALSE}

### Custom function ####
# Create needed function that grabs file names and stitches them together with the correct path and extension
# Included in legaldmlab package
bundle_paths=function(df_list, output_location, file_type){
  names=names(df_list)
  paths=rep(here::here(output_location), length(names))
  extension=rep(c(file_type), length(names))
  
  fixed_names=paste0("/",names)
  
  path_bundle=list(paths,fixed_names, extension) %>% 
    pmap(., paste0)
  
  return(path_bundle)
}

#### Exporting the .csv files for SPSS/JASP/etc. ####
# Define list 1
dfs=list(na_blank=na_blank,
         na_zero=na_zero,
         na_zero_helpreint=na_zero_helpreint)

# list 2
paths_csv=bundle_paths(df_list = dfs, folder_location = "JLWOP/Data and Models", file_type = ".sav")


# Iterate over all elements in list 1 and corresponding element in list 2;
# and apply the the write_csv function to each
walk2(.x=dfs, .y= paths, .f=haven::write_sav)


#### .RData file for R users ####
# Combine multiple data frames into a single .RData file and export
save(list = c("na_blank", "na_zero", "na_zero_helpreint"), 
     file = here::here("JLWOP", "Data and Models","ranking_data.RData"))
```


### `map2` (and `walk2`)

```{r, map2_pic1}
knitr::include_graphics(here::here("pics", "map2_a.png"))
```

```{r map2_pic2}
knitr::include_graphics(here::here("pics", "map2_b.png"))
```


### `pmap` for when you have a bunch of shit

This function is for iterating over *three or more* elements. As soon as you have >2 items you have to iterate over, you need pmap().It works similar to to map and map2, but instead of  iterating over a single object x or two objects x and y, it acts on **a list object called .l**

The list is a list of all the objects you want to iterate over. If you
give it a list of 18 items, it iterates over all 18. If the list only
has two things, it only acts on those two.

She says its easiest to imagine the list as a data frame, and the
columns of the data frame like the elements of that list.

```{r pmap_pic1}
knitr::include_graphics(here::here("pics", "pmap.png"))
```

```{r pmap_pic2}
knitr::include_graphics(here::here("pics", "pmap_2.png"))
```


## Using purrr to manage many models

Imagine the concept of Russian Dolls, applied to data sets. You can manage data sets more effectively my collapsing them into a single tiny, mini data frame, and stuffing that inside of another one.

This is done via "nesting"...

Effectively, you smush/collapse everything down so it fits inside one column. You can unnest to expand this data back out later when you need it, and keep it collapsed when you don't. Code works like this:

```{r 3.18, eval=FALSE}
by_country=gapminder::gapminder %>% 
  group_by(continent,country) %>% # indicate the variables to keep at the top level
  nest() # smush the rest into a list-column


country_model=function(df){
  lm(lifExp~year1950,data = df)
}

# Transform a list of models into a df
models=by_country %>% 
  mutate(mod=map(data,country_model))
```

- You can store anything in a data frame. You can keep the df connected to the model, which makes it very easy to manage a whole slew of related models
- You can use functional programming (i.e., iterative functions) to map functions or combinations of functions in new ways.
- Converting data into tidy data sets gives you a whole new way (and easier way) to manage lots of information

Below is the full script I copied from Hadley Wickham's lecture, which you can watch [here](https://www.youtube.com/watch?v=rz3_FDVt9eg)

```{r eval=FALSE}

pacman::p_load(dplyr,purrr,tidyverse,gapminder)

#### Workflow for managing many models in R ####
# 1. Nest data with {tidyr}
# 2. Use {purrr} to map a modeling function
# 3. Use {broom} to inspect your tidy data


gapminder=gapminder %>% 
  mutate(year1950= year-1950) #the number of years it's been since 1950

#--------------------------------------------------------------------------------------------
#### Step 1. Nest the data. ####

# A nested data frame has one column per country. You're essentially 
# creating a Russian doll; a data frame inside of a larger data frame.

by_country=gapminder %>% 
  group_by(continent,country) %>% # variables to keep at the top level
  nest() # smush everything else into a df, and store this mini-df in its own column

# with this, you can have an entire table per row; a whole data frame for each country
# Essentially condensing a list into a table
by_country$data[[1]]


#--------------------------------------------------------------------------------------------

#### Step 2. Use purrr to map stuff. ####

# 12:50
country_model=function(df){
  lm(lifeExp ~ year1950, data = df)
}

models= by_country %>% 
  mutate(
    mod=map(data,country_model)
  )


gapminder %>% 
  group_by(continent,country) %>% 
  nest() %>% 
  mutate(
    mod= data %>% map(country_model)
  )

# 27:11

#--------------------------------------------------------------------------------------------
##### Step 3. ####

# This creates another nested df inside of your main data frame that has the summary stats of each model
models=models %>% mutate(
  tidy=map(mod, broom::tidy), # tidy() gives model estimates
  glance=map(mod,broom::glance), # glance() gives model summaries
  augment=map(mod,broom::augment) # model coefficients
)

# What can you do with this nest of data frames? 
# The reverse of step 1; un-nest it to unpack everything!
# 34:40
# Keeps a massive list of related information neatly organized!

unnest(models,data) # back to where we started
unnest(models,glance, .drop = TRUE) 
unnest(models,tidy) 
```

...and here is a version I made of the above to manage many Latent Growth Curve models.

```{r eval=FALSE}
# CONDENSED MASTER TABLE VERSION -----------------------------------------------------------------------------
# Models table that has all models condensed

models_noCovs=tibble(
  
  #### Define model names ####
  model_name=c("Linear", "Quadratic", "Latent_Basis"),
  
  ##### List model specifications for lavaan ####
  model_spec=list(
  
  linear_model= '
  # intercept and slope with fixed coefficients
  i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 +  1*panss_total_3400 + 1*panss_total_5200
  s =~ 0*panss_total_400 + 3*panss_total_1000 + 6*panss_total_1600 + 9*panss_total_2200 + 12*panss_total_2800 + 15*panss_total_3400 + 24*panss_total_5200
  ',
  
  quadratic_model= '
  # intercept and slope with fixed coefficients
  i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 +  1*panss_total_3400
  s =~ 0*panss_total_400 + 3*panss_total_1000 + 6*panss_total_1600 + 9*panss_total_2200 + 12*panss_total_2800 + 15*panss_total_3400 + 24*panss_total_5200
  
  qs =~ 0*panss_total_400 + 9*panss_total_1000 + 36*panss_total_1600 + 81*panss_total_2200 + 144*panss_total_2800 + 225*panss_total_3400 + 576*panss_total_5200
  ',
  
  latentBasis_model= '
  # intercept and slope with fixed coefficients
  i =~ 1*panss_total_400 + 1*panss_total_1000 + 1*panss_total_1600 + 1*panss_total_2200 + 1*panss_total_2800 +  1*panss_total_3400 + 1*panss_total_5200
  s =~ 0*panss_total_400 + NA*panss_total_1000 + NA*panss_total_1600 + NA*panss_total_2200 + NA*panss_total_2800 + NA*panss_total_3400 + 1*panss_total_5200
  '
  ),
  
  #### Fit all models at once with purrr ####
  fitted_model=model_spec |> map(lavaan::growth, data=panss_sem_data, missing="FIML"),
  
)



#### Add parameter estimates and fit stats ####
models_noCovs=models_noCovs |> 
  mutate(tidy_parameters=map(fitted_model, tidy, conf.int=.95), #parameter estimates
         global_fit=map(fitted_model, performance::model_performance)) #global fit of models

#### Clean up stuff ####
models_noCovs$tidy_parameters=models_noCovs$tidy_parameters |> 
  map(select,-c(std.lv:std.nox, op)) |> # remove extra columns
  map(mutate, estimate=round(estimate, digits = 2)) |> # round numbers
  map(mutate, (across(c(std.error:p.value, conf.low, conf.high), round, 3)))

models_noCovs$global_fit=models_noCovs$global_fit |> 
  map(select, c(Chi2:p_Chi2, RMSEA:SRMR, CFI, AIC))
```

Note how the functions inside `map` take on a slightly different form, but work the same.

Using this framework, you can easily drill down into any column and be sure that you're accessing the right thing. Everything is always kept together, and always acted upon in the same way. This minimizes mistakes.

```{r eval=FALSE}
models_noCovs$tidy_parameters$latent_Basis_model
```

